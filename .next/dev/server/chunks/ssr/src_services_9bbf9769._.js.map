{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 4, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/engine/promptService.ts"],"sourcesContent":["import { TargetAudience, TokenUsage, CostBreakdown } from '../../types';\nimport { PRICING } from '../../config/constants';\n\nconst asNumber = (...values: any[]): number => {\n    for (const value of values) {\n        const num = typeof value === 'string' ? Number(value) : value;\n        if (typeof num === 'number' && Number.isFinite(num)) {\n            return num;\n        }\n    }\n    return 0;\n};\n\nconst pickUsageContainer = (usage: any): any => {\n    if (!usage) return null;\n\n    // Prefer totalUsage when present (Vercel AI SDK format)\n    if (usage.totalUsage || usage.usage) {\n        return usage.totalUsage || usage.usage;\n    }\n\n    // Sometimes usage is nested under data/metadata\n    if (usage.data?.totalUsage || usage.data?.usage) {\n        return usage.data.totalUsage || usage.data.usage;\n    }\n    if (usage.metadata?.totalUsage || usage.metadata?.usage) {\n        return usage.metadata.totalUsage || usage.metadata.usage;\n    }\n\n    return usage;\n};\n\nexport const normalizeTokenUsage = (usage: any): TokenUsage => {\n    const source = pickUsageContainer(usage) || {};\n\n    let inputTokens = asNumber(\n        source.inputTokens,\n        source.promptTokens,\n        source.prompt_tokens,\n        source.input_tokens,\n        source.promptTokenCount,\n        source.inputTokenCount,\n        source.tokens?.inputTokens,\n        source.tokens?.promptTokens\n    );\n\n    let outputTokens = asNumber(\n        source.outputTokens,\n        source.completionTokens,\n        source.output_tokens,\n        source.completion_tokens,\n        source.candidatesTokenCount,\n        source.outputTokenCount,\n        source.tokens?.outputTokens,\n        source.tokens?.completionTokens\n    );\n\n    let totalTokens = asNumber(\n        source.totalTokens,\n        source.total_tokens,\n        source.totalTokenCount,\n        source.tokens?.totalTokens\n    );\n\n    if (!totalTokens) {\n        totalTokens = inputTokens + outputTokens;\n    } else if (!inputTokens && outputTokens && totalTokens >= outputTokens) {\n        inputTokens = totalTokens - outputTokens;\n    } else if (!outputTokens && inputTokens && totalTokens >= inputTokens) {\n        outputTokens = totalTokens - inputTokens;\n    } else if (!inputTokens && !outputTokens) {\n        inputTokens = totalTokens;\n    }\n\n    return {\n        inputTokens,\n        outputTokens,\n        totalTokens\n    };\n};\n\n// Helper: Calculate Cost\nexport const calculateCost = (usage: any, modelType: keyof typeof PRICING): { usage: TokenUsage, cost: CostBreakdown } => {\n    const normalized = normalizeTokenUsage(usage);\n    const rates = PRICING[modelType];\n    const inputCost = normalized.inputTokens * rates.input;\n    const outputCost = normalized.outputTokens * rates.output;\n\n    return {\n        usage: normalized,\n        cost: { inputCost, outputCost, totalCost: inputCost + outputCost }\n    };\n};\n\n// Helper: Normalize usage to TokenUsage shape\nexport const toTokenUsage = (usage: any): TokenUsage => normalizeTokenUsage(usage);\n\n// Helper: Get Language Instruction\nexport const getLanguageInstruction = (audience: TargetAudience): string => {\n    switch (audience) {\n        case 'zh-HK':\n            return `\n          **OUTPUT LANGUAGE:** Traditional Chinese (Hong Kong).\n          - Use Hong Kong specific vocabulary (e.g., '質素' instead of '品質', '互聯網' instead of '網際網路', '智能手機').\n          - Style should be natural for Hong Kong readers (Standard Written Chinese with HK nuances).\n          - **STRICTLY FORBIDDEN:** Spoken Cantonese particles (e.g., 嘅, 係, 咗, 佢, 咁) unless explicitly requested.\n          - Maintain a professional written tone (Standard Written Chinese).\n\n          `;\n        case 'zh-MY':\n            return `\n          **OUTPUT LANGUAGE:** Simplified Chinese (Malaysia).\n          - Use Simplified characters.\n          - Use Malaysia-specific Chinese vocabulary and context where applicable (e.g., '巴刹' for market, '巴士' for bus, local currency references if needed).\n          - Tone: Relatable to Malaysian Chinese readers.\n\n          `;\n        case 'zh-TW':\n        default:\n            return `\n          **OUTPUT LANGUAGE:** Traditional Chinese (Taiwan).\n          - Use Taiwan specific vocabulary (e.g., '品質', '網際網路', '計程車').\n          - Style should be natural for Taiwanese readers.\n          - **STRICTLY FORBIDDEN:** Cantonese particles (e.g., 嘅, 係, 咗, 佢, 咁) and Hong Kong specific vocabulary (e.g., '質素').\n          - Ensure the tone is standard written Chinese suitable for Taiwan.\n\n          `;\n    }\n};\n\n// Helper: Extract raw snippets from text based on keywords\nexport const extractRawSnippets = (text: string, keyword: string, contextWindowChars: number = 100): string[] => {\n    if (!text || !keyword) return [];\n\n    const regex = new RegExp(keyword.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&'), 'gi');\n    const matches: string[] = [];\n    let match;\n\n    // Limit processing to prevent hang on massive texts\n    let count = 0;\n    const MAX_LOOPS = 50;\n\n    while ((match = regex.exec(text)) !== null && count < MAX_LOOPS) {\n        count++;\n        const start = Math.max(0, match.index - contextWindowChars);\n        const end = Math.min(text.length, match.index + keyword.length + contextWindowChars);\n\n        // Clean up whitespace\n        const snippet = \"...\" + text.substring(start, end).replace(/\\s+/g, ' ').trim() + \"...\";\n        matches.push(snippet);\n    }\n\n    // Deduplicate snippets\n    const uniqueMatches = Array.from(new Set(matches));\n\n    // Return max 3 snippets to avoid token overflow and UI clutter\n    return uniqueMatches.slice(0, 3);\n};\n"],"names":[],"mappings":";;;;;;;;;;;;AACA;;AAEA,MAAM,WAAW,CAAC,GAAG;IACjB,KAAK,MAAM,SAAS,OAAQ;QACxB,MAAM,MAAM,OAAO,UAAU,WAAW,OAAO,SAAS;QACxD,IAAI,OAAO,QAAQ,YAAY,OAAO,QAAQ,CAAC,MAAM;YACjD,OAAO;QACX;IACJ;IACA,OAAO;AACX;AAEA,MAAM,qBAAqB,CAAC;IACxB,IAAI,CAAC,OAAO,OAAO;IAEnB,wDAAwD;IACxD,IAAI,MAAM,UAAU,IAAI,MAAM,KAAK,EAAE;QACjC,OAAO,MAAM,UAAU,IAAI,MAAM,KAAK;IAC1C;IAEA,gDAAgD;IAChD,IAAI,MAAM,IAAI,EAAE,cAAc,MAAM,IAAI,EAAE,OAAO;QAC7C,OAAO,MAAM,IAAI,CAAC,UAAU,IAAI,MAAM,IAAI,CAAC,KAAK;IACpD;IACA,IAAI,MAAM,QAAQ,EAAE,cAAc,MAAM,QAAQ,EAAE,OAAO;QACrD,OAAO,MAAM,QAAQ,CAAC,UAAU,IAAI,MAAM,QAAQ,CAAC,KAAK;IAC5D;IAEA,OAAO;AACX;AAEO,MAAM,sBAAsB,CAAC;IAChC,MAAM,SAAS,mBAAmB,UAAU,CAAC;IAE7C,IAAI,cAAc,SACd,OAAO,WAAW,EAClB,OAAO,YAAY,EACnB,OAAO,aAAa,EACpB,OAAO,YAAY,EACnB,OAAO,gBAAgB,EACvB,OAAO,eAAe,EACtB,OAAO,MAAM,EAAE,aACf,OAAO,MAAM,EAAE;IAGnB,IAAI,eAAe,SACf,OAAO,YAAY,EACnB,OAAO,gBAAgB,EACvB,OAAO,aAAa,EACpB,OAAO,iBAAiB,EACxB,OAAO,oBAAoB,EAC3B,OAAO,gBAAgB,EACvB,OAAO,MAAM,EAAE,cACf,OAAO,MAAM,EAAE;IAGnB,IAAI,cAAc,SACd,OAAO,WAAW,EAClB,OAAO,YAAY,EACnB,OAAO,eAAe,EACtB,OAAO,MAAM,EAAE;IAGnB,IAAI,CAAC,aAAa;QACd,cAAc,cAAc;IAChC,OAAO,IAAI,CAAC,eAAe,gBAAgB,eAAe,cAAc;QACpE,cAAc,cAAc;IAChC,OAAO,IAAI,CAAC,gBAAgB,eAAe,eAAe,aAAa;QACnE,eAAe,cAAc;IACjC,OAAO,IAAI,CAAC,eAAe,CAAC,cAAc;QACtC,cAAc;IAClB;IAEA,OAAO;QACH;QACA;QACA;IACJ;AACJ;AAGO,MAAM,gBAAgB,CAAC,OAAY;IACtC,MAAM,aAAa,oBAAoB;IACvC,MAAM,QAAQ,qIAAO,CAAC,UAAU;IAChC,MAAM,YAAY,WAAW,WAAW,GAAG,MAAM,KAAK;IACtD,MAAM,aAAa,WAAW,YAAY,GAAG,MAAM,MAAM;IAEzD,OAAO;QACH,OAAO;QACP,MAAM;YAAE;YAAW;YAAY,WAAW,YAAY;QAAW;IACrE;AACJ;AAGO,MAAM,eAAe,CAAC,QAA2B,oBAAoB;AAGrE,MAAM,yBAAyB,CAAC;IACnC,OAAQ;QACJ,KAAK;YACD,OAAO,CAAC;;;;;;;UAOV,CAAC;QACH,KAAK;YACD,OAAO,CAAC;;;;;;UAMV,CAAC;QACH,KAAK;QACL;YACI,OAAO,CAAC;;;;;;;UAOV,CAAC;IACP;AACJ;AAGO,MAAM,qBAAqB,CAAC,MAAc,SAAiB,qBAA6B,GAAG;IAC9F,IAAI,CAAC,QAAQ,CAAC,SAAS,OAAO,EAAE;IAEhC,MAAM,QAAQ,IAAI,OAAO,QAAQ,OAAO,CAAC,uBAAuB,SAAS;IACzE,MAAM,UAAoB,EAAE;IAC5B,IAAI;IAEJ,oDAAoD;IACpD,IAAI,QAAQ;IACZ,MAAM,YAAY;IAElB,MAAO,CAAC,QAAQ,MAAM,IAAI,CAAC,KAAK,MAAM,QAAQ,QAAQ,UAAW;QAC7D;QACA,MAAM,QAAQ,KAAK,GAAG,CAAC,GAAG,MAAM,KAAK,GAAG;QACxC,MAAM,MAAM,KAAK,GAAG,CAAC,KAAK,MAAM,EAAE,MAAM,KAAK,GAAG,QAAQ,MAAM,GAAG;QAEjE,sBAAsB;QACtB,MAAM,UAAU,QAAQ,KAAK,SAAS,CAAC,OAAO,KAAK,OAAO,CAAC,QAAQ,KAAK,IAAI,KAAK;QACjF,QAAQ,IAAI,CAAC;IACjB;IAEA,uBAAuB;IACvB,MAAM,gBAAgB,MAAM,IAAI,CAAC,IAAI,IAAI;IAEzC,+DAA+D;IAC/D,OAAO,cAAc,KAAK,CAAC,GAAG;AAClC"}},
    {"offset": {"line": 133, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/engine/genAIClient.ts"],"sourcesContent":["import { AIResponse } from '../../types';\nimport { AI_DEFAULTS } from '../../config/constants';\n\ninterface GenAIRequest {\n    model: string;\n    contents: any;\n    config?: any;\n    promptId?: string;\n    timeoutMs?: number;\n    signal?: AbortSignal;\n}\n\ninterface RetryOptions {\n    attempts?: number;\n    delayMs?: number;\n}\n\nconst DEFAULT_RETRY: Required<RetryOptions> = {\n    attempts: AI_DEFAULTS.RETRY_ATTEMPTS,\n    delayMs: AI_DEFAULTS.RETRY_DELAY_MS,\n};\n\nconst DEFAULT_TIMEOUT = AI_DEFAULTS.TIMEOUT_MS;\n\n// Exported for reuse in other services\nexport const env = process.env;\n\nconst isBrowser = typeof window !== 'undefined';\n\n// In Next.js, we'll use the local API route (/api/ai) as a proxy\n// This avoids CORS issues and keeps the AI token secure on the server.\nconst AI_BASE_URL = isBrowser ? '' : (env.AI_BASE_URL || '');\nconst AI_PATH = isBrowser ? '/api/ai' : (env.AI_PATH || '/ai');\n\nexport const buildAiUrl = (path: string) => {\n    const prefix = AI_PATH\n        ? (AI_PATH.startsWith('/') ? AI_PATH : `/${AI_PATH}`)\n        : '';\n    return `${AI_BASE_URL}${prefix}${path}`;\n};\n\nexport const getAiHeaders = () => {\n    const token = env.VITE_AI_TOKEN || env.AI_TOKEN;\n    const headers: Record<string, string> = {\n        'Content-Type': 'application/json',\n    };\n    if (token) {\n        headers['Authorization'] = `Bearer ${token}`;\n    }\n    return headers;\n};\n\nconst extractTextFromCandidates = (candidates: any[] | undefined): string => {\n    if (!Array.isArray(candidates)) return '';\n    for (const candidate of candidates) {\n        const parts = candidate?.content?.parts;\n        if (Array.isArray(parts)) {\n            const text = parts.map((p: any) => p?.text || '').filter(Boolean).join('');\n            if (text) return text;\n        }\n    }\n    return '';\n};\n\nconst buildPrompt = (contents: any): string => {\n    if (!contents) return '';\n    if (typeof contents === 'string') return contents;\n    if (Array.isArray(contents)) {\n        const parts: string[] = [];\n        for (const item of contents) {\n            if (typeof item === 'string') {\n                parts.push(item);\n            } else if (item?.parts?.length) {\n                const joined = item.parts.map((p: any) => p?.text || '').filter(Boolean).join('');\n                if (joined) parts.push(joined);\n            } else if (item?.text) {\n                parts.push(item.text);\n            }\n        }\n        return parts.join('\\n\\n');\n    }\n    if (contents?.parts?.length) {\n        return contents.parts.map((p: any) => p?.text || '').filter(Boolean).join('');\n    }\n    if (contents?.text) return contents.text;\n    try {\n        return JSON.stringify(contents);\n    } catch {\n        return '';\n    }\n};\n\nconst extractTextFromUIMessage = (message: any): string => {\n    const content = message?.content;\n    if (!content) return '';\n    if (typeof content === 'string') return content;\n    if (Array.isArray(content)) {\n        return content\n            .map((part: any) => {\n                if (typeof part === 'string') return part;\n                if (typeof part?.text === 'string') return part.text;\n                if (typeof part?.value === 'string') return part.value;\n                if (typeof part?.data?.text === 'string') return part.data.text;\n                return '';\n            })\n            .filter(Boolean)\n            .join('');\n    }\n    return '';\n};\n\nconst parseSseTextContent = (raw: string): { text: string; usageMetadata?: any; object?: any } | null => {\n    if (typeof raw !== 'string') return null;\n    if (!raw.includes('data:')) return null;\n\n    const chunks = raw.split(/data:\\s*/).slice(1);\n    if (chunks.length === 0) return null;\n\n    let text = '';\n    let usageMetadata: any;\n    let object: any;\n\n    for (const chunk of chunks) {\n        const line = chunk.split(/\\n/)[0].trim();\n        if (!line || line === '[DONE]') continue;\n\n        let evt: any;\n        try {\n            evt = JSON.parse(line);\n        } catch {\n            continue;\n        }\n\n        const type = evt.type || evt.event || evt.kind;\n        if (type === 'text-delta') {\n            const delta = evt.textDelta ?? evt.delta ?? evt.text;\n            if (typeof delta === 'string') text += delta;\n        } else if (type === 'message') {\n            const msgText = extractTextFromUIMessage(evt.message);\n            if (msgText) text += msgText;\n            usageMetadata = evt.message?.metadata?.totalUsage || evt.message?.metadata?.usage || usageMetadata;\n        } else if (type === 'object') {\n            object = evt.object ?? object;\n        } else if (type === 'finish') {\n            usageMetadata = evt.message?.metadata?.totalUsage ||\n                evt.message?.metadata?.usage ||\n                evt.usage ||\n                evt.totalUsage ||\n                usageMetadata;\n            if (evt.object !== undefined) object = evt.object;\n        } else if (typeof evt.text === 'string' && !type) {\n            text += evt.text;\n        }\n    }\n\n    const cleaned = text.trim() || raw.trim();\n    return { text: cleaned, usageMetadata, object };\n};\n\nconst consumeEventStream = async (body: ReadableStream<Uint8Array>): Promise<AIResponse> => {\n    const reader = body.getReader();\n    const decoder = new TextDecoder();\n    let buffer = '';\n    let text = '';\n    let object: any;\n    let usageMetadata: any;\n\n    const processBuffer = () => {\n        // Normalize CRLF to LF so we can reliably find blank-line delimiters\n        if (buffer.includes('\\r\\n')) {\n            buffer = buffer.replace(/\\r\\n/g, '\\n');\n        }\n\n        let idx;\n        while ((idx = buffer.indexOf('\\n\\n')) !== -1) {\n            const raw = buffer.slice(0, idx).trim();\n            buffer = buffer.slice(idx + 2);\n\n            if (!raw) continue;\n\n            // Support standard SSE blocks that include an optional `event:` line before `data:`\n            let eventName = '';\n            const dataLines: string[] = [];\n            for (const line of raw.split('\\n')) {\n                const trimmed = line.trim();\n                if (!trimmed) continue;\n                if (trimmed.startsWith(':')) continue; // comment line\n                if (trimmed.startsWith('event:')) {\n                    eventName = trimmed.slice('event:'.length).trim();\n                } else if (trimmed.startsWith('data:')) {\n                    dataLines.push(trimmed.slice('data:'.length).trim());\n                }\n            }\n\n            const payload = dataLines.join('\\n').trim();\n            if (!payload) continue;\n\n            let evt: any;\n            try {\n                evt = JSON.parse(payload);\n            } catch {\n                continue;\n            }\n            const type = evt.type || evt.event || evt.kind || eventName;\n            if (type === 'text-delta') {\n                const delta = evt.textDelta ?? evt.delta ?? evt.text;\n                if (typeof delta === 'string') text += delta;\n            } else if (type === 'message') {\n                const msgText = extractTextFromUIMessage(evt.message);\n                if (msgText) text += msgText;\n                usageMetadata = evt.message?.metadata?.totalUsage || evt.message?.metadata?.usage || usageMetadata;\n            } else if (type === 'object') {\n                object = evt.object ?? object;\n            } else if (type === 'finish') {\n                usageMetadata = evt.message?.metadata?.totalUsage || evt.message?.metadata?.usage || evt.usage || evt.totalUsage || usageMetadata;\n                if (evt.object !== undefined) object = evt.object;\n            } else if (typeof evt.text === 'string') {\n                text += evt.text;\n            }\n        }\n    };\n\n    while (true) {\n        const { value, done } = await reader.read();\n        buffer += decoder.decode(value || new Uint8Array(), { stream: !done });\n        processBuffer();\n        if (done) break;\n    }\n    processBuffer();\n\n    return { text, object, usageMetadata };\n};\n\nconst normalizeResponse = (raw: any): AIResponse => {\n    // Some backends wrap the actual payload under `data`\n    const envelope = raw?.data && typeof raw.data === 'object' ? raw.data : raw;\n    const candidates =\n        envelope?.candidates ||\n        raw?.candidates ||\n        envelope?.response?.candidates ||\n        raw?.response?.candidates;\n    let usageMetadata =\n        envelope?.totalUsage ||\n        envelope?.usageMetadata ||\n        envelope?.usage ||\n        raw?.totalUsage ||\n        raw?.usageMetadata ||\n        raw?.usage ||\n        envelope?.response?.totalUsage ||\n        envelope?.response?.usageMetadata;\n    let object = envelope?.object;\n\n    // Check if this is a schema-based response (backend returns 'object' field)\n    if (envelope?.object !== undefined) {\n        return {\n            text: typeof envelope.text === 'string' ? envelope.text : JSON.stringify(envelope.object),\n            object: envelope.object,\n            usageMetadata,\n            candidates\n        };\n    }\n\n    // Original text-based response\n    let text =\n        envelope?.text ||\n        raw?.text ||\n        envelope?.response?.text ||\n        raw?.response?.text ||\n        extractTextFromCandidates(candidates) ||\n        (typeof envelope === 'string' ? envelope : '');\n\n    const parsedStream = parseSseTextContent(text);\n    if (parsedStream) {\n        text = parsedStream.text;\n        object = object ?? parsedStream.object;\n        usageMetadata = usageMetadata ?? parsedStream.usageMetadata;\n    }\n\n    return { text, usageMetadata, candidates, object };\n};\n\nexport class GenAIClient {\n    async request(\n        { model, contents, config, timeoutMs, signal }: GenAIRequest,\n        retryOpts: RetryOptions = {}\n    ): Promise<AIResponse> {\n        const { attempts, delayMs } = { ...DEFAULT_RETRY, ...retryOpts };\n        const controller = new AbortController();\n        const combinedSignal = signal\n            ? new AbortSignalAny([signal, controller.signal]).signal\n            : controller.signal;\n\n        const prompt = buildPrompt(contents);\n        const payload: Record<string, any> = { model };\n        if (prompt) payload.prompt = prompt;\n\n        // Extract schema from config if present\n        if (config) {\n            const { responseSchema, responseMimeType, providerOptions, ...restConfig } = config;\n\n            // Some backends expect `responseSchema`, others expect `schema`.\n            // Send both to stay backward compatible and avoid missing-field errors.\n            if (responseSchema) {\n                payload.schema = responseSchema;\n                payload.responseSchema = responseSchema;\n            }\n\n            if (responseMimeType) payload.responseMimeType = responseMimeType;\n\n            // Pass providerOptions for features like Google Search Grounding\n            if (providerOptions) payload.providerOptions = providerOptions;\n\n            if (Object.keys(restConfig).length > 0) payload.config = restConfig;\n        }\n\n        if (contents && typeof contents !== 'string') {\n            payload.contents = contents;\n        }\n\n        let lastError: unknown = null;\n        for (let attempt = 0; attempt < attempts; attempt++) {\n            const timer = setTimeout(\n                () => {\n                    console.error('[GenAIClient] Request TIMEOUT after', timeoutMs || DEFAULT_TIMEOUT, 'ms');\n                    controller.abort('GenAI request timed out');\n                },\n                timeoutMs || DEFAULT_TIMEOUT\n            );\n            try {\n                // DEBUG: Log request payload for debugging\n                console.log('[GenAIClient] Request payload:', JSON.stringify(payload, null, 2));\n                console.log('[GenAIClient] Request URL:', buildAiUrl('/generate'));\n\n                const headers = getAiHeaders();\n\n                const doRequest = async (path: string) => fetch(buildAiUrl(path), {\n                    method: 'POST',\n                    headers,\n                    body: JSON.stringify(payload),\n                    signal: combinedSignal,\n                });\n\n                let response = await doRequest('/generate');\n\n                // Backward compatibility: try /stream if /generate is missing\n                if (response.status === 404) {\n                    response = await doRequest('/stream');\n                }\n\n                const contentType = response.headers.get('content-type') || '';\n                const isJson = contentType.includes('application/json');\n                const isEventStream = contentType.includes('text/event-stream');\n\n                if (!response.ok) {\n                    const errorData = isJson ? await response.json() : await response.text();\n                    const detail = typeof errorData === 'string'\n                        ? errorData\n                        : (errorData.error || JSON.stringify(errorData));\n                    throw new Error(`Failed to generate content (HTTP ${response.status}): ${detail}`);\n                }\n\n                if (isEventStream) {\n                    if (!response.body) throw new Error('Stream response missing body');\n                    const streamed = await consumeEventStream(response.body);\n                    clearTimeout(timer);\n                    return streamed;\n                }\n\n                const data = isJson ? await response.json() : { text: await response.text() };\n                clearTimeout(timer);\n                return normalizeResponse(data);\n            } catch (err: any) {\n                lastError = err;\n                clearTimeout(timer);\n                const message = err?.message || '';\n                const retryable = message.includes('503') || message.includes('UNAVAILABLE') || message.toLowerCase().includes('overloaded');\n                if (attempt < attempts - 1 && retryable) {\n                    const wait = delayMs * (attempt + 1);\n                    console.warn(`GenAI retry (${attempt + 1}/${attempts}) after ${wait}ms due to: ${message}`);\n                    await new Promise((res) => setTimeout(res, wait));\n                    continue;\n                }\n                if (attempt < attempts - 1) {\n                    await new Promise((res) => setTimeout(res, delayMs));\n                }\n            }\n        }\n        throw lastError instanceof Error ? lastError : new Error(`GenAI request failed: ${String(lastError)}`);\n    }\n}\n\n// Simple helper to merge multiple AbortSignals\nclass AbortSignalAny {\n    private controller: AbortController;\n    public signal: AbortSignal;\n\n    constructor(signals: AbortSignal[]) {\n        this.controller = new AbortController();\n        this.signal = this.controller.signal;\n\n        signals.forEach((sig) => {\n            if (sig) {\n                if (sig.aborted) {\n                    this.controller.abort(sig.reason);\n                } else {\n                    sig.addEventListener('abort', () => this.controller.abort(sig.reason));\n                }\n            }\n        });\n    }\n}\n\nexport const genAIClient = new GenAIClient();\n"],"names":[],"mappings":";;;;;;;;;;;;AACA;;AAgBA,MAAM,gBAAwC;IAC1C,UAAU,yIAAW,CAAC,cAAc;IACpC,SAAS,yIAAW,CAAC,cAAc;AACvC;AAEA,MAAM,kBAAkB,yIAAW,CAAC,UAAU;AAGvC,MAAM,MAAM,QAAQ,GAAG;AAE9B,MAAM,YAAY,kDAAkB;AAEpC,iEAAiE;AACjE,uEAAuE;AACvE,MAAM,cAAc,sCAAY,0BAAM,IAAI,WAAW,IAAI;AACzD,MAAM,UAAU,sCAAY,0BAAa,IAAI,OAAO,IAAI;AAEjD,MAAM,aAAa,CAAC;IACvB,MAAM,SAAS,uCACR,QAAQ,UAAU,CAAC,OAAO,UAAU,CAAC,CAAC,EAAE,SAAS,GAClD;IACN,OAAO,GAAG,cAAc,SAAS,MAAM;AAC3C;AAEO,MAAM,eAAe;IACxB,MAAM,QAAQ,IAAI,aAAa,IAAI,IAAI,QAAQ;IAC/C,MAAM,UAAkC;QACpC,gBAAgB;IACpB;IACA,IAAI,OAAO;QACP,OAAO,CAAC,gBAAgB,GAAG,CAAC,OAAO,EAAE,OAAO;IAChD;IACA,OAAO;AACX;AAEA,MAAM,4BAA4B,CAAC;IAC/B,IAAI,CAAC,MAAM,OAAO,CAAC,aAAa,OAAO;IACvC,KAAK,MAAM,aAAa,WAAY;QAChC,MAAM,QAAQ,WAAW,SAAS;QAClC,IAAI,MAAM,OAAO,CAAC,QAAQ;YACtB,MAAM,OAAO,MAAM,GAAG,CAAC,CAAC,IAAW,GAAG,QAAQ,IAAI,MAAM,CAAC,SAAS,IAAI,CAAC;YACvE,IAAI,MAAM,OAAO;QACrB;IACJ;IACA,OAAO;AACX;AAEA,MAAM,cAAc,CAAC;IACjB,IAAI,CAAC,UAAU,OAAO;IACtB,IAAI,OAAO,aAAa,UAAU,OAAO;IACzC,IAAI,MAAM,OAAO,CAAC,WAAW;QACzB,MAAM,QAAkB,EAAE;QAC1B,KAAK,MAAM,QAAQ,SAAU;YACzB,IAAI,OAAO,SAAS,UAAU;gBAC1B,MAAM,IAAI,CAAC;YACf,OAAO,IAAI,MAAM,OAAO,QAAQ;gBAC5B,MAAM,SAAS,KAAK,KAAK,CAAC,GAAG,CAAC,CAAC,IAAW,GAAG,QAAQ,IAAI,MAAM,CAAC,SAAS,IAAI,CAAC;gBAC9E,IAAI,QAAQ,MAAM,IAAI,CAAC;YAC3B,OAAO,IAAI,MAAM,MAAM;gBACnB,MAAM,IAAI,CAAC,KAAK,IAAI;YACxB;QACJ;QACA,OAAO,MAAM,IAAI,CAAC;IACtB;IACA,IAAI,UAAU,OAAO,QAAQ;QACzB,OAAO,SAAS,KAAK,CAAC,GAAG,CAAC,CAAC,IAAW,GAAG,QAAQ,IAAI,MAAM,CAAC,SAAS,IAAI,CAAC;IAC9E;IACA,IAAI,UAAU,MAAM,OAAO,SAAS,IAAI;IACxC,IAAI;QACA,OAAO,KAAK,SAAS,CAAC;IAC1B,EAAE,OAAM;QACJ,OAAO;IACX;AACJ;AAEA,MAAM,2BAA2B,CAAC;IAC9B,MAAM,UAAU,SAAS;IACzB,IAAI,CAAC,SAAS,OAAO;IACrB,IAAI,OAAO,YAAY,UAAU,OAAO;IACxC,IAAI,MAAM,OAAO,CAAC,UAAU;QACxB,OAAO,QACF,GAAG,CAAC,CAAC;YACF,IAAI,OAAO,SAAS,UAAU,OAAO;YACrC,IAAI,OAAO,MAAM,SAAS,UAAU,OAAO,KAAK,IAAI;YACpD,IAAI,OAAO,MAAM,UAAU,UAAU,OAAO,KAAK,KAAK;YACtD,IAAI,OAAO,MAAM,MAAM,SAAS,UAAU,OAAO,KAAK,IAAI,CAAC,IAAI;YAC/D,OAAO;QACX,GACC,MAAM,CAAC,SACP,IAAI,CAAC;IACd;IACA,OAAO;AACX;AAEA,MAAM,sBAAsB,CAAC;IACzB,IAAI,OAAO,QAAQ,UAAU,OAAO;IACpC,IAAI,CAAC,IAAI,QAAQ,CAAC,UAAU,OAAO;IAEnC,MAAM,SAAS,IAAI,KAAK,CAAC,YAAY,KAAK,CAAC;IAC3C,IAAI,OAAO,MAAM,KAAK,GAAG,OAAO;IAEhC,IAAI,OAAO;IACX,IAAI;IACJ,IAAI;IAEJ,KAAK,MAAM,SAAS,OAAQ;QACxB,MAAM,OAAO,MAAM,KAAK,CAAC,KAAK,CAAC,EAAE,CAAC,IAAI;QACtC,IAAI,CAAC,QAAQ,SAAS,UAAU;QAEhC,IAAI;QACJ,IAAI;YACA,MAAM,KAAK,KAAK,CAAC;QACrB,EAAE,OAAM;YACJ;QACJ;QAEA,MAAM,OAAO,IAAI,IAAI,IAAI,IAAI,KAAK,IAAI,IAAI,IAAI;QAC9C,IAAI,SAAS,cAAc;YACvB,MAAM,QAAQ,IAAI,SAAS,IAAI,IAAI,KAAK,IAAI,IAAI,IAAI;YACpD,IAAI,OAAO,UAAU,UAAU,QAAQ;QAC3C,OAAO,IAAI,SAAS,WAAW;YAC3B,MAAM,UAAU,yBAAyB,IAAI,OAAO;YACpD,IAAI,SAAS,QAAQ;YACrB,gBAAgB,IAAI,OAAO,EAAE,UAAU,cAAc,IAAI,OAAO,EAAE,UAAU,SAAS;QACzF,OAAO,IAAI,SAAS,UAAU;YAC1B,SAAS,IAAI,MAAM,IAAI;QAC3B,OAAO,IAAI,SAAS,UAAU;YAC1B,gBAAgB,IAAI,OAAO,EAAE,UAAU,cACnC,IAAI,OAAO,EAAE,UAAU,SACvB,IAAI,KAAK,IACT,IAAI,UAAU,IACd;YACJ,IAAI,IAAI,MAAM,KAAK,WAAW,SAAS,IAAI,MAAM;QACrD,OAAO,IAAI,OAAO,IAAI,IAAI,KAAK,YAAY,CAAC,MAAM;YAC9C,QAAQ,IAAI,IAAI;QACpB;IACJ;IAEA,MAAM,UAAU,KAAK,IAAI,MAAM,IAAI,IAAI;IACvC,OAAO;QAAE,MAAM;QAAS;QAAe;IAAO;AAClD;AAEA,MAAM,qBAAqB,OAAO;IAC9B,MAAM,SAAS,KAAK,SAAS;IAC7B,MAAM,UAAU,IAAI;IACpB,IAAI,SAAS;IACb,IAAI,OAAO;IACX,IAAI;IACJ,IAAI;IAEJ,MAAM,gBAAgB;QAClB,qEAAqE;QACrE,IAAI,OAAO,QAAQ,CAAC,SAAS;YACzB,SAAS,OAAO,OAAO,CAAC,SAAS;QACrC;QAEA,IAAI;QACJ,MAAO,CAAC,MAAM,OAAO,OAAO,CAAC,OAAO,MAAM,CAAC,EAAG;YAC1C,MAAM,MAAM,OAAO,KAAK,CAAC,GAAG,KAAK,IAAI;YACrC,SAAS,OAAO,KAAK,CAAC,MAAM;YAE5B,IAAI,CAAC,KAAK;YAEV,oFAAoF;YACpF,IAAI,YAAY;YAChB,MAAM,YAAsB,EAAE;YAC9B,KAAK,MAAM,QAAQ,IAAI,KAAK,CAAC,MAAO;gBAChC,MAAM,UAAU,KAAK,IAAI;gBACzB,IAAI,CAAC,SAAS;gBACd,IAAI,QAAQ,UAAU,CAAC,MAAM,UAAU,eAAe;gBACtD,IAAI,QAAQ,UAAU,CAAC,WAAW;oBAC9B,YAAY,QAAQ,KAAK,CAAC,SAAS,MAAM,EAAE,IAAI;gBACnD,OAAO,IAAI,QAAQ,UAAU,CAAC,UAAU;oBACpC,UAAU,IAAI,CAAC,QAAQ,KAAK,CAAC,QAAQ,MAAM,EAAE,IAAI;gBACrD;YACJ;YAEA,MAAM,UAAU,UAAU,IAAI,CAAC,MAAM,IAAI;YACzC,IAAI,CAAC,SAAS;YAEd,IAAI;YACJ,IAAI;gBACA,MAAM,KAAK,KAAK,CAAC;YACrB,EAAE,OAAM;gBACJ;YACJ;YACA,MAAM,OAAO,IAAI,IAAI,IAAI,IAAI,KAAK,IAAI,IAAI,IAAI,IAAI;YAClD,IAAI,SAAS,cAAc;gBACvB,MAAM,QAAQ,IAAI,SAAS,IAAI,IAAI,KAAK,IAAI,IAAI,IAAI;gBACpD,IAAI,OAAO,UAAU,UAAU,QAAQ;YAC3C,OAAO,IAAI,SAAS,WAAW;gBAC3B,MAAM,UAAU,yBAAyB,IAAI,OAAO;gBACpD,IAAI,SAAS,QAAQ;gBACrB,gBAAgB,IAAI,OAAO,EAAE,UAAU,cAAc,IAAI,OAAO,EAAE,UAAU,SAAS;YACzF,OAAO,IAAI,SAAS,UAAU;gBAC1B,SAAS,IAAI,MAAM,IAAI;YAC3B,OAAO,IAAI,SAAS,UAAU;gBAC1B,gBAAgB,IAAI,OAAO,EAAE,UAAU,cAAc,IAAI,OAAO,EAAE,UAAU,SAAS,IAAI,KAAK,IAAI,IAAI,UAAU,IAAI;gBACpH,IAAI,IAAI,MAAM,KAAK,WAAW,SAAS,IAAI,MAAM;YACrD,OAAO,IAAI,OAAO,IAAI,IAAI,KAAK,UAAU;gBACrC,QAAQ,IAAI,IAAI;YACpB;QACJ;IACJ;IAEA,MAAO,KAAM;QACT,MAAM,EAAE,KAAK,EAAE,IAAI,EAAE,GAAG,MAAM,OAAO,IAAI;QACzC,UAAU,QAAQ,MAAM,CAAC,SAAS,IAAI,cAAc;YAAE,QAAQ,CAAC;QAAK;QACpE;QACA,IAAI,MAAM;IACd;IACA;IAEA,OAAO;QAAE;QAAM;QAAQ;IAAc;AACzC;AAEA,MAAM,oBAAoB,CAAC;IACvB,qDAAqD;IACrD,MAAM,WAAW,KAAK,QAAQ,OAAO,IAAI,IAAI,KAAK,WAAW,IAAI,IAAI,GAAG;IACxE,MAAM,aACF,UAAU,cACV,KAAK,cACL,UAAU,UAAU,cACpB,KAAK,UAAU;IACnB,IAAI,gBACA,UAAU,cACV,UAAU,iBACV,UAAU,SACV,KAAK,cACL,KAAK,iBACL,KAAK,SACL,UAAU,UAAU,cACpB,UAAU,UAAU;IACxB,IAAI,SAAS,UAAU;IAEvB,4EAA4E;IAC5E,IAAI,UAAU,WAAW,WAAW;QAChC,OAAO;YACH,MAAM,OAAO,SAAS,IAAI,KAAK,WAAW,SAAS,IAAI,GAAG,KAAK,SAAS,CAAC,SAAS,MAAM;YACxF,QAAQ,SAAS,MAAM;YACvB;YACA;QACJ;IACJ;IAEA,+BAA+B;IAC/B,IAAI,OACA,UAAU,QACV,KAAK,QACL,UAAU,UAAU,QACpB,KAAK,UAAU,QACf,0BAA0B,eAC1B,CAAC,OAAO,aAAa,WAAW,WAAW,EAAE;IAEjD,MAAM,eAAe,oBAAoB;IACzC,IAAI,cAAc;QACd,OAAO,aAAa,IAAI;QACxB,SAAS,UAAU,aAAa,MAAM;QACtC,gBAAgB,iBAAiB,aAAa,aAAa;IAC/D;IAEA,OAAO;QAAE;QAAM;QAAe;QAAY;IAAO;AACrD;AAEO,MAAM;IACT,MAAM,QACF,EAAE,KAAK,EAAE,QAAQ,EAAE,MAAM,EAAE,SAAS,EAAE,MAAM,EAAgB,EAC5D,YAA0B,CAAC,CAAC,EACT;QACnB,MAAM,EAAE,QAAQ,EAAE,OAAO,EAAE,GAAG;YAAE,GAAG,aAAa;YAAE,GAAG,SAAS;QAAC;QAC/D,MAAM,aAAa,IAAI;QACvB,MAAM,iBAAiB,SACjB,IAAI,eAAe;YAAC;YAAQ,WAAW,MAAM;SAAC,EAAE,MAAM,GACtD,WAAW,MAAM;QAEvB,MAAM,SAAS,YAAY;QAC3B,MAAM,UAA+B;YAAE;QAAM;QAC7C,IAAI,QAAQ,QAAQ,MAAM,GAAG;QAE7B,wCAAwC;QACxC,IAAI,QAAQ;YACR,MAAM,EAAE,cAAc,EAAE,gBAAgB,EAAE,eAAe,EAAE,GAAG,YAAY,GAAG;YAE7E,iEAAiE;YACjE,wEAAwE;YACxE,IAAI,gBAAgB;gBAChB,QAAQ,MAAM,GAAG;gBACjB,QAAQ,cAAc,GAAG;YAC7B;YAEA,IAAI,kBAAkB,QAAQ,gBAAgB,GAAG;YAEjD,iEAAiE;YACjE,IAAI,iBAAiB,QAAQ,eAAe,GAAG;YAE/C,IAAI,OAAO,IAAI,CAAC,YAAY,MAAM,GAAG,GAAG,QAAQ,MAAM,GAAG;QAC7D;QAEA,IAAI,YAAY,OAAO,aAAa,UAAU;YAC1C,QAAQ,QAAQ,GAAG;QACvB;QAEA,IAAI,YAAqB;QACzB,IAAK,IAAI,UAAU,GAAG,UAAU,UAAU,UAAW;YACjD,MAAM,QAAQ,WACV;gBACI,QAAQ,KAAK,CAAC,uCAAuC,aAAa,iBAAiB;gBACnF,WAAW,KAAK,CAAC;YACrB,GACA,aAAa;YAEjB,IAAI;gBACA,2CAA2C;gBAC3C,QAAQ,GAAG,CAAC,kCAAkC,KAAK,SAAS,CAAC,SAAS,MAAM;gBAC5E,QAAQ,GAAG,CAAC,8BAA8B,WAAW;gBAErD,MAAM,UAAU;gBAEhB,MAAM,YAAY,OAAO,OAAiB,MAAM,WAAW,OAAO;wBAC9D,QAAQ;wBACR;wBACA,MAAM,KAAK,SAAS,CAAC;wBACrB,QAAQ;oBACZ;gBAEA,IAAI,WAAW,MAAM,UAAU;gBAE/B,8DAA8D;gBAC9D,IAAI,SAAS,MAAM,KAAK,KAAK;oBACzB,WAAW,MAAM,UAAU;gBAC/B;gBAEA,MAAM,cAAc,SAAS,OAAO,CAAC,GAAG,CAAC,mBAAmB;gBAC5D,MAAM,SAAS,YAAY,QAAQ,CAAC;gBACpC,MAAM,gBAAgB,YAAY,QAAQ,CAAC;gBAE3C,IAAI,CAAC,SAAS,EAAE,EAAE;oBACd,MAAM,YAAY,SAAS,MAAM,SAAS,IAAI,KAAK,MAAM,SAAS,IAAI;oBACtE,MAAM,SAAS,OAAO,cAAc,WAC9B,YACC,UAAU,KAAK,IAAI,KAAK,SAAS,CAAC;oBACzC,MAAM,IAAI,MAAM,CAAC,iCAAiC,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,QAAQ;gBACrF;gBAEA,IAAI,eAAe;oBACf,IAAI,CAAC,SAAS,IAAI,EAAE,MAAM,IAAI,MAAM;oBACpC,MAAM,WAAW,MAAM,mBAAmB,SAAS,IAAI;oBACvD,aAAa;oBACb,OAAO;gBACX;gBAEA,MAAM,OAAO,SAAS,MAAM,SAAS,IAAI,KAAK;oBAAE,MAAM,MAAM,SAAS,IAAI;gBAAG;gBAC5E,aAAa;gBACb,OAAO,kBAAkB;YAC7B,EAAE,OAAO,KAAU;gBACf,YAAY;gBACZ,aAAa;gBACb,MAAM,UAAU,KAAK,WAAW;gBAChC,MAAM,YAAY,QAAQ,QAAQ,CAAC,UAAU,QAAQ,QAAQ,CAAC,kBAAkB,QAAQ,WAAW,GAAG,QAAQ,CAAC;gBAC/G,IAAI,UAAU,WAAW,KAAK,WAAW;oBACrC,MAAM,OAAO,UAAU,CAAC,UAAU,CAAC;oBACnC,QAAQ,IAAI,CAAC,CAAC,aAAa,EAAE,UAAU,EAAE,CAAC,EAAE,SAAS,QAAQ,EAAE,KAAK,WAAW,EAAE,SAAS;oBAC1F,MAAM,IAAI,QAAQ,CAAC,MAAQ,WAAW,KAAK;oBAC3C;gBACJ;gBACA,IAAI,UAAU,WAAW,GAAG;oBACxB,MAAM,IAAI,QAAQ,CAAC,MAAQ,WAAW,KAAK;gBAC/C;YACJ;QACJ;QACA,MAAM,qBAAqB,QAAQ,YAAY,IAAI,MAAM,CAAC,sBAAsB,EAAE,OAAO,YAAY;IACzG;AACJ;AAEA,+CAA+C;AAC/C,MAAM;IACM,WAA4B;IAC7B,OAAoB;IAE3B,YAAY,OAAsB,CAAE;QAChC,IAAI,CAAC,UAAU,GAAG,IAAI;QACtB,IAAI,CAAC,MAAM,GAAG,IAAI,CAAC,UAAU,CAAC,MAAM;QAEpC,QAAQ,OAAO,CAAC,CAAC;YACb,IAAI,KAAK;gBACL,IAAI,IAAI,OAAO,EAAE;oBACb,IAAI,CAAC,UAAU,CAAC,KAAK,CAAC,IAAI,MAAM;gBACpC,OAAO;oBACH,IAAI,gBAAgB,CAAC,SAAS,IAAM,IAAI,CAAC,UAAU,CAAC,KAAK,CAAC,IAAI,MAAM;gBACxE;YACJ;QACJ;IACJ;AACJ;AAEO,MAAM,cAAc,IAAI"}},
    {"offset": {"line": 483, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/engine/aiService.ts"],"sourcesContent":["import { genAIClient, buildAiUrl } from './genAIClient';\nimport { MODEL } from '../../config/constants';\nimport { calculateCost } from './promptService';\nimport { TokenUsage, CostBreakdown, AIRequestConfig, AIResponse } from '../../types';\nimport { useAppStore } from '../../store/useAppStore';\n\nexport type LlmModelKey = keyof typeof MODEL;\n\nclass AIService {\n    private getModel(key: LlmModelKey): string {\n        const settings = useAppStore.getState();\n        if (key === 'FLASH') return settings.modelFlash;\n        if (key === 'IMAGE_PREVIEW') return settings.modelImage;\n        return MODEL[key];\n    }\n\n    /**\n     * Run a text generation request\n     */\n    async runText(\n        prompt: string,\n        modelKey: LlmModelKey = 'FLASH',\n        config?: AIRequestConfig\n    ): Promise<{ text: string; usage: TokenUsage; cost: CostBreakdown; duration: number }> {\n        const start = Date.now();\n        const model = this.getModel(modelKey);\n\n        try {\n            const response = await genAIClient.request({\n                model,\n                contents: prompt,\n                config\n            });\n\n            const { usage, cost } = calculateCost(response.usageMetadata, modelKey);\n\n            return {\n                text: response.text,\n                usage,\n                cost,\n                duration: Date.now() - start\n            };\n        } catch (error) {\n            console.error(`[AIService] runText failed for ${modelKey}`, error);\n            throw error;\n        }\n    }\n\n    /**\n     * Run a JSON generation request\n     */\n    async runJson<T>(\n        prompt: string,\n        modelKey: LlmModelKey = 'FLASH',\n        schema?: any\n    ): Promise<{ data: T; usage: TokenUsage; cost: CostBreakdown; duration: number }> {\n        const start = Date.now();\n        const model = this.getModel(modelKey);\n        const config: AIRequestConfig = {\n            responseMimeType: 'application/json',\n            responseSchema: schema\n        };\n\n        try {\n            const response = await genAIClient.request({\n                model,\n                contents: prompt,\n                config\n            });\n\n            const { usage, cost } = calculateCost(response.usageMetadata, modelKey);\n\n            let data: T;\n            if (response.object) {\n                data = response.object as T;\n            } else {\n                try {\n                    // Clean markdown code blocks if present\n                    const cleanText = response.text.replace(/```(?:json)?\\n?|\\n?```/gi, '').trim();\n                    data = JSON.parse(cleanText) as T;\n                } catch (e) {\n                    throw new Error(`Failed to parse JSON response: ${response.text.substring(0, 100)}...`);\n                }\n            }\n\n            return {\n                data,\n                usage,\n                cost,\n                duration: Date.now() - start\n            };\n        } catch (error) {\n            console.error(`[AIService] runJson failed for ${modelKey}`, error);\n            throw error;\n        }\n    }\n\n    /**\n     * Run a JSON generation request with Google Search Grounding enabled\n     * Use this for queries that need real-time web information (e.g., finding local brand alternatives)\n     */\n    async runJsonWithSearch<T>(\n        prompt: string,\n        modelKey: LlmModelKey = 'FLASH',\n        schema?: any\n    ): Promise<{ data: T; usage: TokenUsage; cost: CostBreakdown; duration: number }> {\n        const start = Date.now();\n        const model = this.getModel(modelKey);\n        const config: AIRequestConfig = {\n            responseMimeType: 'application/json',\n            responseSchema: schema,\n            providerOptions: {\n                vertex: {\n                    useSearchGrounding: true,\n                },\n            },\n        };\n\n        try {\n            const response = await genAIClient.request({\n                model,\n                contents: prompt,\n                config\n            });\n\n            const { usage, cost } = calculateCost(response.usageMetadata, modelKey);\n\n            let data: T;\n            if (response.object) {\n                data = response.object as T;\n            } else {\n                try {\n                    // Clean markdown code blocks if present\n                    const cleanText = response.text.replace(/```(?:json)?\\n?|\\n?```/gi, '').trim();\n                    data = JSON.parse(cleanText) as T;\n                } catch (e) {\n                    throw new Error(`Failed to parse JSON response: ${response.text.substring(0, 100)}...`);\n                }\n            }\n\n            return {\n                data,\n                usage,\n                cost,\n                duration: Date.now() - start\n            };\n        } catch (error) {\n            console.error(`[AIService] runJsonWithSearch failed for ${modelKey}`, error);\n            throw error;\n        }\n    }\n\n}\n\nexport const aiService = new AIService();\n\n// --- Shared Utils ---\n\nexport const parseSchemaResponse = <T>(\n    response: { data: T; usage: TokenUsage; cost: CostBreakdown; duration: number },\n    fallback?: T\n): { data: T; usage: TokenUsage; cost: CostBreakdown; duration: number } => {\n    if (!response.data && fallback) {\n        return { ...response, data: fallback };\n    }\n    return response;\n};\n\nexport const trackCost = (\n    metricsStore: { addCost: (cost: number, tokens: number) => void },\n    ...responses: { cost: CostBreakdown; usage: TokenUsage }[]\n) => {\n    let totalCost = 0;\n    let totalTokens = 0;\n\n    responses.forEach(r => {\n        if (r && r.cost && r.usage) {\n            totalCost += r.cost.totalCost || 0;\n            totalTokens += (r.usage.totalTokens ?? 0);\n        }\n    });\n\n    if (!isNaN(totalCost) && !isNaN(totalTokens)) {\n        metricsStore.addCost(totalCost, totalTokens);\n    }\n\n    return { totalCost, totalTokens };\n};\n"],"names":[],"mappings":";;;;;;;;AAAA;AACA;AACA;AAEA;;;;;AAIA,MAAM;IACM,SAAS,GAAgB,EAAU;QACvC,MAAM,WAAW,0IAAW,CAAC,QAAQ;QACrC,IAAI,QAAQ,SAAS,OAAO,SAAS,UAAU;QAC/C,IAAI,QAAQ,iBAAiB,OAAO,SAAS,UAAU;QACvD,OAAO,mIAAK,CAAC,IAAI;IACrB;IAEA;;KAEC,GACD,MAAM,QACF,MAAc,EACd,WAAwB,OAAO,EAC/B,MAAwB,EAC2D;QACnF,MAAM,QAAQ,KAAK,GAAG;QACtB,MAAM,QAAQ,IAAI,CAAC,QAAQ,CAAC;QAE5B,IAAI;YACA,MAAM,WAAW,MAAM,uJAAW,CAAC,OAAO,CAAC;gBACvC;gBACA,UAAU;gBACV;YACJ;YAEA,MAAM,EAAE,KAAK,EAAE,IAAI,EAAE,GAAG,IAAA,2JAAa,EAAC,SAAS,aAAa,EAAE;YAE9D,OAAO;gBACH,MAAM,SAAS,IAAI;gBACnB;gBACA;gBACA,UAAU,KAAK,GAAG,KAAK;YAC3B;QACJ,EAAE,OAAO,OAAO;YACZ,QAAQ,KAAK,CAAC,CAAC,+BAA+B,EAAE,UAAU,EAAE;YAC5D,MAAM;QACV;IACJ;IAEA;;KAEC,GACD,MAAM,QACF,MAAc,EACd,WAAwB,OAAO,EAC/B,MAAY,EACkE;QAC9E,MAAM,QAAQ,KAAK,GAAG;QACtB,MAAM,QAAQ,IAAI,CAAC,QAAQ,CAAC;QAC5B,MAAM,SAA0B;YAC5B,kBAAkB;YAClB,gBAAgB;QACpB;QAEA,IAAI;YACA,MAAM,WAAW,MAAM,uJAAW,CAAC,OAAO,CAAC;gBACvC;gBACA,UAAU;gBACV;YACJ;YAEA,MAAM,EAAE,KAAK,EAAE,IAAI,EAAE,GAAG,IAAA,2JAAa,EAAC,SAAS,aAAa,EAAE;YAE9D,IAAI;YACJ,IAAI,SAAS,MAAM,EAAE;gBACjB,OAAO,SAAS,MAAM;YAC1B,OAAO;gBACH,IAAI;oBACA,wCAAwC;oBACxC,MAAM,YAAY,SAAS,IAAI,CAAC,OAAO,CAAC,4BAA4B,IAAI,IAAI;oBAC5E,OAAO,KAAK,KAAK,CAAC;gBACtB,EAAE,OAAO,GAAG;oBACR,MAAM,IAAI,MAAM,CAAC,+BAA+B,EAAE,SAAS,IAAI,CAAC,SAAS,CAAC,GAAG,KAAK,GAAG,CAAC;gBAC1F;YACJ;YAEA,OAAO;gBACH;gBACA;gBACA;gBACA,UAAU,KAAK,GAAG,KAAK;YAC3B;QACJ,EAAE,OAAO,OAAO;YACZ,QAAQ,KAAK,CAAC,CAAC,+BAA+B,EAAE,UAAU,EAAE;YAC5D,MAAM;QACV;IACJ;IAEA;;;KAGC,GACD,MAAM,kBACF,MAAc,EACd,WAAwB,OAAO,EAC/B,MAAY,EACkE;QAC9E,MAAM,QAAQ,KAAK,GAAG;QACtB,MAAM,QAAQ,IAAI,CAAC,QAAQ,CAAC;QAC5B,MAAM,SAA0B;YAC5B,kBAAkB;YAClB,gBAAgB;YAChB,iBAAiB;gBACb,QAAQ;oBACJ,oBAAoB;gBACxB;YACJ;QACJ;QAEA,IAAI;YACA,MAAM,WAAW,MAAM,uJAAW,CAAC,OAAO,CAAC;gBACvC;gBACA,UAAU;gBACV;YACJ;YAEA,MAAM,EAAE,KAAK,EAAE,IAAI,EAAE,GAAG,IAAA,2JAAa,EAAC,SAAS,aAAa,EAAE;YAE9D,IAAI;YACJ,IAAI,SAAS,MAAM,EAAE;gBACjB,OAAO,SAAS,MAAM;YAC1B,OAAO;gBACH,IAAI;oBACA,wCAAwC;oBACxC,MAAM,YAAY,SAAS,IAAI,CAAC,OAAO,CAAC,4BAA4B,IAAI,IAAI;oBAC5E,OAAO,KAAK,KAAK,CAAC;gBACtB,EAAE,OAAO,GAAG;oBACR,MAAM,IAAI,MAAM,CAAC,+BAA+B,EAAE,SAAS,IAAI,CAAC,SAAS,CAAC,GAAG,KAAK,GAAG,CAAC;gBAC1F;YACJ;YAEA,OAAO;gBACH;gBACA;gBACA;gBACA,UAAU,KAAK,GAAG,KAAK;YAC3B;QACJ,EAAE,OAAO,OAAO;YACZ,QAAQ,KAAK,CAAC,CAAC,yCAAyC,EAAE,UAAU,EAAE;YACtE,MAAM;QACV;IACJ;AAEJ;AAEO,MAAM,YAAY,IAAI;AAItB,MAAM,sBAAsB,CAC/B,UACA;IAEA,IAAI,CAAC,SAAS,IAAI,IAAI,UAAU;QAC5B,OAAO;YAAE,GAAG,QAAQ;YAAE,MAAM;QAAS;IACzC;IACA,OAAO;AACX;AAEO,MAAM,YAAY,CACrB,cACA,GAAG;IAEH,IAAI,YAAY;IAChB,IAAI,cAAc;IAElB,UAAU,OAAO,CAAC,CAAA;QACd,IAAI,KAAK,EAAE,IAAI,IAAI,EAAE,KAAK,EAAE;YACxB,aAAa,EAAE,IAAI,CAAC,SAAS,IAAI;YACjC,eAAgB,EAAE,KAAK,CAAC,WAAW,IAAI;QAC3C;IACJ;IAEA,IAAI,CAAC,MAAM,cAAc,CAAC,MAAM,cAAc;QAC1C,aAAa,OAAO,CAAC,WAAW;IACpC;IAEA,OAAO;QAAE;QAAW;IAAY;AACpC"}},
    {"offset": {"line": 645, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/engine/schemaTypes.ts"],"sourcesContent":["// Minimal JSON schema type helper used by our generationConfig.\n// Keeps client-side code independent from the @google/genai package.\nexport const Type = {\n    OBJECT: 'object',\n    ARRAY: 'array',\n    STRING: 'string',\n    INTEGER: 'integer',\n    NUMBER: 'number',\n    BOOLEAN: 'boolean',\n} as const;\n\nexport type SchemaType = typeof Type[keyof typeof Type];\n"],"names":[],"mappings":"AAAA,gEAAgE;AAChE,qEAAqE;;;;;AAC9D,MAAM,OAAO;IAChB,QAAQ;IACR,OAAO;IACP,QAAQ;IACR,SAAS;IACT,QAAQ;IACR,SAAS;AACb"}},
    {"offset": {"line": 663, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/engine/promptTemplates.ts"],"sourcesContent":["import { TargetAudience } from '../../types';\n\nexport type PromptBuilderPayload = Record<string, any>;\nexport type PromptBuilder<T extends PromptBuilderPayload = PromptBuilderPayload> = (payload: T) => string;\n\nexport const promptTemplates = {\n  sectionContent: ({\n    sectionTitle,\n    languageInstruction,\n    previousSections,\n    futureSections,\n    generalPlan,\n    specificPlan,\n    kbInsights,\n    keywordPlans,\n    relevantAuthTerms,\n    points,\n    injectionPlan,\n    articleTitle,\n    coreQuestion,\n    difficulty,\n    writingMode,\n    solutionAngles,\n    avoidContent,\n    renderMode,\n    suppressHints,\n    augmentHints,\n    subheadings, // NEW: Accept subheadings\n    regionReplacements, // NEW: Regional replacements\n    humanWritingVoice, // NEW: Human Writing Voice (why it sounds human)\n    regionVoiceDetect, // NEW: Region Voice Detect %\n    replacementRules, // NEW: Blocked terms/Safety\n  }: {\n    sectionTitle: string;\n    languageInstruction: string;\n    previousSections: string[];\n    futureSections: string[];\n    generalPlan?: string[];\n    specificPlan?: string[];\n    kbInsights: string[];\n    keywordPlans: { word: string }[];\n    relevantAuthTerms: string[];\n    points: string[];\n    injectionPlan: string;\n    articleTitle: string;\n    coreQuestion?: string;\n    difficulty?: 'easy' | 'medium' | 'unclear';\n    writingMode?: 'direct' | 'multi_solutions';\n    solutionAngles?: string[];\n    avoidContent?: string[];\n    renderMode?: 'checklist' | 'normal';\n    // shiftPlan removed\n    suppressHints?: string[];\n    augmentHints?: string[];\n    subheadings?: string[]; // NEW\n    regionReplacements?: { original: string; replacement: string }[]; // NEW: Regional text replacements\n    humanWritingVoice?: string; // NEW\n    regionVoiceDetect?: string; // NEW\n    replacementRules?: string[]; // NEW\n  }) => {\n    const resolvedDifficulty = difficulty || 'easy';\n    const mode = writingMode || (resolvedDifficulty === 'easy' ? 'direct' : 'multi_solutions');\n    return `You are an expert editor writing the section:\n    <SectionTitle>\n      ${sectionTitle}\n    </SectionTitle>\n    DEFINITION: The title of the specific section you need to write.\n    \n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: The target language and audience for the article.\n    \n    ## Context Structure\n    <ArticleTopic>\n    ${articleTitle}\n    </ArticleTopic>\n    DEFINITION: The main topic of the entire article.\n\n    <PreviousSections>\n    ${previousSections.slice(-2).map((s: string) => s.substring(0, 100) + \"...\").join(\" | \")}\n    </PreviousSections>\n    DEFINITION: Summaries of the immediately preceding sections.\n\n    <UpcomingSections>\n    ${futureSections.join(\", \")}\n    </UpcomingSections>\n    DEFINITION: Titles of sections to be written later.\n\n\n    ## Strategy & Style\n    <OverallVoice>\n    ${generalPlan?.join(\"; \") || \"Professional, authoritative\"}\n    </OverallVoice>\n    DEFINITION: The desired tone and persona for the article.\n\n    <SectionStrategy>\n    ${specificPlan?.join(\"; \") || \"Explain thoroughly\"}\n    </SectionStrategy>\n    DEFINITION: Specific goals or angles for this section.\n\n    <BrandKnowledge>\n    ${kbInsights.length > 0 ? kbInsights.join(\"; \") : \"None\"}\n    </BrandKnowledge>\n    DEFINITION: Background facts or guidelines about the brand.\n\n    ${humanWritingVoice ? `\n    <HumanWritingVoice>\n    ${humanWritingVoice}\n    </HumanWritingVoice>\n    DEFINITION: Instructions on how to sound human and not like an AI.\n    ` : ''}\n\n    ${regionVoiceDetect ? `\n    <RegionVoiceProfile>\n    ${regionVoiceDetect}\n    </RegionVoiceProfile>\n    DEFINITION: The detected regional voice composition (e.g., 70% HK / 30% TW).\n    INSTRUCTION: Adhere to the dominant regional tone.\n    ` : ''}\n\n\n    ## Localization & Safety\n    ${(replacementRules && replacementRules.length > 0) || (regionReplacements && regionReplacements.length > 0) ? `\n    <LocalizationAndSafety>\n    ${replacementRules && replacementRules.length > 0 ? `\n    **Blocked Terms (DO NOT USE):**\n    ${replacementRules.map(r => `- ❌ ${r}`).join('\\n')}\n    ` : ''}\n    ${regionReplacements && regionReplacements.length > 0 ? `\n    **Regional Replacements (MANDATORY):**\n    ${regionReplacements.map(r => r.replacement\n      ? `- \"${r.original}\" → \"${r.replacement}\"`\n      : `- \"${r.original}\" → [REMOVE from text]`\n    ).join('\\n')}\n    ` : ''}\n    </LocalizationAndSafety>\n    DEFINITION: Terms to avoid and mandatory vocabulary corrections for the target region.\n    INSTRUCTION:\n    1. NEVER use any Blocked Terms.\n    2. ALWAYS replace \"original\" terms with their \"replacement\".\n    3. If replacement is [REMOVE], rewrite the sentence to exclude that term entirely.\n    ` : '(No localization constraints)'}\n\n\n    ## Task Definition\n    <CoreQuestion>\n    ${coreQuestion || \"Infer the precise question and answer it\"}\n    </CoreQuestion>\n    DEFINITION: The comprehensive question this section must answer.\n\n    <Difficulty>\n    ${resolvedDifficulty}\n    </Difficulty>\n    DEFINITION: The complexity level of the topic (easy, medium, unclear).\n\n    <WritingMode>\n    ${mode === 'direct' ? \"direct answer first\" : \"multi solutions then synthesize\"}\n    </WritingMode>\n    DEFINITION: The structural approach (Direct vs Multi-angle).\n    \n    ${mode === 'multi_solutions'\n        ? `- Provide 2-3 distinct, non-overlapping solution paths before the final synthesized answer.`\n        : `- Lead with a concise, direct answer to the core question.`\n      }\n\n\n    ## Conciseness Constraints\n    - ** General Rule **: Cut all fluff. Be crisp and direct. Stop immediately after answering the core question.\n    ${/introduction|conclusion|intro|outcome|result|summary|引言|結尾|結論/i.test(sectionTitle)\n        ? '- ** SPECIAL CONSTRAINT **: Target 40~80 words. Write as a SINGLE block of text (one paragraph). Do NOT split into multiple paragraphs.'\n        : ''} \n    - ** If difficulty = \"easy\" **: Target < 160 words. Get straight to the point. No preamble.\n    - ** If difficulty = \"medium\" **: Target < 180 words. Explain efficiently using Lists.\n    - ** If difficulty = \"unclear\" **: Focus on clarifying the ambiguity briefly with Lists.\n\n\n    ## Output Restrictions\n    ${renderMode === 'checklist'\n        ? '- OUTPUT FORMAT: Use checklist/bulleted list. Include every provided Key Fact; do not drop items.'\n        : '- OUTPUT FORMAT: Narrative with Markdown as needed.'\n      }\n\n\n    ## Structure Enforcement\n    ${subheadings && subheadings.length > 0 ? `\n    <MandatorySubheadings>\n    ${subheadings.map((h, i) => `${i + 1}. ${h}`).join('\\n')}\n    </MandatorySubheadings>\n    DEFINITION: The exact H3 subheadings you must use.\n    ` : '(No predefined subheadings)'\n      }\n\n\n    ## Solution Angles\n    ${mode === 'multi_solutions'\n        ? (solutionAngles && solutionAngles.length > 0 ? `\n        <DefinedAngles>\n        ${solutionAngles.join(\"; \")}\n        </DefinedAngles>\n        DEFINITION: The specific angles/perspectives to cover.\n        ` : \"None provided; create 2 distinct angles.\")\n        : \"Not needed for direct mode.\"\n      }\n\n\n    ## Resources & Keywords\n    <SemanticKeywords>\n    ${keywordPlans.map((k: any) => `\n    - \"${k.word}\": ${k.plan?.join('; ') || 'Use naturally.'}`).join('')}\n    </SemanticKeywords>\n    DEFINITION: SEO keywords with semantic usage rules.\n    INSTRUCTION: Use these words according to their \"Semantic Context\" rules to maintain the original depth.\n\n    <AuthorityTerms>\n    ${relevantAuthTerms.slice(0, 5).join(\", \")}\n    </AuthorityTerms>\n    DEFINITION: Technical or authoritative terms.\n\n\n    ## Key Facts & Narrative Points\n    - ** CRITICAL WRITING RULE **: When writing the main sentence for any Key Fact below, ** minimize the use of commas and symbols **. Use clean, direct sentence structures. \n    <KeyPoints>\n    ${points.length > 0 ? points.join(\"; \") : \"(No new key points needed for this section, focus on narrative)\"}\n    </KeyPoints>\n    DEFINITION: The core facts and information for this section.\n\n\n    ## Injection Plan\n    ${injectionPlan}\n\n\n    ## Exclusion Rules\n    ${suppressHints && suppressHints.length > 0 ? `\n    <StrictExclusion>\n    ${suppressHints.map(c => `- ${c}`).join('\\n')}\n    </StrictExclusion>\n    DEFINITION: Topics that are strictly forbidden here.\n    ` : '(None)'\n      }\n\n    ${avoidContent && avoidContent.length > 0 ? `\n    <NegativeConstraints>\n    ${avoidContent.map(c => `- ${c}`).join('\\n')}\n    </NegativeConstraints>\n    DEFINITION: Content to avoid to prevent repetition/redundancy.\n    ` : ''\n      }\n\n\n    ## Output Schema\n    - Return ONLY the content for this section in JSON format.\n    - Use proper Markdown for the content string(H3 for subsections, Lists where appropriate).\n    - Do NOT repeat the H2 Title \"${sectionTitle}\".\n    - Ensure smooth transitions from the previous section.\n    - If writing mode is \"multi_solutions\", list the solution paths clearly, then close with a synthesized recommendation.\n    - ** IMPORTANT **: You must populate the \"usedPoints\" array with the exact strings of any Key Facts you included in the content.\n    - ** IMPORTANT **: You must populate \"injectedCount\" with the number of times you explicitly mentioned the Product Name or Brand Name.\n`;\n  },\n\n  frequentWordsPlacementAnalysis: ({\n    languageInstruction,\n    analysisPayloadString,\n  }: {\n    languageInstruction: string;\n    analysisPayloadString: string;\n  }) => `\n    I have a list of High - Frequency Keywords and their \"Context Snippets\" from a Reference Text.\n\n      TASK:\n    For each keyword, analyze its context snippets to understand the specific ** Sentence Structure ** and ** Syntactic placement **.\n    Generate a \"Usage Action Plan\" (Max 3 actionable points).\n    Extract a ** SINGLE, SHORT ** \"Example Sentence\" (Max 40 chars/15 words).\n\n      <LanguageInstruction>\n      ${languageInstruction}\n      </LanguageInstruction>\n\n    The Action Plan must be specific about the ** Sentence Context **:\n    1. ** Placement **: Where does it appear? (Start/Middle/End/Transition)\n    2. ** Collocations **: What words appear around it?\n    3. ** Tone **: What function does it serve? (e.g., Is it a prefix/suffix?)\n\n    STRICT RULES:\n    - You MUST provide a \"plan\" with exactly 3 points and one \"exampleSentence\" for EVERY keyword.\n    - If a keyword's context is unclear, use your knowledge of the language/style to provide a generic but accurate action plan.\n    - NEVER return an empty array or null for \"plan\" or \"exampleSentence\".\n\n          INPUT DATA:\n    <AnalysisPayload>\n    ${analysisPayloadString}\n    </AnalysisPayload>\n    DEFINITION: JSON list of keywords and snippets.\n    ACTION: Analyze these specific snippets.\n\n    OUTPUT JSON(array):\n    [\n      {\n        \"word\": \"keyword\",\n        \"plan\": [\"actionable guidance 1\", \"actionable guidance 2\", \"actionable guidance 3\"],\n        \"exampleSentence\": \"ONE short sentence (< 15 words) from the text. NO explanations.\",\n        \"isSentenceStart\": true/false,\n        \"isSentenceEnd\": true/false,\n        \"isPrefix\": true/false,\n        \"isSuffix\": true/false\n      }\n    ]\n    Return JSON only.\n    `,\n\n  productBrief: ({ productName, productUrl, languageInstruction }: any) => `\n    I need to create a \"Product Brief\" for a marketing article.\n    \n    <ProductName>\n    \"${productName}\"\n    </ProductName>\n    DEFINITION: The raw name of the product.\n    ACTION: Use this to infer branding.\n\n    <ProductURL>\n    \"${productUrl}\"\n    </ProductURL>\n    DEFINITION: The link to the product.\n    ACTION: Use this to infer more context if obvious, and for the CTA.\n    \n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: User's preferred language.\n    ACTION: Write the output in this language.\n\n    TASK:\n    1. Infer the Brand Name and USP from the Product Name / URL.\n    2. Write a short \"Product Description\"(2 sentences).\n    3. Identify the \"Primary Pain Point\" this product solves.\n    4. Create a \"Call to Action (CTA)\" link text.\n    \n    OUTPUT FORMAT(JSON):\n    {\n      \"brandName\": \"Brand Name\",\n        \"productName\": \"Full Product Name\",\n          \"productDescription\": \"...\",\n            \"usp\": \"...\",\n              \"primaryPainPoint\": \"...\",\n                \"ctaLink\": \"${productUrl}\"\n    }\n    `,\n\n  productMapping: ({ productBrief, articleTopic, languageInstruction }: any) => `\n    I have a Product and an Article Topic.\n\n    <ProductDetails>\n    PRODUCT: ${productBrief.productName} (${productBrief.usp})\n    </ProductDetails>\n    DEFINITION: The product being marketed.\n    ACTION: Find features of this product that match the topic.\n    \n    <ArticleTopic>\n    TOPIC: ${articleTopic}\n    </ArticleTopic>\n    DEFINITION: The subject of the article.\n    ACTION: Identify pain points within this topic.\n    \n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: Target Language.\n    ACTION: Output in this language.\n\n    TASK:\n    Identify 3 - 5 \"Problem-Solution Mappings\".\n    For each mapping:\n    1. ** Pain Point **: A specific problem the reader has related to the Topic.\n    2. ** Product Feature **: The specific feature of the product that solves it.\n    3. ** Relevance Keywords **: List of keywords(from the topic) where this mapping is most relevant.\n    \n    OUTPUT JSON:\n    [\n      { \"painPoint\": \"...\", \"productFeature\": \"...\", \"relevanceKeywords\": [\"...\", \"...\"] }\n    ]\n      `,\n\n  brandSummary: ({ urls, languageInstruction }: any) => `\n    You are a web crawler and marketing copywriter.\n    Crawl and summarize the following URLs to extract the brand's own service/product details, contact info, and unique selling points.\n    \n    <TargetURLs>\n    ${urls.join('\\n')}\n    </TargetURLs>\n    DEFINITION: The list of pages to digest.\n    ACTION: Synthesize information from these pages only.\n    \n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: Target Language.\n    ACTION: Write the summary in this language.\n\n    OUTPUT: A concise paragraph(200 - 300 words) summarizing the brand / service with contact info if present.\n    `,\n\n  visualStyle: ({ languageInstruction, analyzedSamples, websiteType }: any) => `\n    I need to define a consistent \"Visual Identity\"(Master Style Prompt) for an article.\n    \n    <WebsiteContext>\n    ${websiteType}\n    </WebsiteContext>\n    DEFINITION: The type of business this is.\n    ACTION: Ensure style matches this industry.\n    \n    <SourceImageDescriptions>\n    ${analyzedSamples && analyzedSamples.length > 0 ? analyzedSamples : \"No source images available. Infer style strictly from Website Context.\"}\n    </SourceImageDescriptions>\n    DEFINITION: Descriptions of actual images on the site.\n    ACTION: Mimic this existing style.\n\n    TASK:\n    Synthesize a ** single, cohesive Visual Style Description** that I can append to every image prompt to ensure consistency.\n\n      Include:\n    1. ** Color Palette:** (e.g., \"Medical Blue #0055FF and Clean White\", or \"Warm Earth Tones\")\n    2. ** Lighting / Mood:** (e.g., \"Soft bright studio lighting\", \"Moody natural light\", \"Flat vector lighting\")\n    3. ** Art Medium:** (e.g., \"High-resolution Photography\", \"Minimalist 2D Vector Art\", \"3D Product Render\")\n    \n    OUTPUT FORMAT:\n    Return ONLY the style description string(max 30 words).\n  Example: \"Photorealistic style with soft daylight, using a clinical white and teal palette, high-end commercial aesthetic.\"\n    \n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: Output Language.\n    ACTION: Write the style description in English (for AI generation) but if instruction says otherwise, follow it.\n`,\n\n  snippet: ({ prompt, languageInstruction }: any) => `\n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: The language to use.\n    ACTION: Follow this instruction.\n\n    <InputPrompt>\n    ${prompt}\n    </InputPrompt>\n    DEFINITION: The logic or text to process.\n    ACTION: Execute this prompt.\n`,\n  sectionHeading: ({\n    sectionTitle,\n    articleTitle,\n    languageInstruction,\n    keyPoints,\n    keywordPlans,\n    narrativeNotes,\n  }: {\n    sectionTitle: string;\n    articleTitle: string;\n    languageInstruction: string;\n    keyPoints: string[];\n    keywordPlans: { word: string }[];\n    narrativeNotes?: string[];\n  }) => `\n    You are creating a concise H3 heading for an article section.\n\n    <ArticleTitle>\n    \"${articleTitle}\"\n    </ArticleTitle>\n    DEFINITION: The context of the article.\n    ACTION: Ensure heading aligns with article.\n\n    <SectionTitleContext>\n    \"${sectionTitle}\"\n    </SectionTitleContext>\n    DEFINITION: The H2 parent of this subheading.\n    ACTION: Ensure H3 is relevant to this H2.\n    \n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: Output Language.\n    ACTION: Write heading in this language.\n\n    HINTS:\n    <KeyPoints>\n    ${keyPoints.join('; ')}\n    </KeyPoints>\n    DEFINITION: Points that must be covered.\n    ACTION: Condense these into a short heading title.\n\n    <ImportantKeywords>\n    ${keywordPlans.map(k => k.word).join(', ')}\n    </ImportantKeywords>\n    DEFINITION: Keywords to include.\n    ACTION: Use these if they fit naturally.\n\n    <NarrativeNotes>\n    ${narrativeNotes?.join('; ') || 'None'}\n    </NarrativeNotes>\n    DEFINITION: Tone or angle note.\n    ACTION: Reflect this tone.\n\n    RULES:\n    - Return ONLY the heading text(no quotes, no numbering).\n        - Keep it under 10 words.\n    `,\n\n  batchRefineHeadings: ({\n    articleTitle,\n    headings,\n    languageInstruction,\n  }: {\n    articleTitle: string;\n    headings: string[];\n    languageInstruction: string;\n  }) => `\n    You are cleaning and unifying article section headings.\n\n    <ArticleTitle>\n    \"${articleTitle}\"\n    </ArticleTitle>\n    DEFINITION: The context of the article.\n    ACTION: Ensure headings fit this topic.\n\n    <OriginalHeadings>\n    ${headings.map((h, i) => `${i + 1}. ${h}`).join('\\n')}\n    </OriginalHeadings>\n    DEFINITION: The current raw headings.\n    ACTION: Refine these specific headings.\n\n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: Target Language.\n    ACTION: Write optimized headings in this language.\n\n    STEP BY STEP:\n    1) Clarify role: H2 = main section headline; H3 = supporting subpoint under its H2.\n    2) Keep it natural, concise, and engaging.Preserve the original intent / angle.Use everyday, conversational wording(生活化) instead of stiff or academic phrasing.\n    3) Make the H2 clickable: concise(≤ 60 chars).\n    4) Rewrite H2 with ** 5 options in this exact order ** (all must differ from h2_before):\n    - Option 1(經典版): Professional writer version - clear, informative.\n    - Option 2(編輯精選): Editor's pick - concise with strategic keywords.\n    - Option 3(吸睛版): Power words + emotional hooks(必學、秒懂、爆款、神級).\n    - Option 4(痛點版): Pain - point / FOMO angle - address fears or desires.\n    - Option 5(生活化): Lifestyle / conversational - like friend's advice, slang allowed.\n    - H2_after = your single best pick among those 5 options.\n    5) ** CRITICAL **: h2_after text MUST demonstrate what h2_reason describes.Example:\n    - If reason says \"使用生活化情境「下班回家」\", then h2_after MUST contain \"下班回家\".\n    - If reason says \"加入流行語「UP UP」\", then h2_after MUST contain \"UP UP\".\n    - Each option's text MUST match what its reason describes.\n    6) Rewrite / support H3(if any exist or if you invent them):\n    - Keep H3 ultra - compact: product / feature name or a 2 - 6 word keyword fragment.\n    - Align with the chosen H2_after without repeating it.\n    7) Validate: no duplicates, no vague fillers.Reject identical before / after.\n\n      OUTPUT(JSON only):\n    {\n      \"headings\": [\n        {\n          \"h2_before\": \"...\",\n          \"h2_after\": \"...\",\n          \"h2_reason\": \"why this angle/keywords\",\n          \"h2_options\": [\n            { \"text\": \"option 1\", \"reason\": \"reasoning\" },\n            { \"text\": \"option 2\", \"reason\": \"reasoning\" },\n            { \"text\": \"option 3\", \"reason\": \"reasoning\" }\n          ],\n          \"h3\": [\n            { \"h3_before\": \"...\", \"h3_after\": \"...\", \"h3_reason\": \"supporting angle & search phrasing\" }\n          ]\n        }\n      ]\n    }\n    - Array length MUST equal the number of ORIGINAL HEADINGS and keep the same order.\n        - Always include every field; set \"h3\": [] when none.\n        - \"h2_before\" must exactly match the provided heading text.\n        - If no H3s were provided, return \"h3\": [].\n        - If a heading is already good, repeat it in \"h2_after\".\n    `,\n\n  metaSeo: ({ targetAudience, contextLines, articlePreview }: { targetAudience: string; contextLines: string[]; articlePreview: string; }) => `\n    You are an SEO expert.Generate meta Title, Description, and URL slug for the article.\n\n    <TargetAudience>\n    ${targetAudience}\n    </TargetAudience>\n    DEFINITION: Region/Language target.\n    ACTION: Optimize for this audience's search habits.\n\n    <Context>\n    ${contextLines.join('\\n')}\n    </Context>\n    DEFINITION: Article content highlights.\n    ACTION: Base the meta tags on this content.\n\n    <ArticlePreview>\n    ${articlePreview}\n    </ArticlePreview>\n    DEFINITION: Snippet of the actual article.\n    ACTION: Ensure accuracy to the written text.\n\n    Output JSON:\n    { \"title\": \"...\", \"description\": \"...\", \"slug\": \"...\" }\n`,\n\n  rebrandContent: ({ productBrief, languageInstruction, currentContent }: any) => `\n    ## Task\n    REBRAND this article content.\n\n    DEFINITION: The primary goal of this prompt.\n    ACTION: Modify the provided content to reflect the new brand identity.\n    \n    <BrandIdentity>\n    - Name: \"${productBrief.brandName}\"\n    - Product: \"${productBrief.productName}\"\n    - USP: \"${productBrief.usp}\"\n    </BrandIdentity>\n    DEFINITION: The new brand identity to inject.\n    ACTION: Replace generic terms with these brand assets.\n\n    ## Instructions\n    1. Scan the text for generic terms like \"the device\", \"the treatment\", \"many clinics\", or any Competitor Names.\n    2. REWRITE those sentences to specifically feature ** ${productBrief.brandName}** or ** ${productBrief.productName}**.\n    3. Ensure the grammar flows naturally(Subject - Verb agreement).\n    4. Do NOT just find - replace.Rewrite the sentence to sound authoritative.\n    5. Maintain the original structure and formatting(Markdown).\n\n    DEFINITION: Detailed steps for rebranding.\n    ACTION: Follow these steps precisely.\n    \n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: Target Language.\n    ACTION: Maintain this language.\n\n    <ContentToRebrand>\n    ${currentContent}\n    </ContentToRebrand>\n    DEFINITION: The text needing update.\n    ACTION: Rewrite this text.\n`,\n\n  smartFindBlock: ({ pointToInject, blocks }: any) => `\n    I need to insert this Key Point: \n    <PointToInject>\n    \"${pointToInject}\"\n    </PointToInject>\n    DEFINITION: The fact that must be added.\n    ACTION: Find the best place for this fact.\n    \n    Here is a \"Compact Index\" of the article paragraphs:\n    <ArticleBlocks>\n    ${blocks.map((b: any) => `[ID: ${b.id}] ${b.text}`).join('\\n')}\n    </ArticleBlocks>\n    DEFINITION: List of paragraph candidates.\n    ACTION: Select one ID from this list.\n\nTASK: Identify the SINGLE Best Block ID to insert / merge this point into. \n    Return ONLY the ID(e.g. \"5\").\n    `,\n\n  smartRewriteBlock: ({ pointToInject, targetHtml, languageInstruction }: any) => `\nTASK: Rewrite the following HTML Block to naturally include this Key Point.\n    \n    <KeyPoint>\n    \"${pointToInject}\"\n    </KeyPoint>\n    DEFINITION: Information to weave in.\n    ACTION: Merge this into the text naturally.\n    \n    <TargetHTMLBlock>\n    ${targetHtml}\n    </TargetHTMLBlock>\n    DEFINITION: The existing paragraph.\n    ACTION: Rewrite this paragraph to include the point.\n\n    RULES:\n    1. Keep the original meaning and HTML tag structure(<p>or<li>).\n    2. Weave the point in naturally.Do not just append it at the end unless it fits.\n    3. <LanguageInstruction>${languageInstruction}</LanguageInstruction>\n    4. Return ONLY the new HTML string.\n    `,\n\n  productContextFromText: ({ rawText }: any) => `\n    ## Task\n    Extract product / service information from the following text.\n\n    DEFINITION: The main objective.\n    ACTION: Analyze the provided text to identify product-related data.\n\n    <RawText>\n    \"${rawText}\"\n    </RawText>\n    DEFINITION: Source text.\n    ACTION: Analyze this text for product data.\n\n    ## Instructions\n    - Identify Brand Name, Product Name, Unique Selling Point(USP), Target Audience, and Key Features.\n    - Focus on concise extraction, not rewriting.\n\n    DEFINITION: Guidelines for extraction.\n    ACTION: Follow these guidelines for the output.\n    \n    OUTPUT JSON:\n{\n  \"brandName\": \"...\",\n    \"productName\": \"...\",\n      \"usp\": \"...\",\n        \"targetAudience\": \"...\",\n          \"features\": [\"...\", \"...\"]\n}\n`,\n\n  authorityAnalysis: ({ languageInstruction, authorityTerms, websiteType, title }: any) => `\n    ## Task\n    Analyze the Authority Terms for this article and surface only the most credible, relevant ones.\n\n    DEFINITION: The main goal of this analysis.\n    ACTION: Filter and propose strategic combinations of terms.\n    \n    <WebsiteType>\n    ${websiteType}\n    </WebsiteType>\n    DEFINITION: The industry/niche.\n    ACTION: Judge credibility based on this context.\n\n    <ArticleTitle>\n    ${title}\n    </ArticleTitle>\n    DEFINITION: Topic context.\n    ACTION: Ensure terms are relevant to this topic.\n\n    <CandidateTerms>\n    ${authorityTerms}\n    </CandidateTerms>\n    DEFINITION: List of potential terms.\n    ACTION: Filter this list.\n    \n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: The language for the output.\n    ACTION: Provide the output in this language.\n\n    ## Goals\n    - Keep only terms that strengthen trust / credibility for this topic and site type.\n    - Drop vague, unrelated, or unverifiable claims.\n    - Propose strategic combinations of 2 - 3 terms to reinforce authority.\n\n    DEFINITION: Specific objectives for the analysis.\n    ACTION: Ensure the output meets these goals.\n    \n    OUTPUT JSON:\n    {\n          \"relevantTerms\": [\"best-fit term 1\", \"best-fit term 2\"],\n        \"combinations\": [\"term A + term B in intro\", \"term C in meta description\"]\n    }\n    Return JSON only.`,\n\n  narrativeStructure: ({ content, targetAudience, languageInstruction }: any) => `\n    Analyze the reference text to extract the Narrative Structure.\n    \n    1) The H1 title and introductory paragraph (first paragraph after H1).\n    2) A Logical Outline (H2 -> H3) with narrative goals.\n    3) Key information points (Facts) for each section.\n    4) Key information points (General) to preserve.\n    5) **Sentence Feature Analysis**:\n       - Analyze how sentences BEGIN in each section (common starters, transition words).\n       - Analyze how sentences END in each section (punctuation patterns, concluding intent, common closing phrases).\n\n    <TargetAudience>\n    ${targetAudience}\n    </TargetAudience>\n    DEFINITION: The target region.\n    \n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: Output language.\n\n    STRICT HEADING RULES:\n    - Enumerate EVERY H2 and its child H3s in order from the reference.\n    - Use the exact heading text as it appears (do NOT rewrite, paraphrase, translate, or renumber).\n    - Keep awkward wording or punctuation intact; only trim whitespace.\n    - If no clear headings exist, infer concise H2s, otherwise never replace existing ones.\n    \n    SECTION RELEVANCE FILTER:\n    - If a section title is IRRELEVANT to the main article topic (e.g., \"目錄\", \"導覽\", \"清單\", \"延伸閱讀\", \"相關文章\", unrelated sidebar), mark it with difficulty: \"unclear\" AND set \"exclude\": true.\n    - Specifically for \"目錄\" (Table of Contents), ALWAYS set \"exclude\": true.\n    - Excluded sections will be REMOVED from the generated outline.\n    - Only include sections that contribute to the main content.\n\n    MANDATORY KEY FACTS:\n    - For EVERY section you include, you MUST extract at least 3-5 \"keyFacts\" from the reference text.\n    - A \"keyFact\" is a specific, verifiable piece of information (numbers, technical specs, specific benefits).\n    - If you return a section with empty \"keyFacts\", your response will be rejected.\n    \n    ⚠️ NEGATIVE SIGNAL DETECTION(CRITICAL):\n    When analyzing each section, carefully identify NEGATIVE SIGNALS - content that should NOT be included in that section. These include:\n    - Phrases like \"不應在此處\", \"應於...提及\" (content that belongs elsewhere)\n    - Off-topic sidebars\n    \n        IMPORTANT: Any negative instruction or off-topic content MUST go into the \"suppress\" array.\n    \n    <ContentToAnalyze>\n    ${content}\n    </ContentToAnalyze>\n    DEFINITION: The reference text.\n\n    STRICT OUTPUT RULES:\n    1. For every active section (exclude: false), you MUST provide a detailed \"narrativePlan\" (3+ points) and \"keyFacts\" (2+ facts).\n    2. NEVER leave \"narrativePlan\", \"coreQuestion\", or \"keyFacts\" as empty arrays/strings for relevant sections.\n    3. If the reference content is sparse, use your expert knowledge to infer a logical plan and relevant facts that fit the topic.\n    \n    OUTPUT JSON:\n    {\n      \"h1Title\": \"Exact H1 title text\",\n      \"introText\": \"The first paragraph/intro text after H1 (if available)\",\n      \"structure\": [\n        {\n          \"title\": \"Exact H2 text (no rewrite)\",\n          \"subheadings\": [\"Exact H3 text 1\", \"Exact H3 text 2\"],\n          \"narrativePlan\": [\"Positive guidance 1\", \"Positive guidance 2\"],\n          \"coreQuestion\": \"Main question/problem\",\n          \"difficulty\": \"easy | medium | unclear\",\n          \"exclude\": false,\n          \"excludeReason\": \"Only set if exclude=true\",\n          \"writingMode\": \"direct | multi_solutions\",\n          \"solutionAngles\": [\"angle 1\", \"angle 2\"],\n          \"keyFacts\": [\"Fact 1\", \"Fact 2\"],\n          \"uspNotes\": [\"USP relevant to this section\"],\n          \"isChecklist\": false,\n          \"suppress\": [\"Negative constraint 1\"],\n          \"augment\": [\"Content to add\"],\n          \"sentenceStartFeatures\": [\"Observed patterns for sentence beginnings\"],\n          \"sentenceEndFeatures\": [\"Observed patterns for sentence endings (e.g., concludes with a question, uses specific punctuation, ends with a call to action)\"]\n        }\n      ],\n      \"keyInformationPoints\": [\"General key fact 1\", \"General key fact 2\"]\n    }\n    `,\n\n  voiceStrategy: ({ content, targetAudience, languageInstruction }: any) => `\n    Analyze the reference text to extract the Voice and Brand Strategy.\n\n    1) Voice & Tone (General Plan).\n    2) Conversion Strategy (Offers, CTAs, Risk Reversals).\n    3) Brand Exclusive Points (USP).\n    4) Competitor Names/Products to suppress. (CRITICAL: Do NOT list the region name \"${targetAudience}\" itself as a competitor).\n    5) **Regional Entities Detection**: CRITICAL - Identify ALL brands, stores, services, or entities that are SPECIFIC to a different region and NOT available in the target region \"${targetAudience}\".\n       - For zh-HK target: Detect Taiwan-specific brands (如：寶雅、全聯、momo購物、蝦皮台灣), Taiwan fashion brands, Taiwan chain stores.\n       - For zh-TW target: Detect Hong Kong-specific brands (如：HKTVmall、百佳、惠康、sasa), HK chain stores.\n       - These should be added to competitorBrands or competitorProducts even if they are not direct competitors.\n    6) Regional Validity (Brand Availability).\n    7) Human Writing Characteristics.\n\n    <TargetAudience>\n    ${targetAudience}\n    </TargetAudience>\n    DEFINITION: The target region. ALL entities not available in this region should be flagged as competitorBrands/Products.\n    \n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: Output language.\n\n    <ContentToAnalyze>\n    ${content}\n    </ContentToAnalyze>\n    DEFINITION: The reference text to analyze.\n\n    ## Voice Strategy Analysis\n    - ** regionVoiceDetect **: Analyze the \"Regional Voice Composition\" of the text (e.g., usage of regional slang like \"地道/貼地\" for HK vs \"接地氣\" for TW).\n      - Calculate an approximate PERCENTAGE breakdown.\n      - Return a string like: \"70% HK / 30% TW\" or \"100% TW\".\n      - If uncertain or neutral, return \"Neutral / Unclear\".\n    - ** humanWritingVoice **: Analyze the first 300 words (or the full Intro Paragraph). Explain \"Why does this sound HUMAN?\" by following these 5 steps:\n      1. **Emotions & Subjectivity**: Identify subjective judgments/cultural evaluations (unlike AI's objective tone).\n      2. **Tone & Particles**: Look for sentence-final particles (語助詞 like 喔, 唷) that create intimacy.\n      3. **Persona & Self-Ref**: Does the writer refer to themselves (e.g. \"Dr. X\") or assume a character?\n      4. **Cultural Metaphors**: Does it link facts to cultural concepts (e.g. physiognomy/fortune telling 面相) rather than just medical facts?\n      5. **Social Intent**: Is there a call for interaction (save/share) vs just providing info?\n      *Summarize these findings into a concise guide.*\n    - ** competitorBrands **: List ALL brands that should be avoided, including:\n      - Direct competitors mentioned in the text\n      - **Region-specific brands/stores NOT available in ${targetAudience}** (e.g., Taiwan brands 韌 REN, Fashion for Yes, 寶雅 when targeting Hong Kong)\n    - ** competitorProducts **: List ALL products that should be avoided or replaced.\n\n    Return JSON only, no extra text or markdown fences.`,\n\n  keywordAnalysis: ({ content, targetAudience, languageInstruction }: { content: string; targetAudience: TargetAudience; languageInstruction: string }) => `\n    Analyze the reference content to extract high - frequency keywords and their semantic roles.\n    \n    <TargetAudience>\n    ${targetAudience}\n    </TargetAudience>\n    DEFINITION: The target region.\n    ACTION: Factor this into keyword selection.\n    \n    <LanguageInstruction>\n    ${languageInstruction}\n    </LanguageInstruction>\n    DEFINITION: Output language.\n    ACTION: Use this language for keys/roles.\n\n    <Content>\n    ${content}\n    </Content>\n    DEFINITION: Source text.\n    ACTION: Extract keywords from here.\n    \n    OUTPUT JSON:\n    [\n      { \"word\": \"keyword\", \"contextSnippet\": \"snippet from text\", \"frequency\": 3 }\n    ]\n  `,\n\n  imagePromptFromContext: ({ contextText, languageInstruction, visualStyle, guide }: { contextText: string; languageInstruction: string; visualStyle: string; guide: string }) => `\n    Generate a detailed image generation prompt based on the following context.\n\n  <LanguageInstruction>\n  ${languageInstruction}\n  </LanguageInstruction>\n  DEFINITION: Output Language.\n  ACTION: Write the prompt in English unless specified.\n    \n    <ContextText>\n    ${contextText}\n    </ContextText>\n    DEFINITION: The scene usage context.\n    ACTION: Describe this scene.\n    \n    <VisualStyleGuide>\n    ${visualStyle}\n    </VisualStyleGuide>\n    DEFINITION: The artistic style.\n    ACTION: Apply these visual rules strictly.\n    \n    <SpecificGuide>\n    ${guide}\n    </SpecificGuide>\n    DEFINITION: User specific hints.\n    ACTION: Incorporate these hints.\n\nTASK:\n    Create a detailed, specific image generation prompt that:\n    1. Captures the essence of the context text\n    2. Adheres to the visual style guide\n    3. Is optimized for AI image generation\n    4. Avoids abstract concepts and focuses on concrete, photographable subjects\n    \n    Return ONLY the image prompt(no explanations or metadata).\n    `,\n\n  regionalBrandAnalysis: ({ content, targetAudience }: { content: string; targetAudience: string }) => `\n    TASK: Analyze the content for ** Regional Terminology ** and ** Brand Availability ** conflicts in: ${targetAudience}.\n\n    <TargetAudience>\n    ${targetAudience}\n    </TargetAudience>\n\n    Using Google Search (Grounding), verify:\n    1. ** Brand Availability **: Are mentioned brands/products actually available/popular in ${targetAudience}? (e.g. A Taiwan-only clinic appearing in a Hong Kong article is a mismatch).\n    2. ** Regional Vocabulary **: Replace obvious dialect terms (e.g. \"視頻\" -> \"影片\" for TW).\n\n    <ContentSnippet>\n    ${content.slice(0, 15000)}...\n    </ContentSnippet>\n\n    OUTPUT JSON:\n    [\n      { \"original\": \"Wrong Term\", \"replacement\": \"Correct Term\", \"reason\": \"Reason (e.g. 'Taiwan-only brand')\" }\n    ]\n    Return JSON only. If no issues, return [].\n    `,\n};\n\nexport type PromptTemplateKey = keyof typeof promptTemplates;\n"],"names":[],"mappings":";;;;AAKO,MAAM,kBAAkB;IAC7B,gBAAgB,CAAC,EACf,YAAY,EACZ,mBAAmB,EACnB,gBAAgB,EAChB,cAAc,EACd,WAAW,EACX,YAAY,EACZ,UAAU,EACV,YAAY,EACZ,iBAAiB,EACjB,MAAM,EACN,aAAa,EACb,YAAY,EACZ,YAAY,EACZ,UAAU,EACV,WAAW,EACX,cAAc,EACd,YAAY,EACZ,UAAU,EACV,aAAa,EACb,YAAY,EACZ,WAAW,EACX,kBAAkB,EAClB,iBAAiB,EACjB,iBAAiB,EACjB,gBAAgB,EA4BjB;QACC,MAAM,qBAAqB,cAAc;QACzC,MAAM,OAAO,eAAe,CAAC,uBAAuB,SAAS,WAAW,iBAAiB;QACzF,OAAO,CAAC;;MAEN,EAAE,aAAa;;;;;IAKjB,EAAE,oBAAoB;;;;;;IAMtB,EAAE,aAAa;;;;;IAKf,EAAE,iBAAiB,KAAK,CAAC,CAAC,GAAG,GAAG,CAAC,CAAC,IAAc,EAAE,SAAS,CAAC,GAAG,OAAO,OAAO,IAAI,CAAC,OAAO;;;;;IAKzF,EAAE,eAAe,IAAI,CAAC,MAAM;;;;;;;IAO5B,EAAE,aAAa,KAAK,SAAS,8BAA8B;;;;;IAK3D,EAAE,cAAc,KAAK,SAAS,qBAAqB;;;;;IAKnD,EAAE,WAAW,MAAM,GAAG,IAAI,WAAW,IAAI,CAAC,QAAQ,OAAO;;;;IAIzD,EAAE,oBAAoB,CAAC;;IAEvB,EAAE,kBAAkB;;;IAGpB,CAAC,GAAG,GAAG;;IAEP,EAAE,oBAAoB,CAAC;;IAEvB,EAAE,kBAAkB;;;;IAIpB,CAAC,GAAG,GAAG;;;;IAIP,EAAE,AAAC,oBAAoB,iBAAiB,MAAM,GAAG,KAAO,sBAAsB,mBAAmB,MAAM,GAAG,IAAK,CAAC;;IAEhH,EAAE,oBAAoB,iBAAiB,MAAM,GAAG,IAAI,CAAC;;IAErD,EAAE,iBAAiB,GAAG,CAAC,CAAA,IAAK,CAAC,IAAI,EAAE,GAAG,EAAE,IAAI,CAAC,MAAM;IACnD,CAAC,GAAG,GAAG;IACP,EAAE,sBAAsB,mBAAmB,MAAM,GAAG,IAAI,CAAC;;IAEzD,EAAE,mBAAmB,GAAG,CAAC,CAAA,IAAK,EAAE,WAAW,GACvC,CAAC,GAAG,EAAE,EAAE,QAAQ,CAAC,KAAK,EAAE,EAAE,WAAW,CAAC,CAAC,CAAC,GACxC,CAAC,GAAG,EAAE,EAAE,QAAQ,CAAC,sBAAsB,CAAC,EAC1C,IAAI,CAAC,MAAM;IACb,CAAC,GAAG,GAAG;;;;;;;IAOP,CAAC,GAAG,gCAAgC;;;;;IAKpC,EAAE,gBAAgB,2CAA2C;;;;;IAK7D,EAAE,mBAAmB;;;;;IAKrB,EAAE,SAAS,WAAW,wBAAwB,kCAAkC;;;;IAIhF,EAAE,SAAS,oBACL,CAAC,2FAA2F,CAAC,GAC7F,CAAC,0DAA0D,CAAC,CAC/D;;;;;IAKH,EAAE,iEAAiE,IAAI,CAAC,gBAClE,4IACA,GAAG;;;;;;;IAOT,EAAE,eAAe,cACX,sGACA,sDACH;;;;IAIH,EAAE,eAAe,YAAY,MAAM,GAAG,IAAI,CAAC;;IAE3C,EAAE,YAAY,GAAG,CAAC,CAAC,GAAG,IAAM,GAAG,IAAI,EAAE,EAAE,EAAE,GAAG,EAAE,IAAI,CAAC,MAAM;;;IAGzD,CAAC,GAAG,8BACD;;;;IAIH,EAAE,SAAS,oBACJ,kBAAkB,eAAe,MAAM,GAAG,IAAI,CAAC;;QAElD,EAAE,eAAe,IAAI,CAAC,MAAM;;;QAG5B,CAAC,GAAG,6CACF,8BACH;;;;;IAKH,EAAE,aAAa,GAAG,CAAC,CAAC,IAAW,CAAC;OAC7B,EAAE,EAAE,IAAI,CAAC,GAAG,EAAE,EAAE,IAAI,EAAE,KAAK,SAAS,kBAAkB,EAAE,IAAI,CAAC,IAAI;;;;;;IAMpE,EAAE,kBAAkB,KAAK,CAAC,GAAG,GAAG,IAAI,CAAC,MAAM;;;;;;;;IAQ3C,EAAE,OAAO,MAAM,GAAG,IAAI,OAAO,IAAI,CAAC,QAAQ,kEAAkE;;;;;;IAM5G,EAAE,cAAc;;;;IAIhB,EAAE,iBAAiB,cAAc,MAAM,GAAG,IAAI,CAAC;;IAE/C,EAAE,cAAc,GAAG,CAAC,CAAA,IAAK,CAAC,EAAE,EAAE,GAAG,EAAE,IAAI,CAAC,MAAM;;;IAG9C,CAAC,GAAG,SACD;;IAEH,EAAE,gBAAgB,aAAa,MAAM,GAAG,IAAI,CAAC;;IAE7C,EAAE,aAAa,GAAG,CAAC,CAAA,IAAK,CAAC,EAAE,EAAE,GAAG,EAAE,IAAI,CAAC,MAAM;;;IAG7C,CAAC,GAAG,GACD;;;;;;kCAM2B,EAAE,aAAa;;;;;AAKjD,CAAC;IACC;IAEA,gCAAgC,CAAC,EAC/B,mBAAmB,EACnB,qBAAqB,EAItB,GAAK,CAAC;;;;;;;;;MASH,EAAE,oBAAoB;;;;;;;;;;;;;;;IAexB,EAAE,sBAAsB;;;;;;;;;;;;;;;;;;IAkBxB,CAAC;IAEH,cAAc,CAAC,EAAE,WAAW,EAAE,UAAU,EAAE,mBAAmB,EAAO,GAAK,CAAC;;;;KAIvE,EAAE,YAAY;;;;;;KAMd,EAAE,WAAW;;;;;;IAMd,EAAE,oBAAoB;;;;;;;;;;;;;;;;;;4BAkBE,EAAE,WAAW;;IAErC,CAAC;IAEH,gBAAgB,CAAC,EAAE,YAAY,EAAE,YAAY,EAAE,mBAAmB,EAAO,GAAK,CAAC;;;;aAIpE,EAAE,aAAa,WAAW,CAAC,EAAE,EAAE,aAAa,GAAG,CAAC;;;;;;WAMlD,EAAE,aAAa;;;;;;IAMtB,EAAE,oBAAoB;;;;;;;;;;;;;;;;MAgBpB,CAAC;IAEL,cAAc,CAAC,EAAE,IAAI,EAAE,mBAAmB,EAAO,GAAK,CAAC;;;;;IAKrD,EAAE,KAAK,IAAI,CAAC,MAAM;;;;;;IAMlB,EAAE,oBAAoB;;;;;;IAMtB,CAAC;IAEH,aAAa,CAAC,EAAE,mBAAmB,EAAE,eAAe,EAAE,WAAW,EAAO,GAAK,CAAC;;;;IAI5E,EAAE,YAAY;;;;;;IAMd,EAAE,mBAAmB,gBAAgB,MAAM,GAAG,IAAI,kBAAkB,yEAAyE;;;;;;;;;;;;;;;;;;IAkB7I,EAAE,oBAAoB;;;;AAI1B,CAAC;IAEC,SAAS,CAAC,EAAE,MAAM,EAAE,mBAAmB,EAAO,GAAK,CAAC;;IAElD,EAAE,oBAAoB;;;;;;IAMtB,EAAE,OAAO;;;;AAIb,CAAC;IACC,gBAAgB,CAAC,EACf,YAAY,EACZ,YAAY,EACZ,mBAAmB,EACnB,SAAS,EACT,YAAY,EACZ,cAAc,EAQf,GAAK,CAAC;;;;KAIJ,EAAE,aAAa;;;;;;KAMf,EAAE,aAAa;;;;;;IAMhB,EAAE,oBAAoB;;;;;;;IAOtB,EAAE,UAAU,IAAI,CAAC,MAAM;;;;;;IAMvB,EAAE,aAAa,GAAG,CAAC,CAAA,IAAK,EAAE,IAAI,EAAE,IAAI,CAAC,MAAM;;;;;;IAM3C,EAAE,gBAAgB,KAAK,SAAS,OAAO;;;;;;;;IAQvC,CAAC;IAEH,qBAAqB,CAAC,EACpB,YAAY,EACZ,QAAQ,EACR,mBAAmB,EAKpB,GAAK,CAAC;;;;KAIJ,EAAE,aAAa;;;;;;IAMhB,EAAE,SAAS,GAAG,CAAC,CAAC,GAAG,IAAM,GAAG,IAAI,EAAE,EAAE,EAAE,GAAG,EAAE,IAAI,CAAC,MAAM;;;;;;IAMtD,EAAE,oBAAoB;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IAgDtB,CAAC;IAEH,SAAS,CAAC,EAAE,cAAc,EAAE,YAAY,EAAE,cAAc,EAA+E,GAAK,CAAC;;;;IAI3I,EAAE,eAAe;;;;;;IAMjB,EAAE,aAAa,IAAI,CAAC,MAAM;;;;;;IAM1B,EAAE,eAAe;;;;;;;AAOrB,CAAC;IAEC,gBAAgB,CAAC,EAAE,YAAY,EAAE,mBAAmB,EAAE,cAAc,EAAO,GAAK,CAAC;;;;;;;;aAQtE,EAAE,aAAa,SAAS,CAAC;gBACtB,EAAE,aAAa,WAAW,CAAC;YAC/B,EAAE,aAAa,GAAG,CAAC;;;;;;;0DAO2B,EAAE,aAAa,SAAS,CAAC,SAAS,EAAE,aAAa,WAAW,CAAC;;;;;;;;;IASnH,EAAE,oBAAoB;;;;;;IAMtB,EAAE,eAAe;;;;AAIrB,CAAC;IAEC,gBAAgB,CAAC,EAAE,aAAa,EAAE,MAAM,EAAO,GAAK,CAAC;;;KAGlD,EAAE,cAAc;;;;;;;IAOjB,EAAE,OAAO,GAAG,CAAC,CAAC,IAAW,CAAC,KAAK,EAAE,EAAE,EAAE,CAAC,EAAE,EAAE,EAAE,IAAI,EAAE,EAAE,IAAI,CAAC,MAAM;;;;;;;IAO/D,CAAC;IAEH,mBAAmB,CAAC,EAAE,aAAa,EAAE,UAAU,EAAE,mBAAmB,EAAO,GAAK,CAAC;;;;KAI9E,EAAE,cAAc;;;;;;IAMjB,EAAE,WAAW;;;;;;;;4BAQW,EAAE,oBAAoB;;IAE9C,CAAC;IAEH,wBAAwB,CAAC,EAAE,OAAO,EAAO,GAAK,CAAC;;;;;;;;KAQ5C,EAAE,QAAQ;;;;;;;;;;;;;;;;;;;;AAoBf,CAAC;IAEC,mBAAmB,CAAC,EAAE,mBAAmB,EAAE,cAAc,EAAE,WAAW,EAAE,KAAK,EAAO,GAAK,CAAC;;;;;;;;IAQxF,EAAE,YAAY;;;;;;IAMd,EAAE,MAAM;;;;;;IAMR,EAAE,eAAe;;;;;;IAMjB,EAAE,oBAAoB;;;;;;;;;;;;;;;;;;qBAkBL,CAAC;IAEpB,oBAAoB,CAAC,EAAE,OAAO,EAAE,cAAc,EAAE,mBAAmB,EAAO,GAAK,CAAC;;;;;;;;;;;;IAY9E,EAAE,eAAe;;;;;IAKjB,EAAE,oBAAoB;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IA6BtB,EAAE,QAAQ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IAmCV,CAAC;IAEH,eAAe,CAAC,EAAE,OAAO,EAAE,cAAc,EAAE,mBAAmB,EAAO,GAAK,CAAC;;;;;;sFAMS,EAAE,eAAe;sLAC+E,EAAE,eAAe;;;;;;;;IAQnM,EAAE,eAAe;;;;;IAKjB,EAAE,oBAAoB;;;;;IAKtB,EAAE,QAAQ;;;;;;;;;;;;;;;;;;yDAkB2C,EAAE,eAAe;;;uDAGnB,CAAC;IAEtD,iBAAiB,CAAC,EAAE,OAAO,EAAE,cAAc,EAAE,mBAAmB,EAAoF,GAAK,CAAC;;;;IAIxJ,EAAE,eAAe;;;;;;IAMjB,EAAE,oBAAoB;;;;;;IAMtB,EAAE,QAAQ;;;;;;;;;EASZ,CAAC;IAED,wBAAwB,CAAC,EAAE,WAAW,EAAE,mBAAmB,EAAE,WAAW,EAAE,KAAK,EAA4F,GAAK,CAAC;;;;EAIjL,EAAE,oBAAoB;;;;;;IAMpB,EAAE,YAAY;;;;;;IAMd,EAAE,YAAY;;;;;;IAMd,EAAE,MAAM;;;;;;;;;;;;;IAaR,CAAC;IAEH,uBAAuB,CAAC,EAAE,OAAO,EAAE,cAAc,EAA+C,GAAK,CAAC;wGACA,EAAE,eAAe;;;IAGrH,EAAE,eAAe;;;;6FAIwE,EAAE,eAAe;;;;IAI1G,EAAE,QAAQ,KAAK,CAAC,GAAG,OAAO;;;;;;;;IAQ1B,CAAC;AACL"}},
    {"offset": {"line": 1529, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/research/productFeatureToPainPointMapper.ts"],"sourcesContent":["\nimport { ServiceResponse, ProductBrief, ProblemProductMapping, TargetAudience } from '../../types';\nimport { calculateCost, getLanguageInstruction } from '../engine/promptService';\nimport { aiService } from '../engine/aiService';\nimport { Type } from '../engine/schemaTypes';\nimport { promptTemplates } from '../engine/promptTemplates';\nimport { MODEL } from '../../config/constants';\n\nexport const generateProductBrief = async (\n    productName: string,\n    productUrl: string,\n    targetAudience: TargetAudience\n): Promise<ServiceResponse<ProductBrief>> => {\n    const startTs = Date.now();\n    const languageInstruction = getLanguageInstruction(targetAudience);\n\n    // 1. Fetch Product Page Content (Mock/Proxy)\n    // In a real app, we'd use a server-side proxy to fetch the HTML.\n    // For now, we'll ask the AI to \"Infer\" based on the name/URL if it knows it, \n    // or we can pass a \"Context\" string if the user provided one.\n    // Assuming we rely on the AI's internal knowledge or the URL structure for now.\n\n    const prompt = promptTemplates.productBrief({ productName, productUrl, languageInstruction });\n\n    try {\n        const response = await aiService.runJson<ProductBrief>(\n            prompt,\n            'FLASH',\n            {\n                type: Type.OBJECT,\n                properties: {\n                    brandName: { type: Type.STRING },\n                    productName: { type: Type.STRING },\n                    productDescription: { type: Type.STRING },\n                    usp: { type: Type.STRING },\n                    primaryPainPoint: { type: Type.STRING },\n                    ctaLink: { type: Type.STRING }\n                },\n                required: [\"brandName\", \"productName\", \"usp\", \"ctaLink\"]\n            }\n        );\n\n        // Fill in defaults if missing\n        const data = response.data;\n        const finalData: ProductBrief = {\n            brandName: data.brandName || \"Brand\",\n            productName: data.productName || productName,\n            usp: data.usp || \"\",\n            ctaLink: data.ctaLink || productUrl,\n            targetPainPoints: (data as any).primaryPainPoint // Map if exists\n        };\n\n        return {\n            data: finalData,\n            usage: response.usage,\n            cost: response.cost,\n            duration: response.duration\n        };\n\n    } catch (e) {\n        console.error(\"Product Brief Generation Failed\", e);\n        return {\n            data: { brandName: \"\", productName: productName, usp: \"\", ctaLink: productUrl },\n            usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: Date.now() - startTs\n        };\n    }\n};\n\nexport const mapProblemsToProduct = async (\n    productBrief: ProductBrief,\n    articleTopic: string,\n    targetAudience: TargetAudience\n): Promise<ServiceResponse<ProblemProductMapping[]>> => {\n    const startTs = Date.now();\n    const languageInstruction = getLanguageInstruction(targetAudience);\n\n    const prompt = promptTemplates.productMapping({ productBrief, articleTopic, languageInstruction });\n\n    try {\n        const response = await aiService.runJson<ProblemProductMapping[]>(\n            prompt,\n            'FLASH',\n            {\n                type: Type.ARRAY,\n                items: {\n                    type: Type.OBJECT,\n                    properties: {\n                        painPoint: { type: Type.STRING },\n                        productFeature: { type: Type.STRING },\n                        relevanceKeywords: { type: Type.ARRAY, items: { type: Type.STRING } },\n                    },\n                    required: [\"painPoint\", \"productFeature\", \"relevanceKeywords\"]\n                },\n            }\n        );\n\n        return {\n            data: response.data,\n            usage: response.usage,\n            cost: response.cost,\n            duration: response.duration\n        };\n\n    } catch (e) {\n        console.error(\"Brand Content Summarization Failed\", e);\n        return {\n            data: [],\n            usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: Date.now() - startTs\n        };\n    }\n};\n\n// Parse product context from raw text\nexport const parseProductContext = async (\n    rawText: string\n): Promise<ServiceResponse<ProductBrief>> => {\n    const startTs = Date.now();\n\n    const prompt = promptTemplates.productContextFromText({ rawText });\n\n    try {\n        const response = await aiService.runJson<ProductBrief>(\n            prompt,\n            'FLASH',\n            {\n                type: Type.OBJECT,\n                properties: {\n                    brandName: { type: Type.STRING },\n                    productName: { type: Type.STRING },\n                    usp: { type: Type.STRING },\n                    primaryPainPoint: { type: Type.STRING },\n                    ctaLink: { type: Type.STRING }\n                },\n                required: [\"brandName\", \"productName\", \"usp\", \"ctaLink\"]\n            }\n        );\n\n        const data = response.data;\n        return {\n            data: {\n                brandName: data.brandName || \"\",\n                productName: data.productName || \"\",\n                usp: data.usp || \"\",\n                ctaLink: data.ctaLink || \"\",\n                targetPainPoints: (data as any).primaryPainPoint\n            },\n            usage: response.usage,\n            cost: response.cost,\n            duration: response.duration\n        };\n\n    } catch (e) {\n        console.error(\"Product Context Parsing Failed\", e);\n        return {\n            data: { brandName: \"\", productName: \"\", usp: \"\", ctaLink: \"\" },\n            usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: Date.now() - startTs\n        };\n    }\n};\n\n// Alias for backward compatibility with App.tsx\nexport const generateProblemProductMapping = async (\n    productBrief: ProductBrief,\n    targetAudience: TargetAudience,\n    articleTopic: string = \"General Content\"\n): Promise<ServiceResponse<ProblemProductMapping[]>> => {\n    return mapProblemsToProduct(productBrief, articleTopic, targetAudience);\n};\n\nexport const summarizeBrandContent = async (\n    urls: string[],\n    targetAudience: TargetAudience\n): Promise<ServiceResponse<string>> => {\n    const startTs = Date.now();\n    const languageInstruction = getLanguageInstruction(targetAudience);\n\n    const prompt = promptTemplates.brandSummary({ urls, languageInstruction });\n\n    try {\n        const response = await aiService.runText(prompt, 'FLASH');\n\n        return {\n            data: response.text,\n            usage: response.usage,\n            cost: response.cost,\n            duration: response.duration\n        };\n    } catch (e) {\n        console.error(\"Brand Summary Failed\", e);\n        return {\n            data: \"\",\n            usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: Date.now() - startTs\n        };\n    }\n};\n"],"names":[],"mappings":";;;;;;;;;;;;AAEA;AACA;AACA;AACA;;;;;AAGO,MAAM,uBAAuB,OAChC,aACA,YACA;IAEA,MAAM,UAAU,KAAK,GAAG;IACxB,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IAEnD,6CAA6C;IAC7C,iEAAiE;IACjE,8EAA8E;IAC9E,8DAA8D;IAC9D,gFAAgF;IAEhF,MAAM,SAAS,+JAAe,CAAC,YAAY,CAAC;QAAE;QAAa;QAAY;IAAoB;IAE3F,IAAI;QACA,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CACpC,QACA,SACA;YACI,MAAM,gJAAI,CAAC,MAAM;YACjB,YAAY;gBACR,WAAW;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;gBAC/B,aAAa;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;gBACjC,oBAAoB;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;gBACxC,KAAK;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;gBACzB,kBAAkB;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;gBACtC,SAAS;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;YACjC;YACA,UAAU;gBAAC;gBAAa;gBAAe;gBAAO;aAAU;QAC5D;QAGJ,8BAA8B;QAC9B,MAAM,OAAO,SAAS,IAAI;QAC1B,MAAM,YAA0B;YAC5B,WAAW,KAAK,SAAS,IAAI;YAC7B,aAAa,KAAK,WAAW,IAAI;YACjC,KAAK,KAAK,GAAG,IAAI;YACjB,SAAS,KAAK,OAAO,IAAI;YACzB,kBAAkB,AAAC,KAAa,gBAAgB,CAAC,gBAAgB;QACrE;QAEA,OAAO;YACH,MAAM;YACN,OAAO,SAAS,KAAK;YACrB,MAAM,SAAS,IAAI;YACnB,UAAU,SAAS,QAAQ;QAC/B;IAEJ,EAAE,OAAO,GAAG;QACR,QAAQ,KAAK,CAAC,mCAAmC;QACjD,OAAO;YACH,MAAM;gBAAE,WAAW;gBAAI,aAAa;gBAAa,KAAK;gBAAI,SAAS;YAAW;YAC9E,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YACzD,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU,KAAK,GAAG,KAAK;QAC3B;IACJ;AACJ;AAEO,MAAM,uBAAuB,OAChC,cACA,cACA;IAEA,MAAM,UAAU,KAAK,GAAG;IACxB,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IAEnD,MAAM,SAAS,+JAAe,CAAC,cAAc,CAAC;QAAE;QAAc;QAAc;IAAoB;IAEhG,IAAI;QACA,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CACpC,QACA,SACA;YACI,MAAM,gJAAI,CAAC,KAAK;YAChB,OAAO;gBACH,MAAM,gJAAI,CAAC,MAAM;gBACjB,YAAY;oBACR,WAAW;wBAAE,MAAM,gJAAI,CAAC,MAAM;oBAAC;oBAC/B,gBAAgB;wBAAE,MAAM,gJAAI,CAAC,MAAM;oBAAC;oBACpC,mBAAmB;wBAAE,MAAM,gJAAI,CAAC,KAAK;wBAAE,OAAO;4BAAE,MAAM,gJAAI,CAAC,MAAM;wBAAC;oBAAE;gBACxE;gBACA,UAAU;oBAAC;oBAAa;oBAAkB;iBAAoB;YAClE;QACJ;QAGJ,OAAO;YACH,MAAM,SAAS,IAAI;YACnB,OAAO,SAAS,KAAK;YACrB,MAAM,SAAS,IAAI;YACnB,UAAU,SAAS,QAAQ;QAC/B;IAEJ,EAAE,OAAO,GAAG;QACR,QAAQ,KAAK,CAAC,sCAAsC;QACpD,OAAO;YACH,MAAM,EAAE;YACR,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YACzD,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU,KAAK,GAAG,KAAK;QAC3B;IACJ;AACJ;AAGO,MAAM,sBAAsB,OAC/B;IAEA,MAAM,UAAU,KAAK,GAAG;IAExB,MAAM,SAAS,+JAAe,CAAC,sBAAsB,CAAC;QAAE;IAAQ;IAEhE,IAAI;QACA,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CACpC,QACA,SACA;YACI,MAAM,gJAAI,CAAC,MAAM;YACjB,YAAY;gBACR,WAAW;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;gBAC/B,aAAa;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;gBACjC,KAAK;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;gBACzB,kBAAkB;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;gBACtC,SAAS;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;YACjC;YACA,UAAU;gBAAC;gBAAa;gBAAe;gBAAO;aAAU;QAC5D;QAGJ,MAAM,OAAO,SAAS,IAAI;QAC1B,OAAO;YACH,MAAM;gBACF,WAAW,KAAK,SAAS,IAAI;gBAC7B,aAAa,KAAK,WAAW,IAAI;gBACjC,KAAK,KAAK,GAAG,IAAI;gBACjB,SAAS,KAAK,OAAO,IAAI;gBACzB,kBAAkB,AAAC,KAAa,gBAAgB;YACpD;YACA,OAAO,SAAS,KAAK;YACrB,MAAM,SAAS,IAAI;YACnB,UAAU,SAAS,QAAQ;QAC/B;IAEJ,EAAE,OAAO,GAAG;QACR,QAAQ,KAAK,CAAC,kCAAkC;QAChD,OAAO;YACH,MAAM;gBAAE,WAAW;gBAAI,aAAa;gBAAI,KAAK;gBAAI,SAAS;YAAG;YAC7D,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YACzD,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU,KAAK,GAAG,KAAK;QAC3B;IACJ;AACJ;AAGO,MAAM,gCAAgC,OACzC,cACA,gBACA,eAAuB,iBAAiB;IAExC,OAAO,qBAAqB,cAAc,cAAc;AAC5D;AAEO,MAAM,wBAAwB,OACjC,MACA;IAEA,MAAM,UAAU,KAAK,GAAG;IACxB,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IAEnD,MAAM,SAAS,+JAAe,CAAC,YAAY,CAAC;QAAE;QAAM;IAAoB;IAExE,IAAI;QACA,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CAAC,QAAQ;QAEjD,OAAO;YACH,MAAM,SAAS,IAAI;YACnB,OAAO,SAAS,KAAK;YACrB,MAAM,SAAS,IAAI;YACnB,UAAU,SAAS,QAAQ;QAC/B;IACJ,EAAE,OAAO,GAAG;QACR,QAAQ,KAAK,CAAC,wBAAwB;QACtC,OAAO;YACH,MAAM;YACN,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YACzD,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU,KAAK,GAAG,KAAK;QAC3B;IACJ;AACJ"}},
    {"offset": {"line": 1796, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/research/webScraper.ts"],"sourcesContent":["\nimport { ScrapedImage } from '../../types';\n/**\n * Web Scraper Service\n * Uses Jina AI Reader (https://jina.ai/reader) to turn URLs into LLM-friendly Markdown.\n */\n\n// Added options interface\ninterface FetchOptions {\n    includeNav?: boolean;\n}\n\nexport const fetchUrlContent = async (url: string, options: FetchOptions = {}): Promise<{ title: string, content: string, images: ScrapedImage[] }> => {\n    if (!url) return { title: '', content: '', images: [] };\n\n    try {\n        new URL(url);\n    } catch (_) {\n        throw new Error(\"Invalid URL format\");\n    }\n\n    try {\n        // Construct headers, optionally including/excluding nav\n        const headers: Record<string, string> = {\n            'x-no-cache': 'false', // bypass cached content\n            'X-Md-Heading-Style': 'setext', // setext headings (=== for H1, --- for H2)\n        };\n\n        if (!options.includeNav) {\n            // Default behavior: Remove nav, header, footer for article reading\n            headers['X-Remove-Selector'] = 'header, footer, nav, aside';\n        } else {\n            // If includeNav is true, we might still want to remove ads/sidebars but keep header/footer\n            headers['X-Remove-Selector'] = 'aside'; // Keep header/footer/nav for contact info\n        }\n\n        const response = await fetch(`https://r.jina.ai/${url}`, {\n            method: 'GET',\n            headers: headers\n        });\n\n        if (!response.ok) {\n            throw new Error(`Failed to fetch content: ${response.statusText}`);\n        }\n\n        const rawText = await response.text();\n\n        if (!rawText || rawText.length < 50) {\n            throw new Error(\"Content retrieved is too short or empty.\");\n        }\n\n        const cleaned = cleanJinaBySeparator(rawText);\n\n        return { ...cleaned };\n\n    } catch (error: any) {\n        console.error(\"Web Scraping Error:\", error);\n        throw new Error(error.message || \"Failed to scrape URL\");\n    }\n};\n\nconst extractImagesFromContent = (text: string, range: number = 50): ScrapedImage[] => {\n    // Regex to capture ![alt](url)\n    const regex = /!\\[(.*?)\\]\\((.*?)\\)/g;\n    let match;\n    const rawMatches: { index: number; length: number; image: ScrapedImage }[] = [];\n\n    while ((match = regex.exec(text)) !== null) {\n        const fullMatchStr = match[0];\n        const altText = match[1];\n        const url = match[2];\n        const matchIndex = match.index;\n        const matchLength = fullMatchStr.length;\n\n        const startPos = Math.max(0, matchIndex - range);\n        const endPos = Math.min(text.length, matchIndex + matchLength + range);\n\n        const preContext = text.substring(startPos, matchIndex).trim();\n        const postContext = text.substring(matchIndex + matchLength, endPos).trim();\n\n        rawMatches.push({\n            index: matchIndex,\n            length: matchLength,\n            image: {\n                url,\n                preContext,\n                altText,\n                postContext\n            }\n        });\n    }\n\n    if (rawMatches.length <= 10) {\n        return rawMatches.map(item => item.image);\n    }\n\n    const filteredImages: ScrapedImage[] = [];\n    let lastEndPos = -1;\n\n    for (const item of rawMatches) {\n        if (lastEndPos === -1) {\n            filteredImages.push(item.image);\n            lastEndPos = item.index + item.length;\n            continue;\n        }\n\n        const textBetween = text.substring(lastEndPos, item.index);\n        const meaningfulContent = textBetween.replace(/\\s/g, '');\n\n        if (meaningfulContent.length >= 30) {\n            filteredImages.push(item.image);\n            lastEndPos = item.index + item.length;\n        }\n    }\n\n    return filteredImages;\n};\n\nconst cleanJinaBySeparator = (rawText: string): { title: string, content: string, images: ScrapedImage[] } => {\n    const titleMatch = rawText.match(/^Title:\\s*(.+)$/m);\n    const title = titleMatch ? titleMatch[1].trim() : '';\n\n    let contentBody = rawText;\n    if (rawText.includes('Markdown Content:')) {\n        const parts = rawText.split('Markdown Content:');\n        contentBody = parts.slice(1).join('Markdown Content:');\n    }\n\n    // Preserve heading structure: convert setext (=== / ---) to ATX (# / ##)\n    let cleanContent = convertSetextToAtx(contentBody.trim());\n\n    // Extract images BEFORE cleaning them out\n    const images = extractImagesFromContent(cleanContent);\n\n    // Apply cleaning\n    cleanContent = cleanArtifacts(cleanContent);\n\n    if (title && !cleanContent.startsWith('#')) {\n        cleanContent = `# ${title}\\n\\n${cleanContent}`;\n    }\n\n    return { title, content: cleanContent, images };\n};\n\n// Convert setext headings (text + === / ---) into ATX (# / ##) so they survive later cleanup.\n// Skip fenced code blocks to avoid rewriting literal divider lines inside code samples.\nconst convertSetextToAtx = (text: string): string => {\n    const lines = text.split('\\n');\n    const out: string[] = [];\n    let inFence = false;\n\n    for (let i = 0; i < lines.length; i++) {\n        const line = lines[i];\n\n        // Toggle code fence state\n        if (/^\\s*(```|~~~)/.test(line.trim())) {\n            inFence = !inFence;\n            out.push(line);\n            continue;\n        }\n\n        if (!inFence && i + 1 < lines.length) {\n            const underline = lines[i + 1];\n            const match = underline.match(/^\\s*(=+|-+)\\s*$/);\n            const hasText = line.trim().length > 0;\n            const prevIsBlank = i === 0 || lines[i - 1].trim() === '';\n\n            if (match && hasText && prevIsBlank) {\n                const level = match[1].startsWith('=') ? '#' : '##';\n                out.push(`${level} ${line.trim()}`);\n                i++; // Skip underline line\n                continue;\n            }\n        }\n\n        out.push(line);\n    }\n\n    return out.join('\\n');\n};\n\n/**\n * FIXED: Optimized Cleaning Logic\n * Corrects issues with leftover image markdown, broken links, and specific UI junk.\n */\nconst cleanArtifacts = (text: string): string => {\n    let cleaned = text;\n\n    // ============================================================\n    // 1. Specific Junk Phrase Removal (Requested User Rules)\n    // ============================================================\n\n    const junkPhrases = [\n        /^Ad Placement\\s*:.*$/gim,        // Remove \"Ad Placement : xxxx\" lines\n        /^(Login|登入|Sign In).*$/gim,    // Remove lines starting with Login/登入\n        /^ADVERTISEMENT$/gim,             // Remove strict \"ADVERTISEMENT\" lines\n        /^CONTINUE READING BELOW$/gim,    // Remove \"CONTINUE READING BELOW\"\n        /^Share on:.*$/gim,               // Remove \"Share on: ...\" lines\n        /^recommended$/gim,               // Remove standalone \"recommended\" lines\n        /^Related Articles:?$/gim,        // Common noise\n        /^Read More:?$/gim,                // Common noise\n        /^SCROLL TO CONTINUE\\s*:.*$/gim,\n        /^[ \\t]*\\S{1,2}[ \\t]*$/gm         // Remove lines with < 3 chars (e.g. \"US\", \"Go\", \"|\", \"。\")\n    ];\n\n    junkPhrases.forEach(regex => {\n        cleaned = cleaned.replace(regex, '');\n    });\n\n    // ============================================================\n    // 2. Image Cleanup (Prioritized)\n    // ============================================================\n\n    // Fix: Remove standard markdown images `![Alt](Url)`\n    cleaned = cleaned.replace(/!\\[.*?\\]\\(.*?\\)/g, '');\n\n    // Jina non-standard image artifacts cleanup\n    cleaned = cleaned.replace(/^!Image\\s+\\d+:.*$/gm, '');\n    cleaned = cleaned.replace(/!Image\\s*\\[.*?\\]/gi, '');\n\n    // Remove orphaned closing link syntax often left behind\n    cleaned = cleaned.replace(/^\\]\\(.*?\\)/gm, '');\n\n\n    // ============================================================\n    // 3. Link Density Filter\n    // ============================================================\n\n    const linkRegex = /\\[(.*?)\\]\\(.*?\\)/g;\n    const linkMatches: { index: number, length: number }[] = [];\n    let lMatch;\n\n    while ((lMatch = linkRegex.exec(cleaned)) !== null) {\n        linkMatches.push({ index: lMatch.index, length: lMatch[0].length });\n    }\n\n    if (linkMatches.length > 6) {\n        const indicesToRemove: { start: number, end: number }[] = [];\n        let lastValidEnd = -1;\n\n        for (let i = 0; i < linkMatches.length; i++) {\n            const m = linkMatches[i];\n            const mStart = m.index;\n            const mEnd = mStart + m.length;\n\n            if (i === 0) {\n                lastValidEnd = mEnd;\n                continue;\n            }\n\n            const textBetween = cleaned.substring(lastValidEnd, mStart);\n            if (textBetween.replace(/\\s/g, '').length < 30) {\n                indicesToRemove.push({ start: mStart, end: mEnd });\n            } else {\n                lastValidEnd = mEnd;\n            }\n        }\n\n        // Reverse loop removal to keep indices valid\n        for (let i = indicesToRemove.length - 1; i >= 0; i--) {\n            const range = indicesToRemove[i];\n            cleaned = cleaned.substring(0, range.start) + cleaned.substring(range.end);\n        }\n    }\n\n    // ============================================================\n    // 4. General Link Cleaning\n    // ============================================================\n\n    // Remove empty links\n    cleaned = cleaned.replace(/\\[\\s*\\]\\(.*?\\)/g, '');\n\n    // Remove resulting empty list items\n    cleaned = cleaned.replace(/^\\s*([-*]|\\d+\\.)\\s*$/gm, '');\n\n    // Flatten Links: Convert `[Text](Url)` to `Text`\n    cleaned = cleaned.replace(/\\[([^\\]]+)\\]\\([^\\)]+\\)/g, '$1');\n\n    // ============================================================\n    // 5. Noise & Metadata Cleanup\n    // ============================================================\n\n    // Google Analytics / Ads artifacts\n    cleaned = cleaned.replace(/^\\s*(UA-\\d+-\\d+|G-[A-Z0-9]+)\\s*$/gm, '');\n\n    // Common noise words (Case insensitive)\n    cleaned = cleaned.replace(/^(holiday|girlstyle|businessfocus|mamidaily)\\s*$/gim, '');\n\n    // HK style \"All Chinese\" navs\n    cleaned = cleaned.replace(/^All\\s+[\\u4e00-\\u9fa5]+.*$/gm, '');\n\n    // User-requested noise lines between content\n    cleaned = cleaned.replace(/^\\s*-{3,}\\s*$/gm, ''); // ----- separators\n    cleaned = cleaned.replace(/^\\s*\\u25b2?\\s*Cosmopolitan\\.com\\.hk\\s*$/gim, ''); // Cosmopolitan.com.hk with optional ▲\n\n    // Final Whitespace Cleanup\n    cleaned = cleaned.replace(/\\n{3,}/g, '\\n\\n').trim();\n\n    return cleaned;\n};\n"],"names":[],"mappings":";;;;AAYO,MAAM,kBAAkB,OAAO,KAAa,UAAwB,CAAC,CAAC;IACzE,IAAI,CAAC,KAAK,OAAO;QAAE,OAAO;QAAI,SAAS;QAAI,QAAQ,EAAE;IAAC;IAEtD,IAAI;QACA,IAAI,IAAI;IACZ,EAAE,OAAO,GAAG;QACR,MAAM,IAAI,MAAM;IACpB;IAEA,IAAI;QACA,wDAAwD;QACxD,MAAM,UAAkC;YACpC,cAAc;YACd,sBAAsB;QAC1B;QAEA,IAAI,CAAC,QAAQ,UAAU,EAAE;YACrB,mEAAmE;YACnE,OAAO,CAAC,oBAAoB,GAAG;QACnC,OAAO;YACH,2FAA2F;YAC3F,OAAO,CAAC,oBAAoB,GAAG,SAAS,0CAA0C;QACtF;QAEA,MAAM,WAAW,MAAM,MAAM,CAAC,kBAAkB,EAAE,KAAK,EAAE;YACrD,QAAQ;YACR,SAAS;QACb;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YACd,MAAM,IAAI,MAAM,CAAC,yBAAyB,EAAE,SAAS,UAAU,EAAE;QACrE;QAEA,MAAM,UAAU,MAAM,SAAS,IAAI;QAEnC,IAAI,CAAC,WAAW,QAAQ,MAAM,GAAG,IAAI;YACjC,MAAM,IAAI,MAAM;QACpB;QAEA,MAAM,UAAU,qBAAqB;QAErC,OAAO;YAAE,GAAG,OAAO;QAAC;IAExB,EAAE,OAAO,OAAY;QACjB,QAAQ,KAAK,CAAC,uBAAuB;QACrC,MAAM,IAAI,MAAM,MAAM,OAAO,IAAI;IACrC;AACJ;AAEA,MAAM,2BAA2B,CAAC,MAAc,QAAgB,EAAE;IAC9D,+BAA+B;IAC/B,MAAM,QAAQ;IACd,IAAI;IACJ,MAAM,aAAuE,EAAE;IAE/E,MAAO,CAAC,QAAQ,MAAM,IAAI,CAAC,KAAK,MAAM,KAAM;QACxC,MAAM,eAAe,KAAK,CAAC,EAAE;QAC7B,MAAM,UAAU,KAAK,CAAC,EAAE;QACxB,MAAM,MAAM,KAAK,CAAC,EAAE;QACpB,MAAM,aAAa,MAAM,KAAK;QAC9B,MAAM,cAAc,aAAa,MAAM;QAEvC,MAAM,WAAW,KAAK,GAAG,CAAC,GAAG,aAAa;QAC1C,MAAM,SAAS,KAAK,GAAG,CAAC,KAAK,MAAM,EAAE,aAAa,cAAc;QAEhE,MAAM,aAAa,KAAK,SAAS,CAAC,UAAU,YAAY,IAAI;QAC5D,MAAM,cAAc,KAAK,SAAS,CAAC,aAAa,aAAa,QAAQ,IAAI;QAEzE,WAAW,IAAI,CAAC;YACZ,OAAO;YACP,QAAQ;YACR,OAAO;gBACH;gBACA;gBACA;gBACA;YACJ;QACJ;IACJ;IAEA,IAAI,WAAW,MAAM,IAAI,IAAI;QACzB,OAAO,WAAW,GAAG,CAAC,CAAA,OAAQ,KAAK,KAAK;IAC5C;IAEA,MAAM,iBAAiC,EAAE;IACzC,IAAI,aAAa,CAAC;IAElB,KAAK,MAAM,QAAQ,WAAY;QAC3B,IAAI,eAAe,CAAC,GAAG;YACnB,eAAe,IAAI,CAAC,KAAK,KAAK;YAC9B,aAAa,KAAK,KAAK,GAAG,KAAK,MAAM;YACrC;QACJ;QAEA,MAAM,cAAc,KAAK,SAAS,CAAC,YAAY,KAAK,KAAK;QACzD,MAAM,oBAAoB,YAAY,OAAO,CAAC,OAAO;QAErD,IAAI,kBAAkB,MAAM,IAAI,IAAI;YAChC,eAAe,IAAI,CAAC,KAAK,KAAK;YAC9B,aAAa,KAAK,KAAK,GAAG,KAAK,MAAM;QACzC;IACJ;IAEA,OAAO;AACX;AAEA,MAAM,uBAAuB,CAAC;IAC1B,MAAM,aAAa,QAAQ,KAAK,CAAC;IACjC,MAAM,QAAQ,aAAa,UAAU,CAAC,EAAE,CAAC,IAAI,KAAK;IAElD,IAAI,cAAc;IAClB,IAAI,QAAQ,QAAQ,CAAC,sBAAsB;QACvC,MAAM,QAAQ,QAAQ,KAAK,CAAC;QAC5B,cAAc,MAAM,KAAK,CAAC,GAAG,IAAI,CAAC;IACtC;IAEA,yEAAyE;IACzE,IAAI,eAAe,mBAAmB,YAAY,IAAI;IAEtD,0CAA0C;IAC1C,MAAM,SAAS,yBAAyB;IAExC,iBAAiB;IACjB,eAAe,eAAe;IAE9B,IAAI,SAAS,CAAC,aAAa,UAAU,CAAC,MAAM;QACxC,eAAe,CAAC,EAAE,EAAE,MAAM,IAAI,EAAE,cAAc;IAClD;IAEA,OAAO;QAAE;QAAO,SAAS;QAAc;IAAO;AAClD;AAEA,8FAA8F;AAC9F,wFAAwF;AACxF,MAAM,qBAAqB,CAAC;IACxB,MAAM,QAAQ,KAAK,KAAK,CAAC;IACzB,MAAM,MAAgB,EAAE;IACxB,IAAI,UAAU;IAEd,IAAK,IAAI,IAAI,GAAG,IAAI,MAAM,MAAM,EAAE,IAAK;QACnC,MAAM,OAAO,KAAK,CAAC,EAAE;QAErB,0BAA0B;QAC1B,IAAI,gBAAgB,IAAI,CAAC,KAAK,IAAI,KAAK;YACnC,UAAU,CAAC;YACX,IAAI,IAAI,CAAC;YACT;QACJ;QAEA,IAAI,CAAC,WAAW,IAAI,IAAI,MAAM,MAAM,EAAE;YAClC,MAAM,YAAY,KAAK,CAAC,IAAI,EAAE;YAC9B,MAAM,QAAQ,UAAU,KAAK,CAAC;YAC9B,MAAM,UAAU,KAAK,IAAI,GAAG,MAAM,GAAG;YACrC,MAAM,cAAc,MAAM,KAAK,KAAK,CAAC,IAAI,EAAE,CAAC,IAAI,OAAO;YAEvD,IAAI,SAAS,WAAW,aAAa;gBACjC,MAAM,QAAQ,KAAK,CAAC,EAAE,CAAC,UAAU,CAAC,OAAO,MAAM;gBAC/C,IAAI,IAAI,CAAC,GAAG,MAAM,CAAC,EAAE,KAAK,IAAI,IAAI;gBAClC,KAAK,sBAAsB;gBAC3B;YACJ;QACJ;QAEA,IAAI,IAAI,CAAC;IACb;IAEA,OAAO,IAAI,IAAI,CAAC;AACpB;AAEA;;;CAGC,GACD,MAAM,iBAAiB,CAAC;IACpB,IAAI,UAAU;IAEd,+DAA+D;IAC/D,yDAAyD;IACzD,+DAA+D;IAE/D,MAAM,cAAc;QAChB;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA,0BAAkC,0DAA0D;KAC/F;IAED,YAAY,OAAO,CAAC,CAAA;QAChB,UAAU,QAAQ,OAAO,CAAC,OAAO;IACrC;IAEA,+DAA+D;IAC/D,iCAAiC;IACjC,+DAA+D;IAE/D,qDAAqD;IACrD,UAAU,QAAQ,OAAO,CAAC,oBAAoB;IAE9C,4CAA4C;IAC5C,UAAU,QAAQ,OAAO,CAAC,uBAAuB;IACjD,UAAU,QAAQ,OAAO,CAAC,sBAAsB;IAEhD,wDAAwD;IACxD,UAAU,QAAQ,OAAO,CAAC,gBAAgB;IAG1C,+DAA+D;IAC/D,yBAAyB;IACzB,+DAA+D;IAE/D,MAAM,YAAY;IAClB,MAAM,cAAmD,EAAE;IAC3D,IAAI;IAEJ,MAAO,CAAC,SAAS,UAAU,IAAI,CAAC,QAAQ,MAAM,KAAM;QAChD,YAAY,IAAI,CAAC;YAAE,OAAO,OAAO,KAAK;YAAE,QAAQ,MAAM,CAAC,EAAE,CAAC,MAAM;QAAC;IACrE;IAEA,IAAI,YAAY,MAAM,GAAG,GAAG;QACxB,MAAM,kBAAoD,EAAE;QAC5D,IAAI,eAAe,CAAC;QAEpB,IAAK,IAAI,IAAI,GAAG,IAAI,YAAY,MAAM,EAAE,IAAK;YACzC,MAAM,IAAI,WAAW,CAAC,EAAE;YACxB,MAAM,SAAS,EAAE,KAAK;YACtB,MAAM,OAAO,SAAS,EAAE,MAAM;YAE9B,IAAI,MAAM,GAAG;gBACT,eAAe;gBACf;YACJ;YAEA,MAAM,cAAc,QAAQ,SAAS,CAAC,cAAc;YACpD,IAAI,YAAY,OAAO,CAAC,OAAO,IAAI,MAAM,GAAG,IAAI;gBAC5C,gBAAgB,IAAI,CAAC;oBAAE,OAAO;oBAAQ,KAAK;gBAAK;YACpD,OAAO;gBACH,eAAe;YACnB;QACJ;QAEA,6CAA6C;QAC7C,IAAK,IAAI,IAAI,gBAAgB,MAAM,GAAG,GAAG,KAAK,GAAG,IAAK;YAClD,MAAM,QAAQ,eAAe,CAAC,EAAE;YAChC,UAAU,QAAQ,SAAS,CAAC,GAAG,MAAM,KAAK,IAAI,QAAQ,SAAS,CAAC,MAAM,GAAG;QAC7E;IACJ;IAEA,+DAA+D;IAC/D,2BAA2B;IAC3B,+DAA+D;IAE/D,qBAAqB;IACrB,UAAU,QAAQ,OAAO,CAAC,mBAAmB;IAE7C,oCAAoC;IACpC,UAAU,QAAQ,OAAO,CAAC,0BAA0B;IAEpD,iDAAiD;IACjD,UAAU,QAAQ,OAAO,CAAC,2BAA2B;IAErD,+DAA+D;IAC/D,8BAA8B;IAC9B,+DAA+D;IAE/D,mCAAmC;IACnC,UAAU,QAAQ,OAAO,CAAC,sCAAsC;IAEhE,wCAAwC;IACxC,UAAU,QAAQ,OAAO,CAAC,uDAAuD;IAEjF,8BAA8B;IAC9B,UAAU,QAAQ,OAAO,CAAC,gCAAgC;IAE1D,6CAA6C;IAC7C,UAAU,QAAQ,OAAO,CAAC,mBAAmB,KAAK,mBAAmB;IACrE,UAAU,QAAQ,OAAO,CAAC,8CAA8C,KAAK,sCAAsC;IAEnH,2BAA2B;IAC3B,UAAU,QAAQ,OAAO,CAAC,WAAW,QAAQ,IAAI;IAEjD,OAAO;AACX"}},
    {"offset": {"line": 2044, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/research/referenceAnalysisService.ts"],"sourcesContent":["import { ServiceResponse, ReferenceAnalysis, TargetAudience } from '../../types';\nimport { aiService } from '../engine/aiService';\nimport { promptTemplates } from '../engine/promptTemplates';\nimport { getLanguageInstruction, toTokenUsage } from '../engine/promptService';\nimport { Type } from '../engine/schemaTypes';\n\nexport const extractWebsiteTypeAndTerm = async (content: string) => {\n    // Lightweight helper for URL scraping flow to infer websiteType & authorityTerms.\n    const prompt = `\n    Scan the following content and extract:\n    1) websiteType: e.g., \"Medical Clinic\", \"Ecommerce\", \"Blog\".\n    2) authorityTerms: up to 5 key terms related to brand authority (certifications, ingredients, key specs).\n    CONTENT:\n    ${content.substring(0, 6000)}\n    OUTPUT JSON: { \"websiteType\": \"...\", \"authorityTerms\": \"comma separated terms\" }\n    `;\n\n    // Using runJson for structured output\n    return await aiService.runJson<{ websiteType: string; authorityTerms: string }>(\n        prompt,\n        'FLASH',\n        {\n            type: Type.OBJECT,\n            properties: {\n                websiteType: { type: Type.STRING },\n                authorityTerms: { type: Type.STRING },\n            }\n        }\n    );\n};\n\nexport const analyzeReferenceStructure = async (\n    referenceContent: string,\n    targetAudience: TargetAudience\n): Promise<ServiceResponse<ReferenceAnalysis>> => {\n    const startTs = Date.now();\n    const languageInstruction = getLanguageInstruction(targetAudience);\n\n    // Prepare Prompts\n    // Using full content as requested by the user\n    const structurePrompt = promptTemplates.narrativeStructure({ content: referenceContent, targetAudience, languageInstruction });\n    const voicePrompt = promptTemplates.voiceStrategy({ content: referenceContent, targetAudience, languageInstruction });\n\n    try {\n        // Run Parallel Analysis\n        const [structRes, voiceRes] = await Promise.all([\n            // 1. Structure Analysis\n            aiService.runJson<any>(structurePrompt, 'FLASH', {\n                type: Type.OBJECT,\n                properties: {\n                    h1Title: { type: Type.STRING },\n                    introText: { type: Type.STRING },\n                    keyInformationPoints: { type: Type.ARRAY, items: { type: Type.STRING } },\n                    structure: {\n                        type: Type.ARRAY,\n                        items: {\n                            type: Type.OBJECT,\n                            properties: {\n                                title: { type: Type.STRING },\n                                narrativePlan: { type: Type.ARRAY, items: { type: Type.STRING } },\n                                coreQuestion: { type: Type.STRING },\n                                difficulty: { type: Type.STRING, description: \"easy | medium | unclear\" },\n                                exclude: { type: Type.BOOLEAN },\n                                excludeReason: { type: Type.STRING },\n                                writingMode: { type: Type.STRING, description: \"direct | multi_solutions\" },\n                                solutionAngles: { type: Type.ARRAY, items: { type: Type.STRING } },\n                                subheadings: { type: Type.ARRAY, items: { type: Type.STRING } },\n                                keyFacts: { type: Type.ARRAY, items: { type: Type.STRING } },\n                                uspNotes: { type: Type.ARRAY, items: { type: Type.STRING } },\n                                isChecklist: { type: Type.BOOLEAN },\n                                suppress: { type: Type.ARRAY, items: { type: Type.STRING } },\n                                augment: { type: Type.ARRAY, items: { type: Type.STRING } },\n                                sentenceStartFeatures: { type: Type.ARRAY, items: { type: Type.STRING } },\n                                sentenceEndFeatures: { type: Type.ARRAY, items: { type: Type.STRING } },\n                            },\n                            required: [\"title\", \"narrativePlan\", \"coreQuestion\", \"writingMode\", \"keyFacts\"]\n                        }\n                    }\n                },\n                required: [\"h1Title\", \"introText\", \"structure\", \"keyInformationPoints\"]\n            }),\n            // 2. Voice & Strategy Analysis\n            aiService.runJson<any>(voicePrompt, 'FLASH', {\n                type: Type.OBJECT,\n                properties: {\n                    generalPlan: { type: Type.ARRAY, items: { type: Type.STRING } },\n                    conversionPlan: { type: Type.ARRAY, items: { type: Type.STRING } },\n                    brandExclusivePoints: { type: Type.ARRAY, items: { type: Type.STRING } },\n                    competitorBrands: { type: Type.ARRAY, items: { type: Type.STRING } },\n                    competitorProducts: { type: Type.ARRAY, items: { type: Type.STRING } },\n                    regionVoiceDetect: { type: Type.STRING },\n                    humanWritingVoice: { type: Type.STRING }\n                },\n                required: [\"generalPlan\", \"conversionPlan\", \"brandExclusivePoints\", \"regionVoiceDetect\", \"humanWritingVoice\"]\n            })\n        ]);\n\n        const structData = structRes.data;\n        const voiceData = voiceRes.data;\n\n        console.log('[RefAnalysis] Raw AI Structure Data:', JSON.stringify(structData, null, 2));\n        console.log('[RefAnalysis] Raw AI Voice Data:', JSON.stringify(voiceData, null, 2));\n\n        // Filter excluded sections\n        const filteredStructure = (structData.structure || []).filter((item: any) => {\n            if (item.exclude === true) {\n                console.log(`[RefAnalysis] AI Excluded section: \"${item.title}\" - Reason: ${item.excludeReason || 'irrelevant'}`);\n                return false;\n            }\n            // HARDCODED FALLBACK: Exclude common navigational sections that AI might miss\n            const navKeywords = ['目錄', '導覽', '清單', '引言', '延伸閱讀', '相關文章', 'Table of Contents', 'TOC', 'Introduction'];\n            if (navKeywords.some(kw => item.title.includes(kw) && (item.title.length < 15))) {\n                console.log(`[RefAnalysis] Auto-Excluding navigational section: \"${item.title}\"`);\n                return false;\n            }\n            return true;\n        });\n\n        const normalizedStructure = filteredStructure.map((item: any) => {\n            const normalized = {\n                ...item,\n                subheadings: Array.isArray(item.subheadings) ? item.subheadings : [],\n                keyFacts: Array.isArray(item.keyFacts) ? item.keyFacts : [],\n                uspNotes: Array.isArray(item.uspNotes) ? item.uspNotes : [],\n                suppress: Array.isArray(item.suppress) ? item.suppress : [],\n                augment: Array.isArray(item.augment) ? item.augment : [],\n                narrativePlan: Array.isArray(item.narrativePlan) ? item.narrativePlan : [],\n                sentenceStartFeatures: Array.isArray(item.sentenceStartFeatures) ? item.sentenceStartFeatures : [],\n                sentenceEndFeatures: Array.isArray(item.sentenceEndFeatures) ? item.sentenceEndFeatures : [],\n            };\n\n            // STRICT VALIDATION: If a section is included, it MUST have a plan and facts\n            if (normalized.narrativePlan.length === 0 || normalized.keyFacts.length === 0) {\n                console.error(`[RefAnalysis] Section \"${item.title}\" failed validation: Missing Narrative Plan or Key Facts.`);\n                throw new Error(`Invalid Narrative Structure: Section \"${item.title}\" is missing required content (Plan/Facts).`);\n            }\n\n            return normalized;\n        });\n\n        const combinedRules = [\n            ...(voiceData.competitorBrands || []),\n            ...(voiceData.competitorProducts || [])\n        ];\n\n        // Combine Token Usage & Cost\n        const totalUsage = {\n            inputTokens: (structRes.usage?.inputTokens || 0) + (voiceRes.usage?.inputTokens || 0),\n            outputTokens: (structRes.usage?.outputTokens || 0) + (voiceRes.usage?.outputTokens || 0),\n            totalTokens: (structRes.usage?.totalTokens || 0) + (voiceRes.usage?.totalTokens || 0),\n        };\n\n        const totalCost = {\n            inputCost: (structRes.cost?.inputCost || 0) + (voiceRes.cost?.inputCost || 0),\n            outputCost: (structRes.cost?.outputCost || 0) + (voiceRes.cost?.outputCost || 0),\n            totalCost: (structRes.cost?.totalCost || 0) + (voiceRes.cost?.totalCost || 0),\n        };\n\n        return {\n            data: {\n                h1Title: structData.h1Title || '',\n                introText: structData.introText || '',\n                structure: normalizedStructure,\n                keyInformationPoints: structData.keyInformationPoints || [],\n\n                // From Voice Analysis\n                generalPlan: voiceData.generalPlan || [],\n                conversionPlan: voiceData.conversionPlan || [],\n                brandExclusivePoints: voiceData.brandExclusivePoints || [],\n                replacementRules: combinedRules,\n                competitorBrands: voiceData.competitorBrands || [],\n                competitorProducts: voiceData.competitorProducts || [],\n                regionVoiceDetect: voiceData.regionVoiceDetect,\n                humanWritingVoice: voiceData.humanWritingVoice\n            },\n            usage: totalUsage,\n            cost: totalCost,\n            duration: Date.now() - startTs\n        };\n\n    } catch (e: any) {\n        console.error(\"Structure analysis failed\", e);\n        const errorUsage = toTokenUsage(0); // Simplified error handling\n        const errorCost = { inputCost: 0, outputCost: 0, totalCost: 0 };\n        return {\n            data: {\n                structure: [],\n                generalPlan: [],\n                conversionPlan: [],\n                keyInformationPoints: [],\n                brandExclusivePoints: [],\n                replacementRules: [],\n                competitorBrands: [],\n                competitorProducts: []\n            },\n            usage: errorUsage,\n            cost: errorCost,\n            duration: Date.now() - startTs\n        };\n    }\n};\n"],"names":[],"mappings":";;;;;;AACA;AACA;AACA;AACA;;;;;AAEO,MAAM,4BAA4B,OAAO;IAC5C,kFAAkF;IAClF,MAAM,SAAS,CAAC;;;;;IAKhB,EAAE,QAAQ,SAAS,CAAC,GAAG,MAAM;;IAE7B,CAAC;IAED,sCAAsC;IACtC,OAAO,MAAM,mJAAS,CAAC,OAAO,CAC1B,QACA,SACA;QACI,MAAM,gJAAI,CAAC,MAAM;QACjB,YAAY;YACR,aAAa;gBAAE,MAAM,gJAAI,CAAC,MAAM;YAAC;YACjC,gBAAgB;gBAAE,MAAM,gJAAI,CAAC,MAAM;YAAC;QACxC;IACJ;AAER;AAEO,MAAM,4BAA4B,OACrC,kBACA;IAEA,MAAM,UAAU,KAAK,GAAG;IACxB,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IAEnD,kBAAkB;IAClB,8CAA8C;IAC9C,MAAM,kBAAkB,+JAAe,CAAC,kBAAkB,CAAC;QAAE,SAAS;QAAkB;QAAgB;IAAoB;IAC5H,MAAM,cAAc,+JAAe,CAAC,aAAa,CAAC;QAAE,SAAS;QAAkB;QAAgB;IAAoB;IAEnH,IAAI;QACA,wBAAwB;QACxB,MAAM,CAAC,WAAW,SAAS,GAAG,MAAM,QAAQ,GAAG,CAAC;YAC5C,wBAAwB;YACxB,mJAAS,CAAC,OAAO,CAAM,iBAAiB,SAAS;gBAC7C,MAAM,gJAAI,CAAC,MAAM;gBACjB,YAAY;oBACR,SAAS;wBAAE,MAAM,gJAAI,CAAC,MAAM;oBAAC;oBAC7B,WAAW;wBAAE,MAAM,gJAAI,CAAC,MAAM;oBAAC;oBAC/B,sBAAsB;wBAAE,MAAM,gJAAI,CAAC,KAAK;wBAAE,OAAO;4BAAE,MAAM,gJAAI,CAAC,MAAM;wBAAC;oBAAE;oBACvE,WAAW;wBACP,MAAM,gJAAI,CAAC,KAAK;wBAChB,OAAO;4BACH,MAAM,gJAAI,CAAC,MAAM;4BACjB,YAAY;gCACR,OAAO;oCAAE,MAAM,gJAAI,CAAC,MAAM;gCAAC;gCAC3B,eAAe;oCAAE,MAAM,gJAAI,CAAC,KAAK;oCAAE,OAAO;wCAAE,MAAM,gJAAI,CAAC,MAAM;oCAAC;gCAAE;gCAChE,cAAc;oCAAE,MAAM,gJAAI,CAAC,MAAM;gCAAC;gCAClC,YAAY;oCAAE,MAAM,gJAAI,CAAC,MAAM;oCAAE,aAAa;gCAA0B;gCACxE,SAAS;oCAAE,MAAM,gJAAI,CAAC,OAAO;gCAAC;gCAC9B,eAAe;oCAAE,MAAM,gJAAI,CAAC,MAAM;gCAAC;gCACnC,aAAa;oCAAE,MAAM,gJAAI,CAAC,MAAM;oCAAE,aAAa;gCAA2B;gCAC1E,gBAAgB;oCAAE,MAAM,gJAAI,CAAC,KAAK;oCAAE,OAAO;wCAAE,MAAM,gJAAI,CAAC,MAAM;oCAAC;gCAAE;gCACjE,aAAa;oCAAE,MAAM,gJAAI,CAAC,KAAK;oCAAE,OAAO;wCAAE,MAAM,gJAAI,CAAC,MAAM;oCAAC;gCAAE;gCAC9D,UAAU;oCAAE,MAAM,gJAAI,CAAC,KAAK;oCAAE,OAAO;wCAAE,MAAM,gJAAI,CAAC,MAAM;oCAAC;gCAAE;gCAC3D,UAAU;oCAAE,MAAM,gJAAI,CAAC,KAAK;oCAAE,OAAO;wCAAE,MAAM,gJAAI,CAAC,MAAM;oCAAC;gCAAE;gCAC3D,aAAa;oCAAE,MAAM,gJAAI,CAAC,OAAO;gCAAC;gCAClC,UAAU;oCAAE,MAAM,gJAAI,CAAC,KAAK;oCAAE,OAAO;wCAAE,MAAM,gJAAI,CAAC,MAAM;oCAAC;gCAAE;gCAC3D,SAAS;oCAAE,MAAM,gJAAI,CAAC,KAAK;oCAAE,OAAO;wCAAE,MAAM,gJAAI,CAAC,MAAM;oCAAC;gCAAE;gCAC1D,uBAAuB;oCAAE,MAAM,gJAAI,CAAC,KAAK;oCAAE,OAAO;wCAAE,MAAM,gJAAI,CAAC,MAAM;oCAAC;gCAAE;gCACxE,qBAAqB;oCAAE,MAAM,gJAAI,CAAC,KAAK;oCAAE,OAAO;wCAAE,MAAM,gJAAI,CAAC,MAAM;oCAAC;gCAAE;4BAC1E;4BACA,UAAU;gCAAC;gCAAS;gCAAiB;gCAAgB;gCAAe;6BAAW;wBACnF;oBACJ;gBACJ;gBACA,UAAU;oBAAC;oBAAW;oBAAa;oBAAa;iBAAuB;YAC3E;YACA,+BAA+B;YAC/B,mJAAS,CAAC,OAAO,CAAM,aAAa,SAAS;gBACzC,MAAM,gJAAI,CAAC,MAAM;gBACjB,YAAY;oBACR,aAAa;wBAAE,MAAM,gJAAI,CAAC,KAAK;wBAAE,OAAO;4BAAE,MAAM,gJAAI,CAAC,MAAM;wBAAC;oBAAE;oBAC9D,gBAAgB;wBAAE,MAAM,gJAAI,CAAC,KAAK;wBAAE,OAAO;4BAAE,MAAM,gJAAI,CAAC,MAAM;wBAAC;oBAAE;oBACjE,sBAAsB;wBAAE,MAAM,gJAAI,CAAC,KAAK;wBAAE,OAAO;4BAAE,MAAM,gJAAI,CAAC,MAAM;wBAAC;oBAAE;oBACvE,kBAAkB;wBAAE,MAAM,gJAAI,CAAC,KAAK;wBAAE,OAAO;4BAAE,MAAM,gJAAI,CAAC,MAAM;wBAAC;oBAAE;oBACnE,oBAAoB;wBAAE,MAAM,gJAAI,CAAC,KAAK;wBAAE,OAAO;4BAAE,MAAM,gJAAI,CAAC,MAAM;wBAAC;oBAAE;oBACrE,mBAAmB;wBAAE,MAAM,gJAAI,CAAC,MAAM;oBAAC;oBACvC,mBAAmB;wBAAE,MAAM,gJAAI,CAAC,MAAM;oBAAC;gBAC3C;gBACA,UAAU;oBAAC;oBAAe;oBAAkB;oBAAwB;oBAAqB;iBAAoB;YACjH;SACH;QAED,MAAM,aAAa,UAAU,IAAI;QACjC,MAAM,YAAY,SAAS,IAAI;QAE/B,QAAQ,GAAG,CAAC,wCAAwC,KAAK,SAAS,CAAC,YAAY,MAAM;QACrF,QAAQ,GAAG,CAAC,oCAAoC,KAAK,SAAS,CAAC,WAAW,MAAM;QAEhF,2BAA2B;QAC3B,MAAM,oBAAoB,CAAC,WAAW,SAAS,IAAI,EAAE,EAAE,MAAM,CAAC,CAAC;YAC3D,IAAI,KAAK,OAAO,KAAK,MAAM;gBACvB,QAAQ,GAAG,CAAC,CAAC,oCAAoC,EAAE,KAAK,KAAK,CAAC,YAAY,EAAE,KAAK,aAAa,IAAI,cAAc;gBAChH,OAAO;YACX;YACA,8EAA8E;YAC9E,MAAM,cAAc;gBAAC;gBAAM;gBAAM;gBAAM;gBAAM;gBAAQ;gBAAQ;gBAAqB;gBAAO;aAAe;YACxG,IAAI,YAAY,IAAI,CAAC,CAAA,KAAM,KAAK,KAAK,CAAC,QAAQ,CAAC,OAAQ,KAAK,KAAK,CAAC,MAAM,GAAG,KAAM;gBAC7E,QAAQ,GAAG,CAAC,CAAC,oDAAoD,EAAE,KAAK,KAAK,CAAC,CAAC,CAAC;gBAChF,OAAO;YACX;YACA,OAAO;QACX;QAEA,MAAM,sBAAsB,kBAAkB,GAAG,CAAC,CAAC;YAC/C,MAAM,aAAa;gBACf,GAAG,IAAI;gBACP,aAAa,MAAM,OAAO,CAAC,KAAK,WAAW,IAAI,KAAK,WAAW,GAAG,EAAE;gBACpE,UAAU,MAAM,OAAO,CAAC,KAAK,QAAQ,IAAI,KAAK,QAAQ,GAAG,EAAE;gBAC3D,UAAU,MAAM,OAAO,CAAC,KAAK,QAAQ,IAAI,KAAK,QAAQ,GAAG,EAAE;gBAC3D,UAAU,MAAM,OAAO,CAAC,KAAK,QAAQ,IAAI,KAAK,QAAQ,GAAG,EAAE;gBAC3D,SAAS,MAAM,OAAO,CAAC,KAAK,OAAO,IAAI,KAAK,OAAO,GAAG,EAAE;gBACxD,eAAe,MAAM,OAAO,CAAC,KAAK,aAAa,IAAI,KAAK,aAAa,GAAG,EAAE;gBAC1E,uBAAuB,MAAM,OAAO,CAAC,KAAK,qBAAqB,IAAI,KAAK,qBAAqB,GAAG,EAAE;gBAClG,qBAAqB,MAAM,OAAO,CAAC,KAAK,mBAAmB,IAAI,KAAK,mBAAmB,GAAG,EAAE;YAChG;YAEA,6EAA6E;YAC7E,IAAI,WAAW,aAAa,CAAC,MAAM,KAAK,KAAK,WAAW,QAAQ,CAAC,MAAM,KAAK,GAAG;gBAC3E,QAAQ,KAAK,CAAC,CAAC,uBAAuB,EAAE,KAAK,KAAK,CAAC,yDAAyD,CAAC;gBAC7G,MAAM,IAAI,MAAM,CAAC,sCAAsC,EAAE,KAAK,KAAK,CAAC,2CAA2C,CAAC;YACpH;YAEA,OAAO;QACX;QAEA,MAAM,gBAAgB;eACd,UAAU,gBAAgB,IAAI,EAAE;eAChC,UAAU,kBAAkB,IAAI,EAAE;SACzC;QAED,6BAA6B;QAC7B,MAAM,aAAa;YACf,aAAa,CAAC,UAAU,KAAK,EAAE,eAAe,CAAC,IAAI,CAAC,SAAS,KAAK,EAAE,eAAe,CAAC;YACpF,cAAc,CAAC,UAAU,KAAK,EAAE,gBAAgB,CAAC,IAAI,CAAC,SAAS,KAAK,EAAE,gBAAgB,CAAC;YACvF,aAAa,CAAC,UAAU,KAAK,EAAE,eAAe,CAAC,IAAI,CAAC,SAAS,KAAK,EAAE,eAAe,CAAC;QACxF;QAEA,MAAM,YAAY;YACd,WAAW,CAAC,UAAU,IAAI,EAAE,aAAa,CAAC,IAAI,CAAC,SAAS,IAAI,EAAE,aAAa,CAAC;YAC5E,YAAY,CAAC,UAAU,IAAI,EAAE,cAAc,CAAC,IAAI,CAAC,SAAS,IAAI,EAAE,cAAc,CAAC;YAC/E,WAAW,CAAC,UAAU,IAAI,EAAE,aAAa,CAAC,IAAI,CAAC,SAAS,IAAI,EAAE,aAAa,CAAC;QAChF;QAEA,OAAO;YACH,MAAM;gBACF,SAAS,WAAW,OAAO,IAAI;gBAC/B,WAAW,WAAW,SAAS,IAAI;gBACnC,WAAW;gBACX,sBAAsB,WAAW,oBAAoB,IAAI,EAAE;gBAE3D,sBAAsB;gBACtB,aAAa,UAAU,WAAW,IAAI,EAAE;gBACxC,gBAAgB,UAAU,cAAc,IAAI,EAAE;gBAC9C,sBAAsB,UAAU,oBAAoB,IAAI,EAAE;gBAC1D,kBAAkB;gBAClB,kBAAkB,UAAU,gBAAgB,IAAI,EAAE;gBAClD,oBAAoB,UAAU,kBAAkB,IAAI,EAAE;gBACtD,mBAAmB,UAAU,iBAAiB;gBAC9C,mBAAmB,UAAU,iBAAiB;YAClD;YACA,OAAO;YACP,MAAM;YACN,UAAU,KAAK,GAAG,KAAK;QAC3B;IAEJ,EAAE,OAAO,GAAQ;QACb,QAAQ,KAAK,CAAC,6BAA6B;QAC3C,MAAM,aAAa,IAAA,0JAAY,EAAC,IAAI,4BAA4B;QAChE,MAAM,YAAY;YAAE,WAAW;YAAG,YAAY;YAAG,WAAW;QAAE;QAC9D,OAAO;YACH,MAAM;gBACF,WAAW,EAAE;gBACb,aAAa,EAAE;gBACf,gBAAgB,EAAE;gBAClB,sBAAsB,EAAE;gBACxB,sBAAsB,EAAE;gBACxB,kBAAkB,EAAE;gBACpB,kBAAkB,EAAE;gBACpB,oBAAoB,EAAE;YAC1B;YACA,OAAO;YACP,MAAM;YACN,UAAU,KAAK,GAAG,KAAK;QAC3B;IACJ;AACJ"}},
    {"offset": {"line": 2376, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/engine/embeddingService.ts"],"sourcesContent":["import { EMBED_MODEL_ID } from '../../config/constants';\nimport { buildAiUrl } from './genAIClient';\n\nconst env =\n  (typeof import.meta !== 'undefined' && (import.meta as any).env)\n    ? (import.meta as any).env\n    : (process.env as any);\n\ninterface EmbedOptions {\n  taskType?: string;\n  outputDimensionality?: number;\n  signal?: AbortSignal;\n}\n\ninterface EmbedResponse {\n  embeddings?: number[][];\n  embedding?: number[];\n  data?: { embeddings?: number[][]; embedding?: number[] } | number[] | Array<{ embeddings?: number[][]; embedding?: number[] }>;\n}\n\nconst asVector = (value: any): number[] | null => {\n  if (!Array.isArray(value)) return null;\n  return value as number[];\n};\n\nconst asMatrix = (value: any): number[][] | null => {\n  if (!Array.isArray(value)) return null;\n  const vectors = value.map((entry: any) => asVector(entry)).filter(Boolean) as number[][];\n  return vectors.length ? vectors : null;\n};\n\nconst extractEmbeddings = (payload: EmbedResponse): number[][] => {\n  if (!payload) return [];\n\n  const direct = asMatrix(payload.embeddings);\n  if (direct) return direct;\n\n  const dataField: any = payload.data;\n\n  const nested = asMatrix(dataField?.embeddings);\n  if (nested) return nested;\n\n  if (Array.isArray(dataField)) {\n    const vectors = dataField\n      .map((entry: any) => asMatrix(entry?.embeddings)?.[0] || asVector(entry?.embedding) || asVector(entry))\n      .filter(Boolean) as number[][];\n    if (vectors.length) return vectors;\n  }\n\n  const single = asVector(payload.embedding) || asVector(dataField?.embedding) || asVector(dataField);\n  return single ? [single] : [];\n};\n\nexport const embedTexts = async (\n  texts: string[],\n  options: EmbedOptions = {}\n): Promise<number[][]> => {\n  if (!Array.isArray(texts) || texts.length === 0) return [];\n\n  const providerOptions: any = {};\n  if (options.taskType) providerOptions.taskType = options.taskType;\n  if (options.outputDimensionality) providerOptions.outputDimensionality = options.outputDimensionality;\n\n  const body: any = {\n    texts,\n    model: EMBED_MODEL_ID,\n  };\n\n  if (options.taskType) body.taskType = options.taskType;\n  if (options.outputDimensionality) body.outputDimensionality = options.outputDimensionality;\n\n  if (Object.keys(providerOptions).length > 0) {\n    body.providerOptions = { google: providerOptions };\n  }\n\n  const token = env.VITE_AI_TOKEN || env.AI_TOKEN;\n  const headers: Record<string, string> = {\n    'Content-Type': 'application/json',\n  };\n  if (token) {\n    headers['Authorization'] = `Bearer ${token}`;\n  }\n\n  const response = await fetch(buildAiUrl('/embed'), {\n    method: 'POST',\n    headers,\n    body: JSON.stringify(body),\n    ...(options.signal ? { signal: options.signal } : {}),\n  });\n\n  const contentType = response.headers.get('content-type') || '';\n  const isJson = contentType.includes('application/json');\n\n  if (!response.ok) {\n    const detail = isJson ? await response.json().catch(() => undefined) : await response.text();\n    const message = typeof detail === 'string' ? detail : (detail as any)?.error || JSON.stringify(detail || '');\n    throw new Error(`Failed to fetch embeddings: ${response.status} ${message || ''}`.trim());\n  }\n\n  const payload: EmbedResponse = isJson ? await response.json() : { embeddings: [] };\n  const embeddings = extractEmbeddings(payload);\n\n  if (!embeddings.length) {\n    throw new Error('Embedding response did not include vectors');\n  }\n\n  if (embeddings.length !== texts.length) {\n    return texts.map((_, idx) => embeddings[idx] || []);\n  }\n\n  return embeddings;\n};\n\nexport const cosineSimilarity = (a: number[], b: number[]): number => {\n  if (!a?.length || !b?.length || a.length !== b.length) return 0;\n  let dot = 0;\n  let normA = 0;\n  let normB = 0;\n\n  for (let i = 0; i < a.length; i++) {\n    dot += a[i] * b[i];\n    normA += a[i] * a[i];\n    normB += b[i] * b[i];\n  }\n\n  const magnitude = Math.sqrt(normA) * Math.sqrt(normB);\n  if (magnitude === 0) return 0;\n  return dot / magnitude;\n};\n"],"names":[],"mappings":";;;;;;AAAA;AACA;;;;;;;;AAEA,MAAM,MACJ,AAAC,+CAAuB,eAAe,8BAAqB,GAAG,GAC3D,8BAAqB,GAAG,GACvB,QAAQ,GAAG;AAclB,MAAM,WAAW,CAAC;IAChB,IAAI,CAAC,MAAM,OAAO,CAAC,QAAQ,OAAO;IAClC,OAAO;AACT;AAEA,MAAM,WAAW,CAAC;IAChB,IAAI,CAAC,MAAM,OAAO,CAAC,QAAQ,OAAO;IAClC,MAAM,UAAU,MAAM,GAAG,CAAC,CAAC,QAAe,SAAS,QAAQ,MAAM,CAAC;IAClE,OAAO,QAAQ,MAAM,GAAG,UAAU;AACpC;AAEA,MAAM,oBAAoB,CAAC;IACzB,IAAI,CAAC,SAAS,OAAO,EAAE;IAEvB,MAAM,SAAS,SAAS,QAAQ,UAAU;IAC1C,IAAI,QAAQ,OAAO;IAEnB,MAAM,YAAiB,QAAQ,IAAI;IAEnC,MAAM,SAAS,SAAS,WAAW;IACnC,IAAI,QAAQ,OAAO;IAEnB,IAAI,MAAM,OAAO,CAAC,YAAY;QAC5B,MAAM,UAAU,UACb,GAAG,CAAC,CAAC,QAAe,SAAS,OAAO,aAAa,CAAC,EAAE,IAAI,SAAS,OAAO,cAAc,SAAS,QAC/F,MAAM,CAAC;QACV,IAAI,QAAQ,MAAM,EAAE,OAAO;IAC7B;IAEA,MAAM,SAAS,SAAS,QAAQ,SAAS,KAAK,SAAS,WAAW,cAAc,SAAS;IACzF,OAAO,SAAS;QAAC;KAAO,GAAG,EAAE;AAC/B;AAEO,MAAM,aAAa,OACxB,OACA,UAAwB,CAAC,CAAC;IAE1B,IAAI,CAAC,MAAM,OAAO,CAAC,UAAU,MAAM,MAAM,KAAK,GAAG,OAAO,EAAE;IAE1D,MAAM,kBAAuB,CAAC;IAC9B,IAAI,QAAQ,QAAQ,EAAE,gBAAgB,QAAQ,GAAG,QAAQ,QAAQ;IACjE,IAAI,QAAQ,oBAAoB,EAAE,gBAAgB,oBAAoB,GAAG,QAAQ,oBAAoB;IAErG,MAAM,OAAY;QAChB;QACA,OAAO,4IAAc;IACvB;IAEA,IAAI,QAAQ,QAAQ,EAAE,KAAK,QAAQ,GAAG,QAAQ,QAAQ;IACtD,IAAI,QAAQ,oBAAoB,EAAE,KAAK,oBAAoB,GAAG,QAAQ,oBAAoB;IAE1F,IAAI,OAAO,IAAI,CAAC,iBAAiB,MAAM,GAAG,GAAG;QAC3C,KAAK,eAAe,GAAG;YAAE,QAAQ;QAAgB;IACnD;IAEA,MAAM,QAAQ,IAAI,aAAa,IAAI,IAAI,QAAQ;IAC/C,MAAM,UAAkC;QACtC,gBAAgB;IAClB;IACA,IAAI,OAAO;QACT,OAAO,CAAC,gBAAgB,GAAG,CAAC,OAAO,EAAE,OAAO;IAC9C;IAEA,MAAM,WAAW,MAAM,MAAM,AA9EzB,IA8EyB,sJAAU,AA9EhB,EA8EiB,WAAW;QACjD,QAAQ;QACR;QACA,MAAM,KAAK,SAAS,CAAC;QACrB,GAAI,QAAQ,MAAM,GAAG;YAAE,QAAQ,QAAQ,MAAM;QAAC,IAAI,CAAC,CAAC;IACtD;IAEA,MAAM,cAAc,SAAS,OAAO,CAAC,GAAG,CAAC,mBAAmB;IAC5D,MAAM,SAAS,YAAY,QAAQ,CAAC;IAEpC,IAAI,CAAC,SAAS,EAAE,EAAE;QAChB,MAAM,SAAS,SAAS,MAAM,SAAS,IAAI,GAAG,KAAK,CAAC,IAAM,aAAa,MAAM,SAAS,IAAI;QAC1F,MAAM,UAAU,OAAO,WAAW,WAAW,SAAS,AAAC,QAAgB,SAAS,KAAK,SAAS,CAAC,UAAU;QACzG,MAAM,IAAI,MAAM,CAAC,4BAA4B,EAAE,SAAS,MAAM,CAAC,CAAC,EAAE,WAAW,IAAI,CAAC,IAAI;IACxF;IAEA,MAAM,UAAyB,SAAS,MAAM,SAAS,IAAI,KAAK;QAAE,YAAY,EAAE;IAAC;IACjF,MAAM,aAAa,kBAAkB;IAErC,IAAI,CAAC,WAAW,MAAM,EAAE;QACtB,MAAM,IAAI,MAAM;IAClB;IAEA,IAAI,WAAW,MAAM,KAAK,MAAM,MAAM,EAAE;QACtC,OAAO,MAAM,GAAG,CAAC,CAAC,GAAG,MAAQ,UAAU,CAAC,IAAI,IAAI,EAAE;IACpD;IAEA,OAAO;AACT;AAEO,MAAM,mBAAmB,CAAC,GAAa;IAC5C,IAAI,CAAC,GAAG,UAAU,CAAC,GAAG,UAAU,EAAE,MAAM,KAAK,EAAE,MAAM,EAAE,OAAO;IAC9D,IAAI,MAAM;IACV,IAAI,QAAQ;IACZ,IAAI,QAAQ;IAEZ,IAAK,IAAI,IAAI,GAAG,IAAI,EAAE,MAAM,EAAE,IAAK;QACjC,OAAO,CAAC,CAAC,EAAE,GAAG,CAAC,CAAC,EAAE;QAClB,SAAS,CAAC,CAAC,EAAE,GAAG,CAAC,CAAC,EAAE;QACpB,SAAS,CAAC,CAAC,EAAE,GAAG,CAAC,CAAC,EAAE;IACtB;IAEA,MAAM,YAAY,KAAK,IAAI,CAAC,SAAS,KAAK,IAAI,CAAC;IAC/C,IAAI,cAAc,GAAG,OAAO;IAC5B,OAAO,MAAM;AACf"}},
    {"offset": {"line": 2485, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/generation/imageService.ts"],"sourcesContent":["\nimport { ServiceResponse, ScrapedImage, TargetAudience, ImageAssetPlan } from '../../types';\nimport { calculateCost, getLanguageInstruction } from '../engine/promptService';\nimport { aiService } from '../engine/aiService';\nimport { Type } from '../engine/schemaTypes';\nimport { promptTemplates } from '../engine/promptTemplates';\nimport { MODEL } from '../../config/constants';\nimport { buildAiUrl, getAiHeaders } from '../engine/genAIClient';\n\nconst VISUAL_STYLE_GUIDE = `\n    **STRICT VISUAL CATEGORIES (Select ONE):**\n    1. **INFOGRAPHIC:** Clean, modern layout using icons, flowcharts, or \"lazy pack\" style summaries to explain concepts. (Use for: steps, summaries, data).\n    2. **BRANDED_LIFESTYLE:** High-end photography. Real people using the product/service in a specific, authentic environment. (Use for: Brand image, emotional connection).\n    3. **PRODUCT_INFOGRAPHIC:** Close-up of the product with subtle graphical highlights (lines/arrows) emphasizing a specific feature/spec.\n    4. **ECOMMERCE_WHITE_BG:** Pure white background, studio lighting, product isolated. (Use for: Commercial display only).\n\n    **COMPOSITION RULE (Split Screen):**\n    - If the context compares two things (Before/After, Good vs Bad, Option A vs B), or needs a macro detail alongside a wide shot, request a \"Split Screen (Left/Right)\" composition.\n    \n    **NEGATIVE CONSTRAINTS (ABSOLUTELY FORBIDDEN):**\n    - **NO ABSTRACT ART:** No glowing brains, floating digital nodes, surreal metaphors, or \"conceptual\" 3D renders.\n    - **NO TEXT:** Do not try to render specific sentences inside the image (AI cannot spell).\n`;\n\nconst ensureDataUrl = (value: string, mimeType = 'image/png'): string | null => {\n    if (!value) return null;\n    if (value.startsWith('data:') || value.startsWith('http')) return value;\n    return `data:${mimeType};base64,${value}`;\n};\n\nconst asArray = (value: any): any[] => {\n    if (!value) return [];\n    return Array.isArray(value) ? value : [value];\n};\n\nconst pickFromImageLike = (input: any, fallbackMime?: string): string | null => {\n    if (!input) return null;\n    if (typeof input === 'string') return ensureDataUrl(input, fallbackMime || 'image/png');\n\n    if (input.inlineData?.data) {\n        return ensureDataUrl(input.inlineData.data, input.inlineData.mimeType || fallbackMime);\n    }\n\n    const directData = input.b64_json || input.base64 || input.base64Data || input.data || input.image;\n    if (typeof directData === 'string') {\n        return ensureDataUrl(directData, input.mimeType || input.mediaType || fallbackMime);\n    }\n\n    if (directData && typeof directData === 'object' && directData !== input) {\n        const nested = pickFromImageLike(directData, input.mimeType || input.mediaType || fallbackMime);\n        if (nested) return nested;\n    }\n\n    if (typeof input.text === 'string' && input.text.trim().startsWith('data:image')) {\n        return input.text.trim();\n    }\n\n    if (input.url && typeof input.url === 'string') return input.url;\n    return null;\n};\n\nconst extractFromCandidates = (candidates: any[] | undefined): string | null => {\n    if (!Array.isArray(candidates)) return null;\n    for (const candidate of candidates) {\n        const parts = candidate?.content?.parts;\n        if (Array.isArray(parts)) {\n            for (const part of parts) {\n                if (part?.inlineData?.data) {\n                    return ensureDataUrl(part.inlineData.data, part.inlineData.mimeType);\n                }\n                if (typeof part?.text === 'string' && part.text.trim().startsWith('data:image')) {\n                    return part.text.trim();\n                }\n            }\n        }\n    }\n    return null;\n};\n\nconst extractImagePayload = (payload: any): string | null => {\n    if (!payload) return null;\n    if (typeof payload === 'string') return ensureDataUrl(payload);\n\n    const direct = pickFromImageLike(payload, payload.mimeType || payload.mediaType);\n    if (direct) return direct;\n\n    const candidateImage = extractFromCandidates(payload.candidates || payload.response?.candidates);\n    if (candidateImage) return candidateImage;\n\n    const collections = [\n        payload.images,\n        payload.data?.images,\n        payload.response?.images,\n        payload.result?.images,\n        payload.data,\n        payload.result,\n    ];\n\n    for (const collection of collections) {\n        for (const entry of asArray(collection)) {\n            const found = pickFromImageLike(entry, payload?.mimeType || payload?.mediaType);\n            if (found) return found;\n        }\n    }\n\n    if (payload.result?.image) return ensureDataUrl(payload.result.image);\n    return null;\n};\n\n// NEW: Analyze Image with AI (Vision)\nexport const analyzeImageWithAI = async (\n    imageUrl: string,\n    prompt: string = 'Describe this image in detail.',\n    model: string = MODEL.FLASH\n): Promise<ServiceResponse<string>> => {\n    const startTs = Date.now();\n\n    try {\n        const response = await fetch(buildAiUrl('/vision'), {\n            method: 'POST',\n            headers: getAiHeaders(),\n            body: JSON.stringify({\n                prompt,\n                image: imageUrl,\n                model,\n            }),\n        });\n\n        if (!response.ok) {\n            const errorText = await response.text();\n            throw new Error(`Vision API request failed (${response.status}): ${errorText}`);\n        }\n\n        const result = await response.json() as {\n            text: string;\n            usage?: { inputTokens: number; outputTokens: number; totalTokens: number };\n            finishReason?: string;\n        };\n\n        const duration = Date.now() - startTs;\n        const modelType = model.includes('flash') ? 'FLASH' : 'FLASH';\n        const metrics = calculateCost(result, modelType);\n\n        return {\n            data: result.text,\n            ...metrics,\n            duration,\n        };\n    } catch (error) {\n        const duration = Date.now() - startTs;\n        throw new Error(`Failed to analyze image: ${error instanceof Error ? error.message : String(error)}`);\n    }\n};\n\n// NEW: Analyze and Define Global Visual Identity\nexport const analyzeVisualStyle = async (\n    scrapedImages: ScrapedImage[],\n    websiteType: string\n): Promise<ServiceResponse<string>> => {\n    const startTs = Date.now();\n\n    const analyzedSamples = scrapedImages\n        .filter(img => img.aiDescription)\n        .slice(0, 5)\n        .map(img => img.aiDescription)\n        .join(\"\\n---\\n\");\n\n    const prompt = promptTemplates.visualStyle({ languageInstruction: getLanguageInstruction('zh-TW'), analyzedSamples, websiteType });\n\n    try {\n        const response = await aiService.runText(prompt, 'FLASH');\n\n        return {\n            data: response.text,\n            usage: response.usage,\n            cost: response.cost,\n            duration: response.duration\n        };\n    } catch (e) {\n        console.error(\"Visual Style Analysis failed\", e);\n        return {\n            data: \"Clean, modern professional photography with natural lighting.\",\n            usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: Date.now() - startTs\n        };\n    }\n};\n\nexport const generateImagePromptFromContext = async (\n    contextText: string,\n    targetAudience: TargetAudience,\n    visualStyle: string = \"\"\n): Promise<ServiceResponse<string>> => {\n    const startTs = Date.now();\n    const languageInstruction = getLanguageInstruction(targetAudience);\n\n    const prompt = promptTemplates.imagePromptFromContext({\n        contextText,\n        languageInstruction,\n        visualStyle,\n        guide: VISUAL_STYLE_GUIDE,\n    });\n\n    const response = await aiService.runText(prompt, 'FLASH');\n\n    return {\n        data: response.text?.trim() || \"\",\n        usage: response.usage,\n        cost: response.cost,\n        duration: Date.now() - startTs\n    };\n};\n\nexport const generateImage = async (prompt: string): Promise<ServiceResponse<string | null>> => {\n    const startTs = Date.now();\n\n    try {\n        const response = await fetch(buildAiUrl('/image'), {\n            method: 'POST',\n            headers: getAiHeaders(),\n            body: JSON.stringify({\n                prompt,\n                model: MODEL.IMAGE_PREVIEW,\n                aspectRatio: '16:9'\n            })\n        });\n\n        const contentType = response.headers.get('content-type') || '';\n        const isJson = contentType.includes('application/json');\n\n        if (!response.ok) {\n            const detail = isJson ? await response.json() : await response.text();\n            const message = typeof detail === 'string' ? detail : (detail?.error || JSON.stringify(detail));\n            throw new Error(`Image generation failed (${response.status}): ${message}`);\n        }\n\n        const payload = isJson ? await response.json() : { image: await response.text() };\n        const imageData = extractImagePayload(payload);\n        const metrics = calculateCost(\n            payload.totalUsage || payload.usageMetadata || payload.usage,\n            'IMAGE_GEN'\n        );\n\n        return {\n            data: imageData,\n            ...metrics,\n            duration: Date.now() - startTs\n        };\n    } catch (e) {\n        console.error(\"Image generation failed\", e);\n        throw e;\n    }\n};\n\n// Helper: Convert SVG Blob to PNG Base64 for AI Consumption\nconst convertSvgToPng = (svgBlob: Blob): Promise<string> => {\n    return new Promise((resolve, reject) => {\n        const url = URL.createObjectURL(svgBlob);\n        const img = new Image();\n\n        img.onload = () => {\n            try {\n                const canvas = document.createElement('canvas');\n                canvas.width = img.width || 800;\n                canvas.height = img.height || 600;\n\n                const ctx = canvas.getContext('2d');\n                if (!ctx) {\n                    reject(new Error(\"Canvas context not available\"));\n                    return;\n                }\n\n                ctx.fillStyle = '#FFFFFF';\n                ctx.fillRect(0, 0, canvas.width, canvas.height);\n\n                ctx.drawImage(img, 0, 0);\n\n                const dataUrl = canvas.toDataURL('image/png');\n                URL.revokeObjectURL(url);\n                resolve(dataUrl.split(',')[1]);\n            } catch (e) {\n                URL.revokeObjectURL(url);\n                reject(e);\n            }\n        };\n\n        img.onerror = () => {\n            URL.revokeObjectURL(url);\n            reject(new Error(\"Failed to load SVG image\"));\n        };\n\n        img.src = url;\n    });\n};\n\n// NEW: Plan images for the entire article with Visual Style injection\nexport const planImagesForArticle = async (\n    articleContent: string,\n    scrapedImages: ScrapedImage[],\n    targetAudience: TargetAudience,\n    visualStyle: string = \"\"\n): Promise<ServiceResponse<ImageAssetPlan[]>> => {\n    const startTs = Date.now();\n    const languageInstruction = getLanguageInstruction(targetAudience);\n\n    const maxImages = scrapedImages.length > 0 ? scrapedImages.length + 1 : 2;\n\n    const imageContexts = scrapedImages.slice(0, 30).map(img => ({\n        alt: img.altText,\n        aiAnalysis: img.aiDescription || \"N/A\"\n    }));\n\n    const prompt = `\n    I have a draft ARTICLE and a list of SOURCE IMAGES.\n    I also have a MANDATORY GLOBAL VISUAL STYLE that must be applied to all generated images.\n\n    GLOBAL VISUAL STYLE: \"${visualStyle || \"Clean, modern, professional style\"}\"\n    \n    TASK:\n    Create a \"Visual Asset Plan\" for the new article.\n    \n    ${VISUAL_STYLE_GUIDE}\n    \n    **ADDITIONAL CONSTRAINTS:**\n    1. **Quantity:** Generate a plan for **MAXIMUM ${maxImages} images**.\n    2. **Context & Culture:** Ensure the image description is culturally relevant.\n       ${languageInstruction}\n    3. **Insertion Anchor:** Select a unique text phrase (6-12 chars) from the content.\n    4. **Unified Style:** In the 'generatedPrompt', you MUST Explicitly describe how the \"Global Visual Style\" applies to this specific subject. Do NOT frame as an infographic; focus on photography, product, or lifestyle visuals.\n\n    ARTICLE CONTENT:\n    ${articleContent.substring(0, 20000)}\n\n    SOURCE IMAGES (Analyzed Reference):\n    ${JSON.stringify(imageContexts)}\n    `;\n\n    try {\n        const response = await aiService.runJson<any>(\n            prompt,\n            'FLASH',\n            {\n                type: Type.OBJECT,\n                properties: {\n                    plans: {\n                        type: Type.ARRAY,\n                        items: {\n                            type: Type.OBJECT,\n                            properties: {\n                                generatedPrompt: { type: Type.STRING, description: \"Detailed prompt including subject + visual style + mood.\" },\n                                category: { type: Type.STRING, enum: [\"BRANDED_LIFESTYLE\", \"PRODUCT_DETAIL\", \"ECOMMERCE_WHITE_BG\"] },\n                                insertAfter: { type: Type.STRING },\n                                rationale: { type: Type.STRING }\n                            },\n                            required: [\"generatedPrompt\", \"category\", \"insertAfter\"]\n                        }\n                    }\n                },\n                required: [\"plans\"]\n            }\n        );\n\n        const plans: any[] = response.data.plans || [];\n\n        const finalPlans: ImageAssetPlan[] = plans.map((p: any, index: number) => ({\n            id: `plan-${Date.now()}-${index}`,\n            category: p.category,\n            generatedPrompt: p.generatedPrompt,\n            insertAfter: p.insertAfter,\n            status: 'idle'\n        }));\n\n        return {\n            data: finalPlans,\n            usage: response.usage,\n            cost: response.cost,\n            duration: response.duration\n        };\n\n    } catch (e) {\n        console.error(\"Image Planning failed\", e);\n        return { data: [], usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 }, cost: { inputCost: 0, outputCost: 0, totalCost: 0 }, duration: Date.now() - startTs };\n    }\n};\n"],"names":[],"mappings":";;;;;;;;;;;;AAEA;AACA;AACA;AACA;AACA;AACA;;;;;;;AAEA,MAAM,qBAAqB,CAAC;;;;;;;;;;;;;AAa5B,CAAC;AAED,MAAM,gBAAgB,CAAC,OAAe,WAAW,WAAW;IACxD,IAAI,CAAC,OAAO,OAAO;IACnB,IAAI,MAAM,UAAU,CAAC,YAAY,MAAM,UAAU,CAAC,SAAS,OAAO;IAClE,OAAO,CAAC,KAAK,EAAE,SAAS,QAAQ,EAAE,OAAO;AAC7C;AAEA,MAAM,UAAU,CAAC;IACb,IAAI,CAAC,OAAO,OAAO,EAAE;IACrB,OAAO,MAAM,OAAO,CAAC,SAAS,QAAQ;QAAC;KAAM;AACjD;AAEA,MAAM,oBAAoB,CAAC,OAAY;IACnC,IAAI,CAAC,OAAO,OAAO;IACnB,IAAI,OAAO,UAAU,UAAU,OAAO,cAAc,OAAO,gBAAgB;IAE3E,IAAI,MAAM,UAAU,EAAE,MAAM;QACxB,OAAO,cAAc,MAAM,UAAU,CAAC,IAAI,EAAE,MAAM,UAAU,CAAC,QAAQ,IAAI;IAC7E;IAEA,MAAM,aAAa,MAAM,QAAQ,IAAI,MAAM,MAAM,IAAI,MAAM,UAAU,IAAI,MAAM,IAAI,IAAI,MAAM,KAAK;IAClG,IAAI,OAAO,eAAe,UAAU;QAChC,OAAO,cAAc,YAAY,MAAM,QAAQ,IAAI,MAAM,SAAS,IAAI;IAC1E;IAEA,IAAI,cAAc,OAAO,eAAe,YAAY,eAAe,OAAO;QACtE,MAAM,SAAS,kBAAkB,YAAY,MAAM,QAAQ,IAAI,MAAM,SAAS,IAAI;QAClF,IAAI,QAAQ,OAAO;IACvB;IAEA,IAAI,OAAO,MAAM,IAAI,KAAK,YAAY,MAAM,IAAI,CAAC,IAAI,GAAG,UAAU,CAAC,eAAe;QAC9E,OAAO,MAAM,IAAI,CAAC,IAAI;IAC1B;IAEA,IAAI,MAAM,GAAG,IAAI,OAAO,MAAM,GAAG,KAAK,UAAU,OAAO,MAAM,GAAG;IAChE,OAAO;AACX;AAEA,MAAM,wBAAwB,CAAC;IAC3B,IAAI,CAAC,MAAM,OAAO,CAAC,aAAa,OAAO;IACvC,KAAK,MAAM,aAAa,WAAY;QAChC,MAAM,QAAQ,WAAW,SAAS;QAClC,IAAI,MAAM,OAAO,CAAC,QAAQ;YACtB,KAAK,MAAM,QAAQ,MAAO;gBACtB,IAAI,MAAM,YAAY,MAAM;oBACxB,OAAO,cAAc,KAAK,UAAU,CAAC,IAAI,EAAE,KAAK,UAAU,CAAC,QAAQ;gBACvE;gBACA,IAAI,OAAO,MAAM,SAAS,YAAY,KAAK,IAAI,CAAC,IAAI,GAAG,UAAU,CAAC,eAAe;oBAC7E,OAAO,KAAK,IAAI,CAAC,IAAI;gBACzB;YACJ;QACJ;IACJ;IACA,OAAO;AACX;AAEA,MAAM,sBAAsB,CAAC;IACzB,IAAI,CAAC,SAAS,OAAO;IACrB,IAAI,OAAO,YAAY,UAAU,OAAO,cAAc;IAEtD,MAAM,SAAS,kBAAkB,SAAS,QAAQ,QAAQ,IAAI,QAAQ,SAAS;IAC/E,IAAI,QAAQ,OAAO;IAEnB,MAAM,iBAAiB,sBAAsB,QAAQ,UAAU,IAAI,QAAQ,QAAQ,EAAE;IACrF,IAAI,gBAAgB,OAAO;IAE3B,MAAM,cAAc;QAChB,QAAQ,MAAM;QACd,QAAQ,IAAI,EAAE;QACd,QAAQ,QAAQ,EAAE;QAClB,QAAQ,MAAM,EAAE;QAChB,QAAQ,IAAI;QACZ,QAAQ,MAAM;KACjB;IAED,KAAK,MAAM,cAAc,YAAa;QAClC,KAAK,MAAM,SAAS,QAAQ,YAAa;YACrC,MAAM,QAAQ,kBAAkB,OAAO,SAAS,YAAY,SAAS;YACrE,IAAI,OAAO,OAAO;QACtB;IACJ;IAEA,IAAI,QAAQ,MAAM,EAAE,OAAO,OAAO,cAAc,QAAQ,MAAM,CAAC,KAAK;IACpE,OAAO;AACX;AAGO,MAAM,qBAAqB,OAC9B,UACA,SAAiB,gCAAgC,EACjD,QAAgB,mIAAK,CAAC,KAAK;IAE3B,MAAM,UAAU,KAAK,GAAG;IAExB,IAAI;QACA,MAAM,WAAW,MAAM,MAAM,IAAA,sJAAU,EAAC,YAAY;YAChD,QAAQ;YACR,SAAS,IAAA,wJAAY;YACrB,MAAM,KAAK,SAAS,CAAC;gBACjB;gBACA,OAAO;gBACP;YACJ;QACJ;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YACd,MAAM,YAAY,MAAM,SAAS,IAAI;YACrC,MAAM,IAAI,MAAM,CAAC,2BAA2B,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,WAAW;QAClF;QAEA,MAAM,SAAS,MAAM,SAAS,IAAI;QAMlC,MAAM,WAAW,KAAK,GAAG,KAAK;QAC9B,MAAM,YAAY,MAAM,QAAQ,CAAC,WAAW,UAAU;QACtD,MAAM,UAAU,IAAA,2JAAa,EAAC,QAAQ;QAEtC,OAAO;YACH,MAAM,OAAO,IAAI;YACjB,GAAG,OAAO;YACV;QACJ;IACJ,EAAE,OAAO,OAAO;QACZ,MAAM,WAAW,KAAK,GAAG,KAAK;QAC9B,MAAM,IAAI,MAAM,CAAC,yBAAyB,EAAE,iBAAiB,QAAQ,MAAM,OAAO,GAAG,OAAO,QAAQ;IACxG;AACJ;AAGO,MAAM,qBAAqB,OAC9B,eACA;IAEA,MAAM,UAAU,KAAK,GAAG;IAExB,MAAM,kBAAkB,cACnB,MAAM,CAAC,CAAA,MAAO,IAAI,aAAa,EAC/B,KAAK,CAAC,GAAG,GACT,GAAG,CAAC,CAAA,MAAO,IAAI,aAAa,EAC5B,IAAI,CAAC;IAEV,MAAM,SAAS,+JAAe,CAAC,WAAW,CAAC;QAAE,qBAAqB,IAAA,oKAAsB,EAAC;QAAU;QAAiB;IAAY;IAEhI,IAAI;QACA,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CAAC,QAAQ;QAEjD,OAAO;YACH,MAAM,SAAS,IAAI;YACnB,OAAO,SAAS,KAAK;YACrB,MAAM,SAAS,IAAI;YACnB,UAAU,SAAS,QAAQ;QAC/B;IACJ,EAAE,OAAO,GAAG;QACR,QAAQ,KAAK,CAAC,gCAAgC;QAC9C,OAAO;YACH,MAAM;YACN,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YACzD,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU,KAAK,GAAG,KAAK;QAC3B;IACJ;AACJ;AAEO,MAAM,iCAAiC,OAC1C,aACA,gBACA,cAAsB,EAAE;IAExB,MAAM,UAAU,KAAK,GAAG;IACxB,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IAEnD,MAAM,SAAS,+JAAe,CAAC,sBAAsB,CAAC;QAClD;QACA;QACA;QACA,OAAO;IACX;IAEA,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CAAC,QAAQ;IAEjD,OAAO;QACH,MAAM,SAAS,IAAI,EAAE,UAAU;QAC/B,OAAO,SAAS,KAAK;QACrB,MAAM,SAAS,IAAI;QACnB,UAAU,KAAK,GAAG,KAAK;IAC3B;AACJ;AAEO,MAAM,gBAAgB,OAAO;IAChC,MAAM,UAAU,KAAK,GAAG;IAExB,IAAI;QACA,MAAM,WAAW,MAAM,MAAM,IAAA,sJAAU,EAAC,WAAW;YAC/C,QAAQ;YACR,SAAS,IAAA,wJAAY;YACrB,MAAM,KAAK,SAAS,CAAC;gBACjB;gBACA,OAAO,mIAAK,CAAC,aAAa;gBAC1B,aAAa;YACjB;QACJ;QAEA,MAAM,cAAc,SAAS,OAAO,CAAC,GAAG,CAAC,mBAAmB;QAC5D,MAAM,SAAS,YAAY,QAAQ,CAAC;QAEpC,IAAI,CAAC,SAAS,EAAE,EAAE;YACd,MAAM,SAAS,SAAS,MAAM,SAAS,IAAI,KAAK,MAAM,SAAS,IAAI;YACnE,MAAM,UAAU,OAAO,WAAW,WAAW,SAAU,QAAQ,SAAS,KAAK,SAAS,CAAC;YACvF,MAAM,IAAI,MAAM,CAAC,yBAAyB,EAAE,SAAS,MAAM,CAAC,GAAG,EAAE,SAAS;QAC9E;QAEA,MAAM,UAAU,SAAS,MAAM,SAAS,IAAI,KAAK;YAAE,OAAO,MAAM,SAAS,IAAI;QAAG;QAChF,MAAM,YAAY,oBAAoB;QACtC,MAAM,UAAU,IAAA,2JAAa,EACzB,QAAQ,UAAU,IAAI,QAAQ,aAAa,IAAI,QAAQ,KAAK,EAC5D;QAGJ,OAAO;YACH,MAAM;YACN,GAAG,OAAO;YACV,UAAU,KAAK,GAAG,KAAK;QAC3B;IACJ,EAAE,OAAO,GAAG;QACR,QAAQ,KAAK,CAAC,2BAA2B;QACzC,MAAM;IACV;AACJ;AAEA,4DAA4D;AAC5D,MAAM,kBAAkB,CAAC;IACrB,OAAO,IAAI,QAAQ,CAAC,SAAS;QACzB,MAAM,MAAM,IAAI,eAAe,CAAC;QAChC,MAAM,MAAM,IAAI;QAEhB,IAAI,MAAM,GAAG;YACT,IAAI;gBACA,MAAM,SAAS,SAAS,aAAa,CAAC;gBACtC,OAAO,KAAK,GAAG,IAAI,KAAK,IAAI;gBAC5B,OAAO,MAAM,GAAG,IAAI,MAAM,IAAI;gBAE9B,MAAM,MAAM,OAAO,UAAU,CAAC;gBAC9B,IAAI,CAAC,KAAK;oBACN,OAAO,IAAI,MAAM;oBACjB;gBACJ;gBAEA,IAAI,SAAS,GAAG;gBAChB,IAAI,QAAQ,CAAC,GAAG,GAAG,OAAO,KAAK,EAAE,OAAO,MAAM;gBAE9C,IAAI,SAAS,CAAC,KAAK,GAAG;gBAEtB,MAAM,UAAU,OAAO,SAAS,CAAC;gBACjC,IAAI,eAAe,CAAC;gBACpB,QAAQ,QAAQ,KAAK,CAAC,IAAI,CAAC,EAAE;YACjC,EAAE,OAAO,GAAG;gBACR,IAAI,eAAe,CAAC;gBACpB,OAAO;YACX;QACJ;QAEA,IAAI,OAAO,GAAG;YACV,IAAI,eAAe,CAAC;YACpB,OAAO,IAAI,MAAM;QACrB;QAEA,IAAI,GAAG,GAAG;IACd;AACJ;AAGO,MAAM,uBAAuB,OAChC,gBACA,eACA,gBACA,cAAsB,EAAE;IAExB,MAAM,UAAU,KAAK,GAAG;IACxB,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IAEnD,MAAM,YAAY,cAAc,MAAM,GAAG,IAAI,cAAc,MAAM,GAAG,IAAI;IAExE,MAAM,gBAAgB,cAAc,KAAK,CAAC,GAAG,IAAI,GAAG,CAAC,CAAA,MAAO,CAAC;YACzD,KAAK,IAAI,OAAO;YAChB,YAAY,IAAI,aAAa,IAAI;QACrC,CAAC;IAED,MAAM,SAAS,CAAC;;;;0BAIM,EAAE,eAAe,oCAAoC;;;;;IAK3E,EAAE,mBAAmB;;;mDAG0B,EAAE,UAAU;;OAExD,EAAE,oBAAoB;;;;;IAKzB,EAAE,eAAe,SAAS,CAAC,GAAG,OAAO;;;IAGrC,EAAE,KAAK,SAAS,CAAC,eAAe;IAChC,CAAC;IAED,IAAI;QACA,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CACpC,QACA,SACA;YACI,MAAM,gJAAI,CAAC,MAAM;YACjB,YAAY;gBACR,OAAO;oBACH,MAAM,gJAAI,CAAC,KAAK;oBAChB,OAAO;wBACH,MAAM,gJAAI,CAAC,MAAM;wBACjB,YAAY;4BACR,iBAAiB;gCAAE,MAAM,gJAAI,CAAC,MAAM;gCAAE,aAAa;4BAA2D;4BAC9G,UAAU;gCAAE,MAAM,gJAAI,CAAC,MAAM;gCAAE,MAAM;oCAAC;oCAAqB;oCAAkB;iCAAqB;4BAAC;4BACnG,aAAa;gCAAE,MAAM,gJAAI,CAAC,MAAM;4BAAC;4BACjC,WAAW;gCAAE,MAAM,gJAAI,CAAC,MAAM;4BAAC;wBACnC;wBACA,UAAU;4BAAC;4BAAmB;4BAAY;yBAAc;oBAC5D;gBACJ;YACJ;YACA,UAAU;gBAAC;aAAQ;QACvB;QAGJ,MAAM,QAAe,SAAS,IAAI,CAAC,KAAK,IAAI,EAAE;QAE9C,MAAM,aAA+B,MAAM,GAAG,CAAC,CAAC,GAAQ,QAAkB,CAAC;gBACvE,IAAI,CAAC,KAAK,EAAE,KAAK,GAAG,GAAG,CAAC,EAAE,OAAO;gBACjC,UAAU,EAAE,QAAQ;gBACpB,iBAAiB,EAAE,eAAe;gBAClC,aAAa,EAAE,WAAW;gBAC1B,QAAQ;YACZ,CAAC;QAED,OAAO;YACH,MAAM;YACN,OAAO,SAAS,KAAK;YACrB,MAAM,SAAS,IAAI;YACnB,UAAU,SAAS,QAAQ;QAC/B;IAEJ,EAAE,OAAO,GAAG;QACR,QAAQ,KAAK,CAAC,yBAAyB;QACvC,OAAO;YAAE,MAAM,EAAE;YAAE,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YAAG,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAAG,UAAU,KAAK,GAAG,KAAK;QAAQ;IACvK;AACJ"}},
    {"offset": {"line": 2851, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/engine/contextFilterService.ts"],"sourcesContent":["import { ServiceResponse, TargetAudience } from '../../types';\nimport { calculateCost, getLanguageInstruction } from './promptService';\nimport { Type } from './schemaTypes';\nimport { aiService } from './aiService';\n\n// Smart Context Filter with Knowledge Base Support (Stronger RAG)\nexport const filterSectionContext = async (\n    sectionTitle: string,\n    allKeyPoints: string[],\n    allAuthTerms: string[],\n    brandKnowledgeBase: string | undefined,\n    targetAudience: TargetAudience\n): Promise<ServiceResponse<{ filteredPoints: string[], filteredAuthTerms: string[], knowledgeInsights: string[] }>> => {\n\n    const startTs = Date.now();\n    const hasKnowledge = brandKnowledgeBase && brandKnowledgeBase.trim().length > 10;\n\n    if (!hasKnowledge && allKeyPoints.length <= 5 && allAuthTerms.length <= 5) {\n        return {\n            data: { filteredPoints: allKeyPoints, filteredAuthTerms: allAuthTerms, knowledgeInsights: [] },\n            usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: 0\n        };\n    }\n\n    const languageInstruction = getLanguageInstruction(targetAudience);\n\n    const prompt = `\n    I am writing a specific section titled: \"${sectionTitle}\".\n    \n    I have:\n    1. A database of \"Key Information Points\".\n    2. A database of \"Authority Terms\".\n    3. A \"BRAND KNOWLEDGE BASE\" (Guidelines/Specs).\n    \n    TASK:\n    1. **Filter Data**: Select ONLY the Key Points and Authority Terms strictly relevant to \"${sectionTitle}\".\n    2. **Agentic Retrieval**: Read the \"BRAND KNOWLEDGE BASE\". Extract 3-5 specific bullet points (Do's, Don'ts, Specs, Tone) that MUST be applied to this specific section.\n       - If nothing is relevant in the KB for this section, return empty list.\n    \n    ${languageInstruction}\n    \n    DATABASE:\n    Key Points: ${JSON.stringify(allKeyPoints)}\n    Authority Terms: ${JSON.stringify(allAuthTerms)}\n    \n    BRAND KNOWLEDGE BASE:\n    ${brandKnowledgeBase ? brandKnowledgeBase.substring(0, 30000) : \"N/A\"}\n    `;\n\n    try {\n        const response = await aiService.runText(prompt, 'FLASH', {\n            responseMimeType: 'application/json',\n            responseSchema: {\n                type: Type.OBJECT,\n                properties: {\n                    filteredPoints: { type: Type.ARRAY, items: { type: Type.STRING } },\n                    filteredAuthTerms: { type: Type.ARRAY, items: { type: Type.STRING } },\n                    knowledgeInsights: { type: Type.ARRAY, items: { type: Type.STRING } },\n                }\n            }\n        });\n\n        const data = JSON.parse(response.text || \"{}\");\n        const metrics = calculateCost(response.usage, 'FLASH');\n\n        return {\n            data: {\n                filteredPoints: data.filteredPoints || [],\n                filteredAuthTerms: data.filteredAuthTerms || [],\n                knowledgeInsights: data.knowledgeInsights || []\n            },\n            ...metrics,\n            duration: response.duration\n        };\n    } catch (e) {\n        console.error(\"Context filter failed\", e);\n        return {\n            data: { filteredPoints: allKeyPoints, filteredAuthTerms: allAuthTerms, knowledgeInsights: [] },\n            usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: Date.now() - startTs\n        };\n    }\n};\n"],"names":[],"mappings":";;;;AACA;AACA;AACA;;;;AAGO,MAAM,uBAAuB,OAChC,cACA,cACA,cACA,oBACA;IAGA,MAAM,UAAU,KAAK,GAAG;IACxB,MAAM,eAAe,sBAAsB,mBAAmB,IAAI,GAAG,MAAM,GAAG;IAE9E,IAAI,CAAC,gBAAgB,aAAa,MAAM,IAAI,KAAK,aAAa,MAAM,IAAI,GAAG;QACvE,OAAO;YACH,MAAM;gBAAE,gBAAgB;gBAAc,mBAAmB;gBAAc,mBAAmB,EAAE;YAAC;YAC7F,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YACzD,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU;QACd;IACJ;IAEA,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IAEnD,MAAM,SAAS,CAAC;6CACyB,EAAE,aAAa;;;;;;;;6FAQiC,EAAE,aAAa;;;;IAIxG,EAAE,oBAAoB;;;gBAGV,EAAE,KAAK,SAAS,CAAC,cAAc;qBAC1B,EAAE,KAAK,SAAS,CAAC,cAAc;;;IAGhD,EAAE,qBAAqB,mBAAmB,SAAS,CAAC,GAAG,SAAS,MAAM;IACtE,CAAC;IAED,IAAI;QACA,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CAAC,QAAQ,SAAS;YACtD,kBAAkB;YAClB,gBAAgB;gBACZ,MAAM,gJAAI,CAAC,MAAM;gBACjB,YAAY;oBACR,gBAAgB;wBAAE,MAAM,gJAAI,CAAC,KAAK;wBAAE,OAAO;4BAAE,MAAM,gJAAI,CAAC,MAAM;wBAAC;oBAAE;oBACjE,mBAAmB;wBAAE,MAAM,gJAAI,CAAC,KAAK;wBAAE,OAAO;4BAAE,MAAM,gJAAI,CAAC,MAAM;wBAAC;oBAAE;oBACpE,mBAAmB;wBAAE,MAAM,gJAAI,CAAC,KAAK;wBAAE,OAAO;4BAAE,MAAM,gJAAI,CAAC,MAAM;wBAAC;oBAAE;gBACxE;YACJ;QACJ;QAEA,MAAM,OAAO,KAAK,KAAK,CAAC,SAAS,IAAI,IAAI;QACzC,MAAM,UAAU,IAAA,2JAAa,EAAC,SAAS,KAAK,EAAE;QAE9C,OAAO;YACH,MAAM;gBACF,gBAAgB,KAAK,cAAc,IAAI,EAAE;gBACzC,mBAAmB,KAAK,iBAAiB,IAAI,EAAE;gBAC/C,mBAAmB,KAAK,iBAAiB,IAAI,EAAE;YACnD;YACA,GAAG,OAAO;YACV,UAAU,SAAS,QAAQ;QAC/B;IACJ,EAAE,OAAO,GAAG;QACR,QAAQ,KAAK,CAAC,yBAAyB;QACvC,OAAO;YACH,MAAM;gBAAE,gBAAgB;gBAAc,mBAAmB;gBAAc,mBAAmB,EAAE;YAAC;YAC7F,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YACzD,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU,KAAK,GAAG,KAAK;QAC3B;IACJ;AACJ"}},
    {"offset": {"line": 2971, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/generation/contentGenerationService.ts"],"sourcesContent":["import { ArticleConfig, FrequentWordsPlacementAnalysis, AuthorityAnalysis, ServiceResponse, TokenUsage, CostBreakdown, ProductBrief, ProblemProductMapping, SectionGenerationResult, TargetAudience, ReferenceAnalysis, SectionAnalysis } from '../../types';\nimport { calculateCost, getLanguageInstruction } from '../engine/promptService';\nimport { filterSectionContext } from '../engine/contextFilterService';\nimport { promptTemplates } from '../engine/promptTemplates';\nimport { MODEL, SEMANTIC_KEYWORD_LIMIT } from '../../config/constants';\nimport { aiService } from '../engine/aiService';\n\nimport { Type } from '../engine/schemaTypes';\n\n// Helper to determine injection strategy for the current section\nconst getSectionInjectionPlan = (\n    sectionTitle: string,\n    refAnalysis: ReferenceAnalysis | undefined,\n    productMapping: ProblemProductMapping[] = [],\n    productBrief?: ProductBrief,\n    currentInjectedCount: number = 0,\n    isLastSections: boolean = false\n): string => {\n    if (!productBrief || !productBrief.productName) return \"\";\n\n    let injectionPlan = `### 💎 COMMERCIAL & SERVICE STRATEGY (HIGH PRIORITY) \\n`;\n    let forceInjection = false;\n\n    // ==================================================================================\n    // 1. SANITIZATION & REPLACEMENT (CRITICAL)\n    // ==================================================================================\n    const competitorBrands = refAnalysis?.competitorBrands || [];\n    const competitorProducts = refAnalysis?.competitorProducts || [];\n    const genericReplacements = refAnalysis?.replacementRules || []; // Fallback\n\n    const allTargets = [...new Set([...competitorBrands, ...competitorProducts, ...genericReplacements])];\n\n    if (allTargets.length > 0) {\n        injectionPlan += `\n        **🛡️ SANITIZATION PROTOCOL (ABSOLUTE RULES):**\n        You are writing for the brand: **\"${productBrief.brandName}\"**.\n        The Reference Text mentions competitors: ${allTargets.map(t => `\"${t}\"`).join(', ')}.\n\n        1. **TOTAL ANNIHILATION:** Never output these competitor words in the final text.\n        2. **NO HYBRIDS:** Do NOT write \"CompName as ${productBrief.brandName}\". That is nonsense.\n        3. **SUBJECT SWAP (SEMANTIC REWRITE):**\n           - If the reference says: \"${competitorBrands[0] || 'Competitor'} offers the best laser...\"\n           - **REWRITE AS:** \"**${productBrief.brandName}** offers the best laser...\" (Change the Subject).\n           - If the reference discusses a specific machine (e.g., \"${competitorProducts[0] || 'OldMachine'}\"), replace it with **\"${productBrief.productName}\"**.\n        `;\n    }\n\n    // ==================================================================================\n    // 2. DENSITY CONTROL (AVOID KEYWORD STUFFING)\n    // ==================================================================================\n    injectionPlan += `\n    **📉 DENSITY CONTROL (AVOID KEYWORD STUFFING):**\n    - **Full Name Rule:** Use the full product name \"**${productBrief.productName}**\" **MAXIMUM ONCE** in this section.\n    - **Natural Variation:** For subsequent mentions, you MUST use variations:\n      - The Brand Name: \"**${productBrief.brandName}**\"\n      - Pronouns: \"We\", \"Our team\", \"The center\"\n      - Generic: \"This technology\", \"The treatment\", \"Our service\"\n    `;\n\n    // ==================================================================================\n    // 3. INJECTION LOGIC (Force vs Natural)\n    // ==================================================================================\n\n    // Logic: If we haven't mentioned the product enough (<= 2) and we are at the end, FORCE IT.\n    if (isLastSections && currentInjectedCount <= 2) {\n        forceInjection = true;\n        injectionPlan += `\\n**🚀 MANDATORY INJECTION:** You have NOT mentioned \"${productBrief.brandName}\" enough yet. You MUST introduce it here as the solution.\\n`;\n    }\n\n    // Match specific pain points to this section title.\n    const titleLower = sectionTitle.toLowerCase();\n    const relevantMappings = productMapping.filter(m =>\n        m.relevanceKeywords.some(kw => titleLower.includes(kw.toLowerCase()))\n    );\n    const isSolutionSection = titleLower.includes('solution') || titleLower.includes('benefit') || titleLower.includes('guide') || titleLower.includes('how');\n\n    let finalMappings = relevantMappings;\n    if (relevantMappings.length === 0 && (forceInjection || isSolutionSection)) {\n        // Fallback: Pick top 2 generic mappings\n        finalMappings = productMapping.slice(0, 2);\n    }\n\n    if (finalMappings.length > 0) {\n        injectionPlan += `\\n**💡 PROBLEM-SOLUTION WEAVING:**\\nIntegrate the following mapping naturally:\\n`;\n        finalMappings.forEach(m => {\n            injectionPlan += `- Discuss \"${m.painPoint}\" -> Then present **${productBrief.brandName}** (or ${productBrief.productName}) as the solution using [${m.productFeature}].\\n`;\n        });\n    }\n\n    // CTA\n    injectionPlan += `\\n**CTA:** End with a natural link: [${productBrief.ctaLink}] (Anchor: Check ${productBrief.brandName} pricing/details).\\n`;\n\n    return injectionPlan;\n};\n\n// Demote or strip H1/H2 headings from model output to avoid duplicate section titles.\nconst normalizeSectionContent = (content: string): string => {\n    let normalized = content || \"\";\n    normalized = normalized.replace(/^##\\s+/gm, \"### \"); // Demote H2 -> H3\n    normalized = normalized.replace(/^#\\s+/gm, \"### \");  // Demote H1 -> H3\n    normalized = normalized.replace(/<h1>(.*?)<\\/h1>/gi, \"### $1\");\n    normalized = normalized.replace(/<h2>(.*?)<\\/h2>/gi, \"### $1\");\n    return normalized;\n};\n\n// 3. Generate Single Section\nexport const generateSectionContent = async (\n    config: ArticleConfig,\n    sectionTitle: string,\n    specificPlan: string[] | undefined,\n    generalPlan: string[] | undefined,\n    keywordPlans: FrequentWordsPlacementAnalysis[],\n    previousSections: string[] = [],\n    futureSections: string[] = [],\n    authorityAnalysis: AuthorityAnalysis | null = null,\n    keyInfoPoints: string[] = [],\n    currentCoveredPointsHistory: string[] = [],\n    currentInjectedCount: number = 0,\n    sectionMeta: Partial<SectionAnalysis> = {}\n): Promise<ServiceResponse<SectionGenerationResult>> => {\n\n    const startTs = Date.now();\n    const isLastSections = futureSections.length <= 1;\n    const keywordPlansForPrompt = keywordPlans.slice(0, SEMANTIC_KEYWORD_LIMIT);\n\n    // RAG: Filter context to reduce token usage\n    const contextFilter = await filterSectionContext(\n        sectionTitle,\n        keyInfoPoints,\n        authorityAnalysis?.relevantTerms || [],\n        config.brandKnowledge,\n        config.targetAudience\n    );\n\n    const sectionKeyFacts = Array.isArray((sectionMeta as any).keyFacts) ? (sectionMeta as any).keyFacts : [];\n    const augmentFacts = Array.isArray((sectionMeta as any).augment) ? (sectionMeta as any).augment : [];\n    const relevantKeyPoints = Array.from(new Set([\n        ...sectionKeyFacts,\n        ...augmentFacts,\n        ...contextFilter.data.filteredPoints\n    ]));\n    const relevantAuthTerms = contextFilter.data.filteredAuthTerms;\n    const kbInsights = contextFilter.data.knowledgeInsights;\n\n    // Use all relevant points; no frequency cap so checklists can stay intact.\n    const pointsAvailableForThisSection = relevantKeyPoints;\n\n    const { coreQuestion, difficulty, writingMode, solutionAngles } = sectionMeta || {};\n\n    // Inject Product/Commercial Strategy if brief exists\n    const injectionPlan = getSectionInjectionPlan(\n        sectionTitle,\n        config.referenceAnalysis,\n        config.productMapping,\n        config.productBrief,\n        currentInjectedCount,\n        isLastSections\n    );\n\n    const languageInstruction = getLanguageInstruction(config.targetAudience);\n    const suppressHints = Array.isArray((sectionMeta as any).suppress) ? (sectionMeta as any).suppress : [];\n    // shiftPlan logic removed\n    const shiftPlanHints: string[] = [];\n    const renderMode = (sectionMeta as any).isChecklist ? 'checklist' : undefined;\n    const augmentHints = Array.isArray((sectionMeta as any).augment) ? (sectionMeta as any).augment : [];\n    const subheadings = Array.isArray((sectionMeta as any).subheadings) ? (sectionMeta as any).subheadings : [];\n\n    const prompt = promptTemplates.sectionContent({\n        sectionTitle,\n        languageInstruction,\n        previousSections,\n        futureSections,\n        generalPlan,\n        specificPlan,\n        kbInsights,\n        keywordPlans: keywordPlansForPrompt,\n        relevantAuthTerms,\n        points: pointsAvailableForThisSection,\n        injectionPlan,\n        articleTitle: config.title,\n        coreQuestion,\n        difficulty,\n        writingMode,\n        solutionAngles,\n        renderMode,\n        // shiftPlan removed\n        suppressHints,\n        augmentHints,\n        subheadings, // Pass extracted subheadings\n        avoidContent: [\n            ...futureSections,\n            ...previousSections,\n            ...relevantKeyPoints.filter(p => !pointsAvailableForThisSection.includes(p)),\n            ...suppressHints\n        ],\n        regionReplacements: config.referenceAnalysis?.regionalReplacements,\n        humanWritingVoice: config.referenceAnalysis?.humanWritingVoice, // NEW: Pass human voice\n        regionVoiceDetect: config.referenceAnalysis?.regionVoiceDetect, // NEW: Pass region voice %\n        replacementRules: config.referenceAnalysis?.replacementRules // NEW: Pass blocked terms\n    });\n\n    const response = await aiService.runJson<any>(\n        prompt,\n        'FLASH',\n        {\n            type: Type.OBJECT,\n            properties: {\n                content: { type: Type.STRING },\n                usedPoints: { type: Type.ARRAY, items: { type: Type.STRING } },\n                injectedCount: { type: Type.INTEGER, description: \"How many times did you mention the Product Name?\" }\n            }\n        }\n    );\n\n    const payload = response.data || {};\n    const rawContent =\n        (typeof payload.content === 'string' && payload.content.trim().length > 0)\n            ? payload.content\n            : (typeof payload.sectionContent === 'string' ? payload.sectionContent : \"\");\n\n    const usedPointsSource =\n        (Array.isArray(payload.usedPoints) && payload.usedPoints) ||\n        (Array.isArray((payload as any).used_points) && (payload as any).used_points) ||\n        (Array.isArray((payload as any).pointsUsed) && (payload as any).pointsUsed) ||\n        (Array.isArray((payload as any).usedFacts) && (payload as any).usedFacts) ||\n        [];\n    const usedPoints = Array.isArray(usedPointsSource)\n        ? usedPointsSource.map(p => (typeof p === 'string' ? p : String(p || '')).trim()).filter(Boolean)\n        : [];\n    const injectedRaw = payload.injectedCount ?? (payload as any).injected_count ?? 0;\n    const injectedCount = typeof injectedRaw === 'number' ? injectedRaw : Number(injectedRaw) || 0;\n\n    const data = {\n        content: rawContent,\n        usedPoints,\n        injectedCount\n    };\n\n    const totalCost = {\n        inputCost: response.cost.inputCost + contextFilter.cost.inputCost,\n        outputCost: response.cost.outputCost + contextFilter.cost.outputCost,\n        totalCost: response.cost.totalCost + contextFilter.cost.totalCost\n    };\n\n    const normalizedContent = normalizeSectionContent(data.content || \"\");\n\n    const totalUsage = {\n        inputTokens: response.usage.inputTokens + contextFilter.usage.inputTokens,\n        outputTokens: response.usage.outputTokens + contextFilter.usage.outputTokens,\n        totalTokens: response.usage.totalTokens + contextFilter.usage.totalTokens\n    };\n\n    const duration = Date.now() - startTs;\n\n    return {\n        data: {\n            content: normalizedContent,\n            usedPoints: data.usedPoints || [],\n            injectedCount: data.injectedCount || 0\n        },\n        usage: totalUsage,\n        cost: totalCost,\n        duration\n    };\n};\n\n// AI Rewriter / Formatter (Small Tool)\nexport const generateSnippet = async (\n    prompt: string,\n    targetAudience: TargetAudience,\n    config?: any\n): Promise<ServiceResponse<string>> => {\n    const languageInstruction = getLanguageInstruction(targetAudience);\n    const fullPrompt = promptTemplates.snippet({ prompt, languageInstruction });\n    const res = await aiService.runText(fullPrompt, 'FLASH', config);\n    return { data: res.text, usage: res.usage, cost: res.cost, duration: res.duration };\n};\n\n// REBRAND CONTENT (Global Entity Swap)\nexport const rebrandContent = async (\n    currentContent: string,\n    productBrief: ProductBrief,\n    targetAudience: TargetAudience\n): Promise<ServiceResponse<string>> => {\n    const languageInstruction = getLanguageInstruction(targetAudience);\n    const prompt = promptTemplates.rebrandContent({ productBrief, languageInstruction, currentContent });\n    const res = await aiService.runText(prompt, 'FLASH');\n    return { data: res.text, usage: res.usage, cost: res.cost, duration: res.duration };\n};\n\n\n// 🆕 SMART INJECT POINT (Refine with Paragraph Compact Indexing)\nexport const smartInjectPoint = async (\n    fullHtmlContent: string,\n    pointToInject: string,\n    targetAudience: TargetAudience\n): Promise<ServiceResponse<{ originalSnippet: string, newSnippet: string }>> => {\n\n    const startTs = Date.now();\n    const languageInstruction = getLanguageInstruction(targetAudience);\n\n    // 1. PARSE & INDEX (Paragraph Compact Indexing)\n    const parser = new DOMParser();\n    const doc = parser.parseFromString(fullHtmlContent, 'text/html');\n\n    const blocks: { id: number, text: string, html: string }[] = [];\n    const nodes = doc.querySelectorAll('p, li');\n\n    nodes.forEach((node, index) => {\n        const text = node.textContent?.trim() || \"\";\n        if (text.length > 20) {\n            blocks.push({\n                id: index,\n                text: text.substring(0, 80) + \"...\",\n                html: node.outerHTML\n            });\n        }\n    });\n\n    if (blocks.length === 0) {\n        return { data: { originalSnippet: \"\", newSnippet: \"\" }, usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 }, cost: { inputCost: 0, outputCost: 0, totalCost: 0 }, duration: Date.now() - startTs };\n    }\n\n    // 2. FIND BEST BLOCK (Prompt 1)\n    const findPrompt = promptTemplates.smartFindBlock({ pointToInject, blocks });\n\n    const findRes = await aiService.runText(findPrompt, 'FLASH');\n\n    const bestIdStr = findRes.text?.trim().match(/\\d+/)?.[0];\n    const bestId = bestIdStr ? parseInt(bestIdStr) : -1;\n\n    const targetBlock = blocks.find(b => b.id === bestId);\n\n    if (!targetBlock) {\n        return { data: { originalSnippet: \"\", newSnippet: \"\" }, usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 }, cost: { inputCost: 0, outputCost: 0, totalCost: 0 }, duration: Date.now() - startTs };\n    }\n\n    // 3. REWRITE BLOCK (Prompt 2)\n    const rewritePrompt = promptTemplates.smartRewriteBlock({ pointToInject, targetHtml: targetBlock.html, languageInstruction });\n\n    const rewriteRes = await aiService.runText(rewritePrompt, 'FLASH');\n\n    const totalUsage = {\n        inputTokens: (findRes.usage?.inputTokens || 0) + (rewriteRes.usage?.inputTokens || 0),\n        outputTokens: (findRes.usage?.outputTokens || 0) + (rewriteRes.usage?.outputTokens || 0),\n        totalTokens: (findRes.usage?.totalTokens || 0) + (rewriteRes.usage?.totalTokens || 0),\n    };\n    const totalCost = calculateCost(totalUsage, 'FLASH');\n\n    return {\n        data: {\n            originalSnippet: targetBlock.html,\n            newSnippet: rewriteRes.text?.trim() || targetBlock.html\n        },\n        usage: totalUsage,\n        cost: totalCost.cost,\n        duration: Date.now() - startTs\n    };\n};\n"],"names":[],"mappings":";;;;;;;;;;AACA;AACA;AACA;AACA;AACA;AAEA;;;;;;;AAEA,iEAAiE;AACjE,MAAM,0BAA0B,CAC5B,cACA,aACA,iBAA0C,EAAE,EAC5C,cACA,uBAA+B,CAAC,EAChC,iBAA0B,KAAK;IAE/B,IAAI,CAAC,gBAAgB,CAAC,aAAa,WAAW,EAAE,OAAO;IAEvD,IAAI,gBAAgB,CAAC,uDAAuD,CAAC;IAC7E,IAAI,iBAAiB;IAErB,qFAAqF;IACrF,2CAA2C;IAC3C,qFAAqF;IACrF,MAAM,mBAAmB,aAAa,oBAAoB,EAAE;IAC5D,MAAM,qBAAqB,aAAa,sBAAsB,EAAE;IAChE,MAAM,sBAAsB,aAAa,oBAAoB,EAAE,EAAE,WAAW;IAE5E,MAAM,aAAa;WAAI,IAAI,IAAI;eAAI;eAAqB;eAAuB;SAAoB;KAAE;IAErG,IAAI,WAAW,MAAM,GAAG,GAAG;QACvB,iBAAiB,CAAC;;0CAEgB,EAAE,aAAa,SAAS,CAAC;iDAClB,EAAE,WAAW,GAAG,CAAC,CAAA,IAAK,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,EAAE,IAAI,CAAC,MAAM;;;qDAGvC,EAAE,aAAa,SAAS,CAAC;;qCAEzC,EAAE,gBAAgB,CAAC,EAAE,IAAI,aAAa;gCAC3C,EAAE,aAAa,SAAS,CAAC;mEACU,EAAE,kBAAkB,CAAC,EAAE,IAAI,aAAa,uBAAuB,EAAE,aAAa,WAAW,CAAC;QACrJ,CAAC;IACL;IAEA,qFAAqF;IACrF,8CAA8C;IAC9C,qFAAqF;IACrF,iBAAiB,CAAC;;uDAEiC,EAAE,aAAa,WAAW,CAAC;;2BAEvD,EAAE,aAAa,SAAS,CAAC;;;IAGhD,CAAC;IAED,qFAAqF;IACrF,wCAAwC;IACxC,qFAAqF;IAErF,4FAA4F;IAC5F,IAAI,kBAAkB,wBAAwB,GAAG;QAC7C,iBAAiB;QACjB,iBAAiB,CAAC,sDAAsD,EAAE,aAAa,SAAS,CAAC,2DAA2D,CAAC;IACjK;IAEA,oDAAoD;IACpD,MAAM,aAAa,aAAa,WAAW;IAC3C,MAAM,mBAAmB,eAAe,MAAM,CAAC,CAAA,IAC3C,EAAE,iBAAiB,CAAC,IAAI,CAAC,CAAA,KAAM,WAAW,QAAQ,CAAC,GAAG,WAAW;IAErE,MAAM,oBAAoB,WAAW,QAAQ,CAAC,eAAe,WAAW,QAAQ,CAAC,cAAc,WAAW,QAAQ,CAAC,YAAY,WAAW,QAAQ,CAAC;IAEnJ,IAAI,gBAAgB;IACpB,IAAI,iBAAiB,MAAM,KAAK,KAAK,CAAC,kBAAkB,iBAAiB,GAAG;QACxE,wCAAwC;QACxC,gBAAgB,eAAe,KAAK,CAAC,GAAG;IAC5C;IAEA,IAAI,cAAc,MAAM,GAAG,GAAG;QAC1B,iBAAiB,CAAC,gFAAgF,CAAC;QACnG,cAAc,OAAO,CAAC,CAAA;YAClB,iBAAiB,CAAC,WAAW,EAAE,EAAE,SAAS,CAAC,oBAAoB,EAAE,aAAa,SAAS,CAAC,OAAO,EAAE,aAAa,WAAW,CAAC,yBAAyB,EAAE,EAAE,cAAc,CAAC,IAAI,CAAC;QAC/K;IACJ;IAEA,MAAM;IACN,iBAAiB,CAAC,qCAAqC,EAAE,aAAa,OAAO,CAAC,iBAAiB,EAAE,aAAa,SAAS,CAAC,oBAAoB,CAAC;IAE7I,OAAO;AACX;AAEA,sFAAsF;AACtF,MAAM,0BAA0B,CAAC;IAC7B,IAAI,aAAa,WAAW;IAC5B,aAAa,WAAW,OAAO,CAAC,YAAY,SAAS,kBAAkB;IACvE,aAAa,WAAW,OAAO,CAAC,WAAW,SAAU,kBAAkB;IACvE,aAAa,WAAW,OAAO,CAAC,qBAAqB;IACrD,aAAa,WAAW,OAAO,CAAC,qBAAqB;IACrD,OAAO;AACX;AAGO,MAAM,yBAAyB,OAClC,QACA,cACA,cACA,aACA,cACA,mBAA6B,EAAE,EAC/B,iBAA2B,EAAE,EAC7B,oBAA8C,IAAI,EAClD,gBAA0B,EAAE,EAC5B,8BAAwC,EAAE,EAC1C,uBAA+B,CAAC,EAChC,cAAwC,CAAC,CAAC;IAG1C,MAAM,UAAU,KAAK,GAAG;IACxB,MAAM,iBAAiB,eAAe,MAAM,IAAI;IAChD,MAAM,wBAAwB,aAAa,KAAK,CAAC,GAAG,oJAAsB;IAE1E,4CAA4C;IAC5C,MAAM,gBAAgB,MAAM,IAAA,yKAAoB,EAC5C,cACA,eACA,mBAAmB,iBAAiB,EAAE,EACtC,OAAO,cAAc,EACrB,OAAO,cAAc;IAGzB,MAAM,kBAAkB,MAAM,OAAO,CAAC,AAAC,YAAoB,QAAQ,IAAI,AAAC,YAAoB,QAAQ,GAAG,EAAE;IACzG,MAAM,eAAe,MAAM,OAAO,CAAC,AAAC,YAAoB,OAAO,IAAI,AAAC,YAAoB,OAAO,GAAG,EAAE;IACpG,MAAM,oBAAoB,MAAM,IAAI,CAAC,IAAI,IAAI;WACtC;WACA;WACA,cAAc,IAAI,CAAC,cAAc;KACvC;IACD,MAAM,oBAAoB,cAAc,IAAI,CAAC,iBAAiB;IAC9D,MAAM,aAAa,cAAc,IAAI,CAAC,iBAAiB;IAEvD,2EAA2E;IAC3E,MAAM,gCAAgC;IAEtC,MAAM,EAAE,YAAY,EAAE,UAAU,EAAE,WAAW,EAAE,cAAc,EAAE,GAAG,eAAe,CAAC;IAElF,qDAAqD;IACrD,MAAM,gBAAgB,wBAClB,cACA,OAAO,iBAAiB,EACxB,OAAO,cAAc,EACrB,OAAO,YAAY,EACnB,sBACA;IAGJ,MAAM,sBAAsB,IAAA,oKAAsB,EAAC,OAAO,cAAc;IACxE,MAAM,gBAAgB,MAAM,OAAO,CAAC,AAAC,YAAoB,QAAQ,IAAI,AAAC,YAAoB,QAAQ,GAAG,EAAE;IACvG,0BAA0B;IAC1B,MAAM,iBAA2B,EAAE;IACnC,MAAM,aAAa,AAAC,YAAoB,WAAW,GAAG,cAAc;IACpE,MAAM,eAAe,MAAM,OAAO,CAAC,AAAC,YAAoB,OAAO,IAAI,AAAC,YAAoB,OAAO,GAAG,EAAE;IACpG,MAAM,cAAc,MAAM,OAAO,CAAC,AAAC,YAAoB,WAAW,IAAI,AAAC,YAAoB,WAAW,GAAG,EAAE;IAE3G,MAAM,SAAS,+JAAe,CAAC,cAAc,CAAC;QAC1C;QACA;QACA;QACA;QACA;QACA;QACA;QACA,cAAc;QACd;QACA,QAAQ;QACR;QACA,cAAc,OAAO,KAAK;QAC1B;QACA;QACA;QACA;QACA;QACA,oBAAoB;QACpB;QACA;QACA;QACA,cAAc;eACP;eACA;eACA,kBAAkB,MAAM,CAAC,CAAA,IAAK,CAAC,8BAA8B,QAAQ,CAAC;eACtE;SACN;QACD,oBAAoB,OAAO,iBAAiB,EAAE;QAC9C,mBAAmB,OAAO,iBAAiB,EAAE;QAC7C,mBAAmB,OAAO,iBAAiB,EAAE;QAC7C,kBAAkB,OAAO,iBAAiB,EAAE,iBAAiB,0BAA0B;IAC3F;IAEA,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CACpC,QACA,SACA;QACI,MAAM,gJAAI,CAAC,MAAM;QACjB,YAAY;YACR,SAAS;gBAAE,MAAM,gJAAI,CAAC,MAAM;YAAC;YAC7B,YAAY;gBAAE,MAAM,gJAAI,CAAC,KAAK;gBAAE,OAAO;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;YAAE;YAC7D,eAAe;gBAAE,MAAM,gJAAI,CAAC,OAAO;gBAAE,aAAa;YAAmD;QACzG;IACJ;IAGJ,MAAM,UAAU,SAAS,IAAI,IAAI,CAAC;IAClC,MAAM,aACF,AAAC,OAAO,QAAQ,OAAO,KAAK,YAAY,QAAQ,OAAO,CAAC,IAAI,GAAG,MAAM,GAAG,IAClE,QAAQ,OAAO,GACd,OAAO,QAAQ,cAAc,KAAK,WAAW,QAAQ,cAAc,GAAG;IAEjF,MAAM,mBACF,AAAC,MAAM,OAAO,CAAC,QAAQ,UAAU,KAAK,QAAQ,UAAU,IACvD,MAAM,OAAO,CAAC,AAAC,QAAgB,WAAW,KAAK,AAAC,QAAgB,WAAW,IAC3E,MAAM,OAAO,CAAC,AAAC,QAAgB,UAAU,KAAK,AAAC,QAAgB,UAAU,IACzE,MAAM,OAAO,CAAC,AAAC,QAAgB,SAAS,KAAK,AAAC,QAAgB,SAAS,IACxE,EAAE;IACN,MAAM,aAAa,MAAM,OAAO,CAAC,oBAC3B,iBAAiB,GAAG,CAAC,CAAA,IAAK,CAAC,OAAO,MAAM,WAAW,IAAI,OAAO,KAAK,GAAG,EAAE,IAAI,IAAI,MAAM,CAAC,WACvF,EAAE;IACR,MAAM,cAAc,QAAQ,aAAa,IAAI,AAAC,QAAgB,cAAc,IAAI;IAChF,MAAM,gBAAgB,OAAO,gBAAgB,WAAW,cAAc,OAAO,gBAAgB;IAE7F,MAAM,OAAO;QACT,SAAS;QACT;QACA;IACJ;IAEA,MAAM,YAAY;QACd,WAAW,SAAS,IAAI,CAAC,SAAS,GAAG,cAAc,IAAI,CAAC,SAAS;QACjE,YAAY,SAAS,IAAI,CAAC,UAAU,GAAG,cAAc,IAAI,CAAC,UAAU;QACpE,WAAW,SAAS,IAAI,CAAC,SAAS,GAAG,cAAc,IAAI,CAAC,SAAS;IACrE;IAEA,MAAM,oBAAoB,wBAAwB,KAAK,OAAO,IAAI;IAElE,MAAM,aAAa;QACf,aAAa,SAAS,KAAK,CAAC,WAAW,GAAG,cAAc,KAAK,CAAC,WAAW;QACzE,cAAc,SAAS,KAAK,CAAC,YAAY,GAAG,cAAc,KAAK,CAAC,YAAY;QAC5E,aAAa,SAAS,KAAK,CAAC,WAAW,GAAG,cAAc,KAAK,CAAC,WAAW;IAC7E;IAEA,MAAM,WAAW,KAAK,GAAG,KAAK;IAE9B,OAAO;QACH,MAAM;YACF,SAAS;YACT,YAAY,KAAK,UAAU,IAAI,EAAE;YACjC,eAAe,KAAK,aAAa,IAAI;QACzC;QACA,OAAO;QACP,MAAM;QACN;IACJ;AACJ;AAGO,MAAM,kBAAkB,OAC3B,QACA,gBACA;IAEA,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IACnD,MAAM,aAAa,+JAAe,CAAC,OAAO,CAAC;QAAE;QAAQ;IAAoB;IACzE,MAAM,MAAM,MAAM,mJAAS,CAAC,OAAO,CAAC,YAAY,SAAS;IACzD,OAAO;QAAE,MAAM,IAAI,IAAI;QAAE,OAAO,IAAI,KAAK;QAAE,MAAM,IAAI,IAAI;QAAE,UAAU,IAAI,QAAQ;IAAC;AACtF;AAGO,MAAM,iBAAiB,OAC1B,gBACA,cACA;IAEA,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IACnD,MAAM,SAAS,+JAAe,CAAC,cAAc,CAAC;QAAE;QAAc;QAAqB;IAAe;IAClG,MAAM,MAAM,MAAM,mJAAS,CAAC,OAAO,CAAC,QAAQ;IAC5C,OAAO;QAAE,MAAM,IAAI,IAAI;QAAE,OAAO,IAAI,KAAK;QAAE,MAAM,IAAI,IAAI;QAAE,UAAU,IAAI,QAAQ;IAAC;AACtF;AAIO,MAAM,mBAAmB,OAC5B,iBACA,eACA;IAGA,MAAM,UAAU,KAAK,GAAG;IACxB,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IAEnD,gDAAgD;IAChD,MAAM,SAAS,IAAI;IACnB,MAAM,MAAM,OAAO,eAAe,CAAC,iBAAiB;IAEpD,MAAM,SAAuD,EAAE;IAC/D,MAAM,QAAQ,IAAI,gBAAgB,CAAC;IAEnC,MAAM,OAAO,CAAC,CAAC,MAAM;QACjB,MAAM,OAAO,KAAK,WAAW,EAAE,UAAU;QACzC,IAAI,KAAK,MAAM,GAAG,IAAI;YAClB,OAAO,IAAI,CAAC;gBACR,IAAI;gBACJ,MAAM,KAAK,SAAS,CAAC,GAAG,MAAM;gBAC9B,MAAM,KAAK,SAAS;YACxB;QACJ;IACJ;IAEA,IAAI,OAAO,MAAM,KAAK,GAAG;QACrB,OAAO;YAAE,MAAM;gBAAE,iBAAiB;gBAAI,YAAY;YAAG;YAAG,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YAAG,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAAG,UAAU,KAAK,GAAG,KAAK;QAAQ;IAC5M;IAEA,gCAAgC;IAChC,MAAM,aAAa,+JAAe,CAAC,cAAc,CAAC;QAAE;QAAe;IAAO;IAE1E,MAAM,UAAU,MAAM,mJAAS,CAAC,OAAO,CAAC,YAAY;IAEpD,MAAM,YAAY,QAAQ,IAAI,EAAE,OAAO,MAAM,QAAQ,CAAC,EAAE;IACxD,MAAM,SAAS,YAAY,SAAS,aAAa,CAAC;IAElD,MAAM,cAAc,OAAO,IAAI,CAAC,CAAA,IAAK,EAAE,EAAE,KAAK;IAE9C,IAAI,CAAC,aAAa;QACd,OAAO;YAAE,MAAM;gBAAE,iBAAiB;gBAAI,YAAY;YAAG;YAAG,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YAAG,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAAG,UAAU,KAAK,GAAG,KAAK;QAAQ;IAC5M;IAEA,8BAA8B;IAC9B,MAAM,gBAAgB,+JAAe,CAAC,iBAAiB,CAAC;QAAE;QAAe,YAAY,YAAY,IAAI;QAAE;IAAoB;IAE3H,MAAM,aAAa,MAAM,mJAAS,CAAC,OAAO,CAAC,eAAe;IAE1D,MAAM,aAAa;QACf,aAAa,CAAC,QAAQ,KAAK,EAAE,eAAe,CAAC,IAAI,CAAC,WAAW,KAAK,EAAE,eAAe,CAAC;QACpF,cAAc,CAAC,QAAQ,KAAK,EAAE,gBAAgB,CAAC,IAAI,CAAC,WAAW,KAAK,EAAE,gBAAgB,CAAC;QACvF,aAAa,CAAC,QAAQ,KAAK,EAAE,eAAe,CAAC,IAAI,CAAC,WAAW,KAAK,EAAE,eAAe,CAAC;IACxF;IACA,MAAM,YAAY,IAAA,2JAAa,EAAC,YAAY;IAE5C,OAAO;QACH,MAAM;YACF,iBAAiB,YAAY,IAAI;YACjC,YAAY,WAAW,IAAI,EAAE,UAAU,YAAY,IAAI;QAC3D;QACA,OAAO;QACP,MAAM,UAAU,IAAI;QACpB,UAAU,KAAK,GAAG,KAAK;IAC3B;AACJ"}},
    {"offset": {"line": 3305, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/generation/headingRefinerService.ts"],"sourcesContent":["import { HeadingOption, HeadingResult, TargetAudience, ServiceResponse, TokenUsage, CostBreakdown } from '../../types';\nimport { aiService } from '../engine/aiService';\nimport { embedTexts, cosineSimilarity } from '../engine/embeddingService';\nimport { getLanguageInstruction, toTokenUsage } from '../engine/promptService';\nimport { promptTemplates } from '../engine/promptTemplates';\nimport { Type } from '../engine/schemaTypes';\nimport { logger } from '../../utils/logger';\n\nconst cleanHeading = (s: string | undefined): string =>\n    (s || '')\n        .replace(/^#+\\s*/, '')\n        .replace(/\\*\\*/g, '')\n        .replace(/[\"“”]/g, '')\n        .trim();\n\nconst fallbackHeading = (original: string): string => {\n    const base = cleanHeading(original).replace(/[?？]+$/, '');\n    if (!base) return '重點整理';\n    if (base.length > 14) return `${base}重點`;\n    return `${base}解析`;\n};\n\nconst normalizeH3 = (list: any[]): HeadingResult['h3'] => {\n    const normalized = Array.isArray(list)\n        ? list\n            .filter((h: any) => h && (h.h3_before || h.h3_after || h.before || h.after))\n            .map((h: any) => {\n                const h3Before = cleanHeading(h.h3_before || h.before || '');\n                const h3AfterRaw = typeof h.h3_after === 'string' ? h.h3_after : (h.after || h3Before);\n                const h3After = cleanHeading(h3AfterRaw) || h3Before;\n                const h3Reason = typeof h.h3_reason === 'string' ? h.h3_reason : (typeof h.reason === 'string' ? h.reason : '');\n                return { h3_before: h3Before, h3_after: h3After, ...(h3Reason ? { h3_reason: h3Reason } : {}) };\n            })\n        : [];\n    return normalized.length ? normalized : undefined;\n};\n\nconst normalizeOptions = (input: any): HeadingOption[] => {\n    if (!Array.isArray(input)) return [];\n    const seen = new Set<string>();\n    const options: HeadingOption[] = [];\n\n    input.forEach((raw: any) => {\n        const text = cleanHeading(typeof raw === 'string' ? raw : raw?.text);\n        if (!text) return;\n        const key = text.toLowerCase();\n        if (seen.has(key)) return;\n        seen.add(key);\n        const reason = typeof raw?.reason === 'string' ? raw.reason.trim() : '';\n        options.push({ text, ...(reason ? { reason } : {}) });\n    });\n\n    return options;\n};\n\nconst calculateBestOption = (\n    h2Before: string,\n    options: HeadingOption[],\n    fallback: string,\n    baseEmbedding: number[] | undefined,\n    optionEmbeddings: (number[] | undefined)[]\n): { text: string; optionsWithScores: HeadingOption[]; needsManual: boolean } => {\n    const beforeClean = cleanHeading(h2Before);\n    const fallbackClean = cleanHeading(fallback) || beforeClean;\n\n    if (!options.length) {\n        return { text: fallbackClean, optionsWithScores: [], needsManual: true };\n    }\n\n    const scored = options.map((opt, idx) => {\n        const vec = optionEmbeddings[idx];\n        const score = (baseEmbedding && vec) ? cosineSimilarity(baseEmbedding, vec) : undefined;\n        return { ...opt, score };\n    });\n\n    const valid = scored.filter(opt =>\n        opt.text &&\n        opt.text.toLowerCase() !== beforeClean.toLowerCase()\n    );\n\n    const best = valid.reduce<HeadingOption | null>((acc, cur) => {\n        const currentScore = cur.score ?? -1;\n        const accScore = acc?.score ?? -1;\n        return currentScore > accScore ? cur : acc;\n    }, null);\n\n    if (!best) {\n        return { text: fallbackClean, optionsWithScores: scored, needsManual: true };\n    }\n\n    const needsManual = (best.score ?? 0) > 0.995 || best.text.toLowerCase() === beforeClean.toLowerCase();\n    return { text: best.text || fallbackClean, optionsWithScores: scored, needsManual };\n};\n\nconst toCost = (cost: any): CostBreakdown => ({\n    inputCost: cost?.inputCost || 0,\n    outputCost: cost?.outputCost || 0,\n    totalCost: cost?.totalCost || 0,\n});\n\nexport const refineHeadings = async (\n    articleTitle: string,\n    headings: string[],\n    targetAudience: TargetAudience\n): Promise<ServiceResponse<HeadingResult[]>> => {\n    const started = Date.now();\n    const languageInstruction = getLanguageInstruction(targetAudience);\n    const BATCH_SIZE = 12;\n\n    const mergeUsage = (a: TokenUsage, b: TokenUsage): TokenUsage => ({\n        inputTokens: (a.inputTokens || 0) + (b.inputTokens || 0),\n        outputTokens: (a.outputTokens || 0) + (b.outputTokens || 0),\n        totalTokens: (a.totalTokens || 0) + (b.totalTokens || 0),\n    });\n\n    const mergeCost = (a: CostBreakdown, b: CostBreakdown): CostBreakdown => ({\n        inputCost: (a.inputCost || 0) + (b.inputCost || 0),\n        outputCost: (a.outputCost || 0) + (b.outputCost || 0),\n        totalCost: (a.totalCost || 0) + (b.totalCost || 0),\n    });\n\n    const refineBatch = async (batch: string[]): Promise<ServiceResponse<HeadingResult[]>> => {\n        logger.log('refining_headings', `Refining ${batch.length} headings...`);\n        const prompt = promptTemplates.batchRefineHeadings({ articleTitle, headings: batch, languageInstruction });\n\n        const response = await aiService.runJson<{ headings: any[] }>(prompt, 'FLASH', {\n            type: Type.OBJECT,\n            properties: {\n                headings: {\n                    type: Type.ARRAY,\n                    items: {\n                        type: Type.OBJECT,\n                        properties: {\n                            h2_before: { type: Type.STRING },\n                            h2_after: { type: Type.STRING },\n                            h2_reason: { type: Type.STRING },\n                            h2_options: {\n                                type: Type.ARRAY,\n                                items: {\n                                    type: Type.OBJECT,\n                                    properties: {\n                                        text: { type: Type.STRING },\n                                        reason: { type: Type.STRING },\n                                    },\n                                    required: ['text']\n                                }\n                            },\n                            h3: {\n                                type: Type.ARRAY,\n                                items: {\n                                    type: Type.OBJECT,\n                                    properties: {\n                                        h3_before: { type: Type.STRING },\n                                        h3_after: { type: Type.STRING },\n                                        h3_reason: { type: Type.STRING },\n                                    },\n                                    required: ['h3_after']\n                                }\n                            }\n                        },\n                        required: ['h2_before', 'h2_after', 'h2_options', 'h3']\n                    }\n                }\n            },\n            required: ['headings']\n        });\n\n        const list = Array.isArray(response.data?.headings) ? response.data.headings : [];\n\n        // 1. Prepare for Batch Embedding\n        const allTextsToEmbed: string[] = [];\n        const preparedItems: any[] = [];\n\n        for (let idx = 0; idx < batch.length; idx++) {\n            const beforeRaw = batch[idx] || '';\n            const before = cleanHeading(beforeRaw);\n            const match = list.find((h: any) =>\n                cleanHeading(h?.h2_before) === before ||\n                cleanHeading(h?.before) === before\n            ) || list[idx] || {};\n\n            const h2Before = cleanHeading(match?.h2_before || match?.before || beforeRaw) || before;\n            const options = normalizeOptions(match?.h2_options);\n            const rawAfter = typeof match?.h2_after === 'string'\n                ? match.h2_after\n                : (typeof match?.after === 'string' ? match.after : beforeRaw);\n\n            const h3 = normalizeH3(Array.isArray(match?.h3) ? match.h3 : []);\n            const h2Reason = typeof match?.h2_reason === 'string'\n                ? match.h2_reason\n                : (typeof match?.reason === 'string' ? match.reason : '');\n\n            // Record indices for embedding\n            const baseIndex = allTextsToEmbed.length;\n            allTextsToEmbed.push(h2Before);\n\n            const optionIndices = options.map(opt => {\n                const index = allTextsToEmbed.length;\n                allTextsToEmbed.push(opt.text);\n                return index;\n            });\n\n            preparedItems.push({\n                before,\n                h2Before,\n                options,\n                rawAfter,\n                h3,\n                h2Reason,\n                baseIndex,\n                optionIndices\n            });\n        }\n\n        // 2. Execute Batch Embedding\n        let allEmbeddings: number[][] = [];\n        if (allTextsToEmbed.length > 0) {\n            try {\n                logger.log('refining_headings', `Batch embedding ${allTextsToEmbed.length} texts...`);\n                allEmbeddings = await embedTexts(allTextsToEmbed);\n            } catch (err) {\n                logger.warn('refining_headings', 'Batch embedding failed', err);\n            }\n        }\n\n        // 3. Process Results with Embeddings\n        const results: HeadingResult[] = preparedItems.map(item => {\n            const { before, h2Before, options, rawAfter, h3, h2Reason, baseIndex, optionIndices } = item;\n\n            const baseEmbedding = allEmbeddings[baseIndex];\n            const optionEmbeddings = optionIndices.map((idx: number) => allEmbeddings[idx]);\n\n            const { text: picked, optionsWithScores, needsManual } = calculateBestOption(\n                h2Before,\n                options,\n                rawAfter || before,\n                baseEmbedding,\n                optionEmbeddings\n            );\n\n            return {\n                before,\n                after: picked || h2Before,\n                h2_before: h2Before,\n                h2_after: picked,\n                ...(h2Reason ? { h2_reason: h2Reason } : {}),\n                ...(optionsWithScores.length ? { h2_options: optionsWithScores } : {}),\n                ...(h3 ? { h3 } : {}),\n                ...(needsManual ? { needs_manual: true } : {})\n            };\n        });\n\n        return {\n            data: results,\n            usage: toTokenUsage(response.usage),\n            cost: toCost(response.cost),\n            duration: Date.now() - started\n        };\n    };\n\n    if (headings.length <= BATCH_SIZE) {\n        const single = await refineBatch(headings);\n        return { ...single, duration: Date.now() - started };\n    }\n\n    let aggregatedResults: HeadingResult[] = [];\n    let totalUsage: TokenUsage = { inputTokens: 0, outputTokens: 0, totalTokens: 0 };\n    let totalCost: CostBreakdown = { inputCost: 0, outputCost: 0, totalCost: 0 };\n\n    for (let i = 0; i < headings.length; i += BATCH_SIZE) {\n        const slice = headings.slice(i, i + BATCH_SIZE);\n        const batchRes = await refineBatch(slice);\n        aggregatedResults = aggregatedResults.concat(batchRes.data || []);\n        totalUsage = mergeUsage(totalUsage, batchRes.usage);\n        totalCost = mergeCost(totalCost, batchRes.cost);\n    }\n\n    return {\n        data: aggregatedResults,\n        usage: totalUsage,\n        cost: totalCost,\n        duration: Date.now() - started\n    };\n};\n"],"names":[],"mappings":";;;;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;AAEA,MAAM,eAAe,CAAC,IAClB,CAAC,KAAK,EAAE,EACH,OAAO,CAAC,UAAU,IAClB,OAAO,CAAC,SAAS,IACjB,OAAO,CAAC,UAAU,IAClB,IAAI;AAEb,MAAM,kBAAkB,CAAC;IACrB,MAAM,OAAO,aAAa,UAAU,OAAO,CAAC,UAAU;IACtD,IAAI,CAAC,MAAM,OAAO;IAClB,IAAI,KAAK,MAAM,GAAG,IAAI,OAAO,GAAG,KAAK,EAAE,CAAC;IACxC,OAAO,GAAG,KAAK,EAAE,CAAC;AACtB;AAEA,MAAM,cAAc,CAAC;IACjB,MAAM,aAAa,MAAM,OAAO,CAAC,QAC3B,KACG,MAAM,CAAC,CAAC,IAAW,KAAK,CAAC,EAAE,SAAS,IAAI,EAAE,QAAQ,IAAI,EAAE,MAAM,IAAI,EAAE,KAAK,GACzE,GAAG,CAAC,CAAC;QACF,MAAM,WAAW,aAAa,EAAE,SAAS,IAAI,EAAE,MAAM,IAAI;QACzD,MAAM,aAAa,OAAO,EAAE,QAAQ,KAAK,WAAW,EAAE,QAAQ,GAAI,EAAE,KAAK,IAAI;QAC7E,MAAM,UAAU,aAAa,eAAe;QAC5C,MAAM,WAAW,OAAO,EAAE,SAAS,KAAK,WAAW,EAAE,SAAS,GAAI,OAAO,EAAE,MAAM,KAAK,WAAW,EAAE,MAAM,GAAG;QAC5G,OAAO;YAAE,WAAW;YAAU,UAAU;YAAS,GAAI,WAAW;gBAAE,WAAW;YAAS,IAAI,CAAC,CAAC;QAAE;IAClG,KACF,EAAE;IACR,OAAO,WAAW,MAAM,GAAG,aAAa;AAC5C;AAEA,MAAM,mBAAmB,CAAC;IACtB,IAAI,CAAC,MAAM,OAAO,CAAC,QAAQ,OAAO,EAAE;IACpC,MAAM,OAAO,IAAI;IACjB,MAAM,UAA2B,EAAE;IAEnC,MAAM,OAAO,CAAC,CAAC;QACX,MAAM,OAAO,aAAa,OAAO,QAAQ,WAAW,MAAM,KAAK;QAC/D,IAAI,CAAC,MAAM;QACX,MAAM,MAAM,KAAK,WAAW;QAC5B,IAAI,KAAK,GAAG,CAAC,MAAM;QACnB,KAAK,GAAG,CAAC;QACT,MAAM,SAAS,OAAO,KAAK,WAAW,WAAW,IAAI,MAAM,CAAC,IAAI,KAAK;QACrE,QAAQ,IAAI,CAAC;YAAE;YAAM,GAAI,SAAS;gBAAE;YAAO,IAAI,CAAC,CAAC;QAAE;IACvD;IAEA,OAAO;AACX;AAEA,MAAM,sBAAsB,CACxB,UACA,SACA,UACA,eACA;IAEA,MAAM,cAAc,aAAa;IACjC,MAAM,gBAAgB,aAAa,aAAa;IAEhD,IAAI,CAAC,QAAQ,MAAM,EAAE;QACjB,OAAO;YAAE,MAAM;YAAe,mBAAmB,EAAE;YAAE,aAAa;QAAK;IAC3E;IAEA,MAAM,SAAS,QAAQ,GAAG,CAAC,CAAC,KAAK;QAC7B,MAAM,MAAM,gBAAgB,CAAC,IAAI;QACjC,MAAM,QAAQ,AAAC,iBAAiB,MAAO,IAAA,iKAAgB,EAAC,eAAe,OAAO;QAC9E,OAAO;YAAE,GAAG,GAAG;YAAE;QAAM;IAC3B;IAEA,MAAM,QAAQ,OAAO,MAAM,CAAC,CAAA,MACxB,IAAI,IAAI,IACR,IAAI,IAAI,CAAC,WAAW,OAAO,YAAY,WAAW;IAGtD,MAAM,OAAO,MAAM,MAAM,CAAuB,CAAC,KAAK;QAClD,MAAM,eAAe,IAAI,KAAK,IAAI,CAAC;QACnC,MAAM,WAAW,KAAK,SAAS,CAAC;QAChC,OAAO,eAAe,WAAW,MAAM;IAC3C,GAAG;IAEH,IAAI,CAAC,MAAM;QACP,OAAO;YAAE,MAAM;YAAe,mBAAmB;YAAQ,aAAa;QAAK;IAC/E;IAEA,MAAM,cAAc,CAAC,KAAK,KAAK,IAAI,CAAC,IAAI,SAAS,KAAK,IAAI,CAAC,WAAW,OAAO,YAAY,WAAW;IACpG,OAAO;QAAE,MAAM,KAAK,IAAI,IAAI;QAAe,mBAAmB;QAAQ;IAAY;AACtF;AAEA,MAAM,SAAS,CAAC,OAA6B,CAAC;QAC1C,WAAW,MAAM,aAAa;QAC9B,YAAY,MAAM,cAAc;QAChC,WAAW,MAAM,aAAa;IAClC,CAAC;AAEM,MAAM,iBAAiB,OAC1B,cACA,UACA;IAEA,MAAM,UAAU,KAAK,GAAG;IACxB,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IACnD,MAAM,aAAa;IAEnB,MAAM,aAAa,CAAC,GAAe,IAA8B,CAAC;YAC9D,aAAa,CAAC,EAAE,WAAW,IAAI,CAAC,IAAI,CAAC,EAAE,WAAW,IAAI,CAAC;YACvD,cAAc,CAAC,EAAE,YAAY,IAAI,CAAC,IAAI,CAAC,EAAE,YAAY,IAAI,CAAC;YAC1D,aAAa,CAAC,EAAE,WAAW,IAAI,CAAC,IAAI,CAAC,EAAE,WAAW,IAAI,CAAC;QAC3D,CAAC;IAED,MAAM,YAAY,CAAC,GAAkB,IAAoC,CAAC;YACtE,WAAW,CAAC,EAAE,SAAS,IAAI,CAAC,IAAI,CAAC,EAAE,SAAS,IAAI,CAAC;YACjD,YAAY,CAAC,EAAE,UAAU,IAAI,CAAC,IAAI,CAAC,EAAE,UAAU,IAAI,CAAC;YACpD,WAAW,CAAC,EAAE,SAAS,IAAI,CAAC,IAAI,CAAC,EAAE,SAAS,IAAI,CAAC;QACrD,CAAC;IAED,MAAM,cAAc,OAAO;QACvB,gIAAM,CAAC,GAAG,CAAC,qBAAqB,CAAC,SAAS,EAAE,MAAM,MAAM,CAAC,YAAY,CAAC;QACtE,MAAM,SAAS,+JAAe,CAAC,mBAAmB,CAAC;YAAE;YAAc,UAAU;YAAO;QAAoB;QAExG,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CAAsB,QAAQ,SAAS;YAC3E,MAAM,gJAAI,CAAC,MAAM;YACjB,YAAY;gBACR,UAAU;oBACN,MAAM,gJAAI,CAAC,KAAK;oBAChB,OAAO;wBACH,MAAM,gJAAI,CAAC,MAAM;wBACjB,YAAY;4BACR,WAAW;gCAAE,MAAM,gJAAI,CAAC,MAAM;4BAAC;4BAC/B,UAAU;gCAAE,MAAM,gJAAI,CAAC,MAAM;4BAAC;4BAC9B,WAAW;gCAAE,MAAM,gJAAI,CAAC,MAAM;4BAAC;4BAC/B,YAAY;gCACR,MAAM,gJAAI,CAAC,KAAK;gCAChB,OAAO;oCACH,MAAM,gJAAI,CAAC,MAAM;oCACjB,YAAY;wCACR,MAAM;4CAAE,MAAM,gJAAI,CAAC,MAAM;wCAAC;wCAC1B,QAAQ;4CAAE,MAAM,gJAAI,CAAC,MAAM;wCAAC;oCAChC;oCACA,UAAU;wCAAC;qCAAO;gCACtB;4BACJ;4BACA,IAAI;gCACA,MAAM,gJAAI,CAAC,KAAK;gCAChB,OAAO;oCACH,MAAM,gJAAI,CAAC,MAAM;oCACjB,YAAY;wCACR,WAAW;4CAAE,MAAM,gJAAI,CAAC,MAAM;wCAAC;wCAC/B,UAAU;4CAAE,MAAM,gJAAI,CAAC,MAAM;wCAAC;wCAC9B,WAAW;4CAAE,MAAM,gJAAI,CAAC,MAAM;wCAAC;oCACnC;oCACA,UAAU;wCAAC;qCAAW;gCAC1B;4BACJ;wBACJ;wBACA,UAAU;4BAAC;4BAAa;4BAAY;4BAAc;yBAAK;oBAC3D;gBACJ;YACJ;YACA,UAAU;gBAAC;aAAW;QAC1B;QAEA,MAAM,OAAO,MAAM,OAAO,CAAC,SAAS,IAAI,EAAE,YAAY,SAAS,IAAI,CAAC,QAAQ,GAAG,EAAE;QAEjF,iCAAiC;QACjC,MAAM,kBAA4B,EAAE;QACpC,MAAM,gBAAuB,EAAE;QAE/B,IAAK,IAAI,MAAM,GAAG,MAAM,MAAM,MAAM,EAAE,MAAO;YACzC,MAAM,YAAY,KAAK,CAAC,IAAI,IAAI;YAChC,MAAM,SAAS,aAAa;YAC5B,MAAM,QAAQ,KAAK,IAAI,CAAC,CAAC,IACrB,aAAa,GAAG,eAAe,UAC/B,aAAa,GAAG,YAAY,WAC3B,IAAI,CAAC,IAAI,IAAI,CAAC;YAEnB,MAAM,WAAW,aAAa,OAAO,aAAa,OAAO,UAAU,cAAc;YACjF,MAAM,UAAU,iBAAiB,OAAO;YACxC,MAAM,WAAW,OAAO,OAAO,aAAa,WACtC,MAAM,QAAQ,GACb,OAAO,OAAO,UAAU,WAAW,MAAM,KAAK,GAAG;YAExD,MAAM,KAAK,YAAY,MAAM,OAAO,CAAC,OAAO,MAAM,MAAM,EAAE,GAAG,EAAE;YAC/D,MAAM,WAAW,OAAO,OAAO,cAAc,WACvC,MAAM,SAAS,GACd,OAAO,OAAO,WAAW,WAAW,MAAM,MAAM,GAAG;YAE1D,+BAA+B;YAC/B,MAAM,YAAY,gBAAgB,MAAM;YACxC,gBAAgB,IAAI,CAAC;YAErB,MAAM,gBAAgB,QAAQ,GAAG,CAAC,CAAA;gBAC9B,MAAM,QAAQ,gBAAgB,MAAM;gBACpC,gBAAgB,IAAI,CAAC,IAAI,IAAI;gBAC7B,OAAO;YACX;YAEA,cAAc,IAAI,CAAC;gBACf;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;YACJ;QACJ;QAEA,6BAA6B;QAC7B,IAAI,gBAA4B,EAAE;QAClC,IAAI,gBAAgB,MAAM,GAAG,GAAG;YAC5B,IAAI;gBACA,gIAAM,CAAC,GAAG,CAAC,qBAAqB,CAAC,gBAAgB,EAAE,gBAAgB,MAAM,CAAC,SAAS,CAAC;gBACpF,gBAAgB,MAAM,IAAA,2JAAU,EAAC;YACrC,EAAE,OAAO,KAAK;gBACV,gIAAM,CAAC,IAAI,CAAC,qBAAqB,0BAA0B;YAC/D;QACJ;QAEA,qCAAqC;QACrC,MAAM,UAA2B,cAAc,GAAG,CAAC,CAAA;YAC/C,MAAM,EAAE,MAAM,EAAE,QAAQ,EAAE,OAAO,EAAE,QAAQ,EAAE,EAAE,EAAE,QAAQ,EAAE,SAAS,EAAE,aAAa,EAAE,GAAG;YAExF,MAAM,gBAAgB,aAAa,CAAC,UAAU;YAC9C,MAAM,mBAAmB,cAAc,GAAG,CAAC,CAAC,MAAgB,aAAa,CAAC,IAAI;YAE9E,MAAM,EAAE,MAAM,MAAM,EAAE,iBAAiB,EAAE,WAAW,EAAE,GAAG,oBACrD,UACA,SACA,YAAY,QACZ,eACA;YAGJ,OAAO;gBACH;gBACA,OAAO,UAAU;gBACjB,WAAW;gBACX,UAAU;gBACV,GAAI,WAAW;oBAAE,WAAW;gBAAS,IAAI,CAAC,CAAC;gBAC3C,GAAI,kBAAkB,MAAM,GAAG;oBAAE,YAAY;gBAAkB,IAAI,CAAC,CAAC;gBACrE,GAAI,KAAK;oBAAE;gBAAG,IAAI,CAAC,CAAC;gBACpB,GAAI,cAAc;oBAAE,cAAc;gBAAK,IAAI,CAAC,CAAC;YACjD;QACJ;QAEA,OAAO;YACH,MAAM;YACN,OAAO,IAAA,0JAAY,EAAC,SAAS,KAAK;YAClC,MAAM,OAAO,SAAS,IAAI;YAC1B,UAAU,KAAK,GAAG,KAAK;QAC3B;IACJ;IAEA,IAAI,SAAS,MAAM,IAAI,YAAY;QAC/B,MAAM,SAAS,MAAM,YAAY;QACjC,OAAO;YAAE,GAAG,MAAM;YAAE,UAAU,KAAK,GAAG,KAAK;QAAQ;IACvD;IAEA,IAAI,oBAAqC,EAAE;IAC3C,IAAI,aAAyB;QAAE,aAAa;QAAG,cAAc;QAAG,aAAa;IAAE;IAC/E,IAAI,YAA2B;QAAE,WAAW;QAAG,YAAY;QAAG,WAAW;IAAE;IAE3E,IAAK,IAAI,IAAI,GAAG,IAAI,SAAS,MAAM,EAAE,KAAK,WAAY;QAClD,MAAM,QAAQ,SAAS,KAAK,CAAC,GAAG,IAAI;QACpC,MAAM,WAAW,MAAM,YAAY;QACnC,oBAAoB,kBAAkB,MAAM,CAAC,SAAS,IAAI,IAAI,EAAE;QAChE,aAAa,WAAW,YAAY,SAAS,KAAK;QAClD,YAAY,UAAU,WAAW,SAAS,IAAI;IAClD;IAEA,OAAO;QACH,MAAM;QACN,OAAO;QACP,MAAM;QACN,UAAU,KAAK,GAAG,KAAK;IAC3B;AACJ"}},
    {"offset": {"line": 3606, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/engine/nlpService.ts"],"sourcesContent":["import { KeywordData } from '../../types';\n\ninterface NlpResponse {\n  status: string;\n  data: {\n    tokens: string[];\n    frequencies: {\n      token: string;\n      count: number;\n    }[];\n  };\n}\n\nexport const analyzeText = async (text: string): Promise<KeywordData[]> => {\n  if (!text || text.trim().length === 0) return [];\n\n  // External NLP API Endpoint\n  const TARGET_URL = 'https://nlp.award-seo.com/api/v1/tokenize';\n  // CORS Proxy to bypass browser restrictions\n  const PROXY_URL = 'https://corsproxy.io/?';\n\n  try {\n    const response = await fetch(PROXY_URL + encodeURIComponent(TARGET_URL), {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({\n        text: text,\n        min_length: 2,\n        stop_words: [] // Empty array as per requirement, can be populated if needed\n      }),\n    });\n\n    if (!response.ok) {\n      console.warn('NLP API Error:', response.status, response.statusText);\n      return [];\n    }\n\n    const json: NlpResponse = await response.json();\n\n    if (json.status === 'ok' && json.data && json.data.frequencies) {\n      // Sort by count descending to get high-frequency words first\n      // Filter out 1-character tokens generally, though API min_length handles most\n      return json.data.frequencies\n        .filter(f => f.token.length > 1)\n        .sort((a, b) => b.count - a.count);\n    }\n\n    return [];\n  } catch (error) {\n    console.error(\"NLP Service Error:\", error);\n    return [];\n  }\n};\n"],"names":[],"mappings":";;;;AAaO,MAAM,cAAc,OAAO;IAChC,IAAI,CAAC,QAAQ,KAAK,IAAI,GAAG,MAAM,KAAK,GAAG,OAAO,EAAE;IAEhD,4BAA4B;IAC5B,MAAM,aAAa;IACnB,4CAA4C;IAC5C,MAAM,YAAY;IAElB,IAAI;QACF,MAAM,WAAW,MAAM,MAAM,YAAY,mBAAmB,aAAa;YACvE,QAAQ;YACR,SAAS;gBACP,gBAAgB;YAClB;YACA,MAAM,KAAK,SAAS,CAAC;gBACnB,MAAM;gBACN,YAAY;gBACZ,YAAY,EAAE,CAAC,6DAA6D;YAC9E;QACF;QAEA,IAAI,CAAC,SAAS,EAAE,EAAE;YAChB,QAAQ,IAAI,CAAC,kBAAkB,SAAS,MAAM,EAAE,SAAS,UAAU;YACnE,OAAO,EAAE;QACX;QAEA,MAAM,OAAoB,MAAM,SAAS,IAAI;QAE7C,IAAI,KAAK,MAAM,KAAK,QAAQ,KAAK,IAAI,IAAI,KAAK,IAAI,CAAC,WAAW,EAAE;YAC9D,6DAA6D;YAC7D,8EAA8E;YAC9E,OAAO,KAAK,IAAI,CAAC,WAAW,CACzB,MAAM,CAAC,CAAA,IAAK,EAAE,KAAK,CAAC,MAAM,GAAG,GAC7B,IAAI,CAAC,CAAC,GAAG,IAAM,EAAE,KAAK,GAAG,EAAE,KAAK;QACrC;QAEA,OAAO,EAAE;IACX,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,sBAAsB;QACpC,OAAO,EAAE;IACX;AACF"}},
    {"offset": {"line": 3648, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/research/termUsagePlanner.ts"],"sourcesContent":["import { ServiceResponse, FrequentWordsPlacementAnalysis, KeywordData, TargetAudience } from '../../types';\nimport { SEMANTIC_KEYWORD_LIMIT } from '../../config/constants';\nimport { extractRawSnippets, getLanguageInstruction, toTokenUsage } from '../engine/promptService';\nimport { promptTemplates } from '../engine/promptTemplates';\nimport { aiService } from '../engine/aiService';\nimport { Type } from '../engine/schemaTypes';\n\n// Helper to chunk array\nconst chunkArray = <T>(array: T[], size: number): T[][] => {\n    const chunked: T[][] = [];\n    for (let i = 0; i < array.length; i += size) {\n        chunked.push(array.slice(i, i + size));\n    }\n    return chunked;\n};\n\nimport pLimit from 'p-limit'; // v6+ is pure ESM\n\n// ... imports\n\n// Analyze Context & Generate Action Plan for keywords\nexport const extractSemanticKeywordsAnalysis = async (\n    referenceContent: string,\n    keywords: KeywordData[],\n    targetAudience: TargetAudience\n): Promise<ServiceResponse<FrequentWordsPlacementAnalysis[]>> => {\n    const startTs = Date.now();\n\n    // 1. Deduplicate keywords (Case-insensitive) to prevent processing duplicates\n    const uniqueKeywordsMap = new Map<string, KeywordData>();\n    keywords.forEach(k => {\n        const lower = (k.token || '').toLowerCase().trim();\n        if (lower && !uniqueKeywordsMap.has(lower)) {\n            uniqueKeywordsMap.set(lower, k);\n        }\n    });\n    const uniqueKeywords = Array.from(uniqueKeywordsMap.values());\n\n    // Take top keywords to avoid token limits (magic number tuned for speed/cost)\n    const topKeywords = uniqueKeywords.slice(0, SEMANTIC_KEYWORD_LIMIT);\n\n    const truncateSnippet = (text: string, maxLen: number = 160) =>\n        text.length > maxLen ? `${text.slice(0, maxLen - 3)}...` : text;\n\n    // Prepare snippets for ALL keywords first\n    const allAnalysisPayloads = topKeywords.map(k => ({\n        word: k.token,\n        snippets: extractRawSnippets(referenceContent, k.token, 80)\n            .slice(0, 2)\n            .map(snippet => truncateSnippet(snippet))\n    }));\n\n    const languageInstruction = getLanguageInstruction(targetAudience);\n\n    // BATCHING STRATEGY:\n    // Execute multiple batches in parallel but with a CONCURRENCY LIMIT\n    // to prevent 429 errors or network timeouts.\n    const BATCH_SIZE = 10;\n    const CONCURRENCY_LIMIT = 2; // Slightly reduced for more stable proxy handling\n    const limit = pLimit(CONCURRENCY_LIMIT);\n\n    const batches = chunkArray(allAnalysisPayloads, BATCH_SIZE);\n\n    console.log(`[SemanticKeywords] Processing ${allAnalysisPayloads.length} words in ${batches.length} batches (Concurrency: ${CONCURRENCY_LIMIT})...`);\n\n    const batchPromises = batches.map((batchPayload, batchIdx) => limit(async () => {\n        // Breath delay: Space out batch starts by 1.2s to prevent spike failures\n        const delayMs = batchIdx * 1200;\n        if (delayMs > 0) {\n            await new Promise(resolve => setTimeout(resolve, delayMs));\n        }\n\n        // Stringify the analysis payload for this batch\n        const analysisPayloadString = JSON.stringify(batchPayload, null, 2);\n\n        // Use the registry to build the prompt with snippet context\n        const planPrompt = promptTemplates.frequentWordsPlacementAnalysis({\n            analysisPayloadString,\n            languageInstruction\n        });\n\n        try {\n            console.log(`[SemanticKeywords] Starting batch ${batchIdx + 1}/${batches.length}...`);\n            const planRes = await aiService.runJson<any[]>(\n                planPrompt,\n                'FLASH',\n                {\n                    type: Type.ARRAY,\n                    items: {\n                        type: Type.OBJECT,\n                        properties: {\n                            word: { type: Type.STRING },\n                            plan: { type: Type.ARRAY, items: { type: Type.STRING } },\n                            exampleSentence: { type: Type.STRING },\n                            isSentenceStart: { type: Type.BOOLEAN },\n                            isSentenceEnd: { type: Type.BOOLEAN },\n                            isPrefix: { type: Type.BOOLEAN },\n                            isSuffix: { type: Type.BOOLEAN }\n                        },\n                        required: [\"word\", \"plan\", \"exampleSentence\"]\n                    }\n                }\n            );\n\n            console.log(`[SemanticKeywords] Batch ${batchIdx + 1} Result:`, {\n                requested: batchPayload.length,\n                received: planRes.data?.length || 0,\n                duration: planRes.duration\n            });\n\n            if (!planRes.data || planRes.data.length === 0) {\n                console.warn(`[SemanticKeywords] Batch ${batchIdx + 1} returned NO data. Batch payload:`, batchPayload.map(p => p.word));\n                return {\n                    data: [],\n                    usage: planRes.usage,\n                    cost: planRes.cost,\n                    duration: planRes.duration\n                };\n            }\n\n            return {\n                data: planRes.data.map(p => ({\n                    ...p,\n                    plan: Array.isArray(p.plan) ? p.plan : [p.plan].filter(Boolean)\n                })),\n                usage: planRes.usage,\n                cost: planRes.cost,\n                duration: planRes.duration\n            };\n        } catch (e) {\n            console.warn(`[SemanticKeywords] Batch ${batchIdx + 1} failed`, e);\n            return {\n                data: [],\n                usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n                cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n                duration: 0\n            };\n        }\n    }));\n\n    // Execute all batches with the limit\n    const results = await Promise.all(batchPromises);\n\n    // Merge results\n    let mergedPlans: any[] = [];\n    let totalUsage = { inputTokens: 0, outputTokens: 0, totalTokens: 0 };\n    let totalCost = { inputCost: 0, outputCost: 0, totalCost: 0 };\n\n    results.forEach(res => {\n        if (res.data) mergedPlans = mergedPlans.concat(res.data);\n        if (res.usage) {\n            totalUsage.inputTokens += res.usage.inputTokens || 0;\n            totalUsage.outputTokens += res.usage.outputTokens || 0;\n            totalUsage.totalTokens += res.usage.totalTokens || 0;\n        }\n        if (res.cost) {\n            totalCost.inputCost += res.cost.inputCost || 0;\n            totalCost.outputCost += res.cost.outputCost || 0;\n            totalCost.totalCost += res.cost.totalCost || 0;\n        }\n    });\n\n    // Map back to include snippets (using the original payloads)\n    // Also final safety dedupe on the results\n    const seenWords = new Set<string>();\n\n    const finalPlans: FrequentWordsPlacementAnalysis[] = mergedPlans.reduce((acc: FrequentWordsPlacementAnalysis[], p: any) => {\n        const lower = (p.word || '').toLowerCase().trim();\n        if (!lower || seenWords.has(lower)) return acc;\n\n        seenWords.add(lower);\n\n        const original = allAnalysisPayloads.find(ap => ap.word === p.word) ||\n            allAnalysisPayloads.find(ap => (ap.word || '').toLowerCase() === lower);\n\n        acc.push({\n            word: p.word,\n            plan: p.plan || [],\n            snippets: original ? original.snippets : [],\n            exampleSentence: p.exampleSentence || '',\n            isSentenceStart: !!p.isSentenceStart,\n            isSentenceEnd: !!p.isSentenceEnd,\n            isPrefix: !!p.isPrefix,\n            isSuffix: !!p.isSuffix\n        });\n        return acc;\n    }, []);\n\n    return {\n        data: finalPlans,\n        usage: toTokenUsage(totalUsage),\n        cost: totalCost,\n        duration: Date.now() - startTs\n    };\n};\n"],"names":[],"mappings":";;;;AACA;AACA;AACA;AACA;AACA;AAWA,qOAA8B,kBAAkB;;;;;;AAThD,wBAAwB;AACxB,MAAM,aAAa,CAAI,OAAY;IAC/B,MAAM,UAAiB,EAAE;IACzB,IAAK,IAAI,IAAI,GAAG,IAAI,MAAM,MAAM,EAAE,KAAK,KAAM;QACzC,QAAQ,IAAI,CAAC,MAAM,KAAK,CAAC,GAAG,IAAI;IACpC;IACA,OAAO;AACX;;AAOO,MAAM,kCAAkC,OAC3C,kBACA,UACA;IAEA,MAAM,UAAU,KAAK,GAAG;IAExB,8EAA8E;IAC9E,MAAM,oBAAoB,IAAI;IAC9B,SAAS,OAAO,CAAC,CAAA;QACb,MAAM,QAAQ,CAAC,EAAE,KAAK,IAAI,EAAE,EAAE,WAAW,GAAG,IAAI;QAChD,IAAI,SAAS,CAAC,kBAAkB,GAAG,CAAC,QAAQ;YACxC,kBAAkB,GAAG,CAAC,OAAO;QACjC;IACJ;IACA,MAAM,iBAAiB,MAAM,IAAI,CAAC,kBAAkB,MAAM;IAE1D,8EAA8E;IAC9E,MAAM,cAAc,eAAe,KAAK,CAAC,GAAG,oJAAsB;IAElE,MAAM,kBAAkB,CAAC,MAAc,SAAiB,GAAG,GACvD,KAAK,MAAM,GAAG,SAAS,GAAG,KAAK,KAAK,CAAC,GAAG,SAAS,GAAG,GAAG,CAAC,GAAG;IAE/D,0CAA0C;IAC1C,MAAM,sBAAsB,YAAY,GAAG,CAAC,CAAA,IAAK,CAAC;YAC9C,MAAM,EAAE,KAAK;YACb,UAAU,IAAA,gKAAkB,EAAC,kBAAkB,EAAE,KAAK,EAAE,IACnD,KAAK,CAAC,GAAG,GACT,GAAG,CAAC,CAAA,UAAW,gBAAgB;QACxC,CAAC;IAED,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IAEnD,qBAAqB;IACrB,oEAAoE;IACpE,6CAA6C;IAC7C,MAAM,aAAa;IACnB,MAAM,oBAAoB,GAAG,kDAAkD;IAC/E,MAAM,QAAQ,IAAA,8IAAM,EAAC;IAErB,MAAM,UAAU,WAAW,qBAAqB;IAEhD,QAAQ,GAAG,CAAC,CAAC,8BAA8B,EAAE,oBAAoB,MAAM,CAAC,UAAU,EAAE,QAAQ,MAAM,CAAC,uBAAuB,EAAE,kBAAkB,IAAI,CAAC;IAEnJ,MAAM,gBAAgB,QAAQ,GAAG,CAAC,CAAC,cAAc,WAAa,MAAM;YAChE,yEAAyE;YACzE,MAAM,UAAU,WAAW;YAC3B,IAAI,UAAU,GAAG;gBACb,MAAM,IAAI,QAAQ,CAAA,UAAW,WAAW,SAAS;YACrD;YAEA,gDAAgD;YAChD,MAAM,wBAAwB,KAAK,SAAS,CAAC,cAAc,MAAM;YAEjE,4DAA4D;YAC5D,MAAM,aAAa,+JAAe,CAAC,8BAA8B,CAAC;gBAC9D;gBACA;YACJ;YAEA,IAAI;gBACA,QAAQ,GAAG,CAAC,CAAC,kCAAkC,EAAE,WAAW,EAAE,CAAC,EAAE,QAAQ,MAAM,CAAC,GAAG,CAAC;gBACpF,MAAM,UAAU,MAAM,mJAAS,CAAC,OAAO,CACnC,YACA,SACA;oBACI,MAAM,gJAAI,CAAC,KAAK;oBAChB,OAAO;wBACH,MAAM,gJAAI,CAAC,MAAM;wBACjB,YAAY;4BACR,MAAM;gCAAE,MAAM,gJAAI,CAAC,MAAM;4BAAC;4BAC1B,MAAM;gCAAE,MAAM,gJAAI,CAAC,KAAK;gCAAE,OAAO;oCAAE,MAAM,gJAAI,CAAC,MAAM;gCAAC;4BAAE;4BACvD,iBAAiB;gCAAE,MAAM,gJAAI,CAAC,MAAM;4BAAC;4BACrC,iBAAiB;gCAAE,MAAM,gJAAI,CAAC,OAAO;4BAAC;4BACtC,eAAe;gCAAE,MAAM,gJAAI,CAAC,OAAO;4BAAC;4BACpC,UAAU;gCAAE,MAAM,gJAAI,CAAC,OAAO;4BAAC;4BAC/B,UAAU;gCAAE,MAAM,gJAAI,CAAC,OAAO;4BAAC;wBACnC;wBACA,UAAU;4BAAC;4BAAQ;4BAAQ;yBAAkB;oBACjD;gBACJ;gBAGJ,QAAQ,GAAG,CAAC,CAAC,yBAAyB,EAAE,WAAW,EAAE,QAAQ,CAAC,EAAE;oBAC5D,WAAW,aAAa,MAAM;oBAC9B,UAAU,QAAQ,IAAI,EAAE,UAAU;oBAClC,UAAU,QAAQ,QAAQ;gBAC9B;gBAEA,IAAI,CAAC,QAAQ,IAAI,IAAI,QAAQ,IAAI,CAAC,MAAM,KAAK,GAAG;oBAC5C,QAAQ,IAAI,CAAC,CAAC,yBAAyB,EAAE,WAAW,EAAE,iCAAiC,CAAC,EAAE,aAAa,GAAG,CAAC,CAAA,IAAK,EAAE,IAAI;oBACtH,OAAO;wBACH,MAAM,EAAE;wBACR,OAAO,QAAQ,KAAK;wBACpB,MAAM,QAAQ,IAAI;wBAClB,UAAU,QAAQ,QAAQ;oBAC9B;gBACJ;gBAEA,OAAO;oBACH,MAAM,QAAQ,IAAI,CAAC,GAAG,CAAC,CAAA,IAAK,CAAC;4BACzB,GAAG,CAAC;4BACJ,MAAM,MAAM,OAAO,CAAC,EAAE,IAAI,IAAI,EAAE,IAAI,GAAG;gCAAC,EAAE,IAAI;6BAAC,CAAC,MAAM,CAAC;wBAC3D,CAAC;oBACD,OAAO,QAAQ,KAAK;oBACpB,MAAM,QAAQ,IAAI;oBAClB,UAAU,QAAQ,QAAQ;gBAC9B;YACJ,EAAE,OAAO,GAAG;gBACR,QAAQ,IAAI,CAAC,CAAC,yBAAyB,EAAE,WAAW,EAAE,OAAO,CAAC,EAAE;gBAChE,OAAO;oBACH,MAAM,EAAE;oBACR,OAAO;wBAAE,aAAa;wBAAG,cAAc;wBAAG,aAAa;oBAAE;oBACzD,MAAM;wBAAE,WAAW;wBAAG,YAAY;wBAAG,WAAW;oBAAE;oBAClD,UAAU;gBACd;YACJ;QACJ;IAEA,qCAAqC;IACrC,MAAM,UAAU,MAAM,QAAQ,GAAG,CAAC;IAElC,gBAAgB;IAChB,IAAI,cAAqB,EAAE;IAC3B,IAAI,aAAa;QAAE,aAAa;QAAG,cAAc;QAAG,aAAa;IAAE;IACnE,IAAI,YAAY;QAAE,WAAW;QAAG,YAAY;QAAG,WAAW;IAAE;IAE5D,QAAQ,OAAO,CAAC,CAAA;QACZ,IAAI,IAAI,IAAI,EAAE,cAAc,YAAY,MAAM,CAAC,IAAI,IAAI;QACvD,IAAI,IAAI,KAAK,EAAE;YACX,WAAW,WAAW,IAAI,IAAI,KAAK,CAAC,WAAW,IAAI;YACnD,WAAW,YAAY,IAAI,IAAI,KAAK,CAAC,YAAY,IAAI;YACrD,WAAW,WAAW,IAAI,IAAI,KAAK,CAAC,WAAW,IAAI;QACvD;QACA,IAAI,IAAI,IAAI,EAAE;YACV,UAAU,SAAS,IAAI,IAAI,IAAI,CAAC,SAAS,IAAI;YAC7C,UAAU,UAAU,IAAI,IAAI,IAAI,CAAC,UAAU,IAAI;YAC/C,UAAU,SAAS,IAAI,IAAI,IAAI,CAAC,SAAS,IAAI;QACjD;IACJ;IAEA,6DAA6D;IAC7D,0CAA0C;IAC1C,MAAM,YAAY,IAAI;IAEtB,MAAM,aAA+C,YAAY,MAAM,CAAC,CAAC,KAAuC;QAC5G,MAAM,QAAQ,CAAC,EAAE,IAAI,IAAI,EAAE,EAAE,WAAW,GAAG,IAAI;QAC/C,IAAI,CAAC,SAAS,UAAU,GAAG,CAAC,QAAQ,OAAO;QAE3C,UAAU,GAAG,CAAC;QAEd,MAAM,WAAW,oBAAoB,IAAI,CAAC,CAAA,KAAM,GAAG,IAAI,KAAK,EAAE,IAAI,KAC9D,oBAAoB,IAAI,CAAC,CAAA,KAAM,CAAC,GAAG,IAAI,IAAI,EAAE,EAAE,WAAW,OAAO;QAErE,IAAI,IAAI,CAAC;YACL,MAAM,EAAE,IAAI;YACZ,MAAM,EAAE,IAAI,IAAI,EAAE;YAClB,UAAU,WAAW,SAAS,QAAQ,GAAG,EAAE;YAC3C,iBAAiB,EAAE,eAAe,IAAI;YACtC,iBAAiB,CAAC,CAAC,EAAE,eAAe;YACpC,eAAe,CAAC,CAAC,EAAE,aAAa;YAChC,UAAU,CAAC,CAAC,EAAE,QAAQ;YACtB,UAAU,CAAC,CAAC,EAAE,QAAQ;QAC1B;QACA,OAAO;IACX,GAAG,EAAE;IAEL,OAAO;QACH,MAAM;QACN,OAAO,IAAA,0JAAY,EAAC;QACpB,MAAM;QACN,UAAU,KAAK,GAAG,KAAK;IAC3B;AACJ"}},
    {"offset": {"line": 3853, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/research/authorityService.ts"],"sourcesContent":["import { ServiceResponse, AuthorityAnalysis, TargetAudience } from '../../types';\nimport { aiService } from '../engine/aiService';\nimport { promptTemplates } from '../engine/promptTemplates';\nimport { Type } from '../engine/schemaTypes';\nimport { getLanguageInstruction, toTokenUsage } from '../engine/promptService';\n\nexport const analyzeAuthorityTerms = async (\n    authorityTerms: string,\n    articleTitle: string,\n    websiteType: string,\n    targetAudience: TargetAudience\n): Promise<ServiceResponse<AuthorityAnalysis>> => {\n    const startTs = Date.now();\n\n    // Truncate authorityTerms to roughly 3000 chars to prevent timeout/overload\n    const truncatedTerms = authorityTerms.length > 3000\n        ? authorityTerms.slice(0, 3000) + \"...(truncated)\"\n        : authorityTerms;\n\n    // Use the registry to build the prompt\n    const languageInstruction = getLanguageInstruction(targetAudience);\n    const prompt = promptTemplates.authorityAnalysis({ authorityTerms: truncatedTerms, title: articleTitle, websiteType, languageInstruction });\n\n    try {\n        const response = await aiService.runJson<AuthorityAnalysis>(prompt, 'FLASH', {\n            type: Type.OBJECT,\n            properties: {\n                relevantTerms: {\n                    type: Type.ARRAY,\n                    items: { type: Type.STRING },\n                    description: \"Filtered high-relevance authority terms\"\n                },\n                combinations: {\n                    type: Type.ARRAY,\n                    items: { type: Type.STRING },\n                    description: \"Strategic ways to combine these terms\"\n                }\n            },\n            required: [\"relevantTerms\", \"combinations\"]\n        });\n\n        return {\n            data: response.data,\n            usage: toTokenUsage(response.usage),\n            cost: response.cost,\n            duration: response.duration\n        };\n\n    } catch (e) {\n        console.error(\"Authority analysis failed\", e);\n        const usage = toTokenUsage((e as any)?.usage);\n        return {\n            data: {\n                relevantTerms: [],\n                combinations: []\n            },\n            usage,\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: Date.now() - startTs\n        };\n    }\n};\n"],"names":[],"mappings":";;;;AACA;AACA;AACA;AACA;;;;;AAEO,MAAM,wBAAwB,OACjC,gBACA,cACA,aACA;IAEA,MAAM,UAAU,KAAK,GAAG;IAExB,4EAA4E;IAC5E,MAAM,iBAAiB,eAAe,MAAM,GAAG,OACzC,eAAe,KAAK,CAAC,GAAG,QAAQ,mBAChC;IAEN,uCAAuC;IACvC,MAAM,sBAAsB,IAAA,oKAAsB,EAAC;IACnD,MAAM,SAAS,+JAAe,CAAC,iBAAiB,CAAC;QAAE,gBAAgB;QAAgB,OAAO;QAAc;QAAa;IAAoB;IAEzI,IAAI;QACA,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CAAoB,QAAQ,SAAS;YACzE,MAAM,gJAAI,CAAC,MAAM;YACjB,YAAY;gBACR,eAAe;oBACX,MAAM,gJAAI,CAAC,KAAK;oBAChB,OAAO;wBAAE,MAAM,gJAAI,CAAC,MAAM;oBAAC;oBAC3B,aAAa;gBACjB;gBACA,cAAc;oBACV,MAAM,gJAAI,CAAC,KAAK;oBAChB,OAAO;wBAAE,MAAM,gJAAI,CAAC,MAAM;oBAAC;oBAC3B,aAAa;gBACjB;YACJ;YACA,UAAU;gBAAC;gBAAiB;aAAe;QAC/C;QAEA,OAAO;YACH,MAAM,SAAS,IAAI;YACnB,OAAO,IAAA,0JAAY,EAAC,SAAS,KAAK;YAClC,MAAM,SAAS,IAAI;YACnB,UAAU,SAAS,QAAQ;QAC/B;IAEJ,EAAE,OAAO,GAAG;QACR,QAAQ,KAAK,CAAC,6BAA6B;QAC3C,MAAM,QAAQ,IAAA,0JAAY,EAAE,GAAW;QACvC,OAAO;YACH,MAAM;gBACF,eAAe,EAAE;gBACjB,cAAc,EAAE;YACpB;YACA;YACA,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU,KAAK,GAAG,KAAK;QAC3B;IACJ;AACJ"}},
    {"offset": {"line": 3929, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/research/regionalAnalysisService.ts"],"sourcesContent":["import { aiService } from '../engine/aiService';\nimport { promptTemplates } from '../engine/promptTemplates';\nimport { ServiceResponse } from '../../types';\nimport { Type } from '../engine/schemaTypes';\n\nexport const analyzeRegionalTerms = async (\n    content: string,\n    targetAudience: string\n): Promise<ServiceResponse<{ original: string; replacement: string; reason: string }[]>> => {\n\n    // Safety check: if content is too short, skip\n    if (!content || content.length < 50) {\n        return {\n            data: [],\n            usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: 0\n        };\n    }\n\n    const startTs = Date.now();\n    const prompt = promptTemplates.regionalBrandAnalysis({ content, targetAudience });\n\n    // Use Gemini with Grounding enabled (if configured in aiService via 'FLASH' or specific model)\n    // The prompt explicitly asks for \"Use Google Search (Grounding)\" which requires the model to have tools support \n    // or just relying on its internal knowledge if tools aren't active. \n    // Assuming 'FLASH' mapped to a model that supports this or the prompt is enough.\n\n    const response = await aiService.runJson<{ original: string; replacement: string; reason: string }[]>(\n        prompt,\n        'FLASH',\n        {\n            type: Type.ARRAY,\n            items: {\n                type: Type.OBJECT,\n                properties: {\n                    original: { type: Type.STRING },\n                    replacement: { type: Type.STRING },\n                    reason: { type: Type.STRING }\n                },\n                required: [\"original\", \"replacement\", \"reason\"]\n            }\n        }\n    );\n\n    return {\n        data: response.data || [],\n        usage: response.usage,\n        cost: response.cost,\n        duration: Date.now() - startTs\n    };\n};\n"],"names":[],"mappings":";;;;AAAA;AACA;AAEA;;;;AAEO,MAAM,uBAAuB,OAChC,SACA;IAGA,8CAA8C;IAC9C,IAAI,CAAC,WAAW,QAAQ,MAAM,GAAG,IAAI;QACjC,OAAO;YACH,MAAM,EAAE;YACR,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YACzD,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU;QACd;IACJ;IAEA,MAAM,UAAU,KAAK,GAAG;IACxB,MAAM,SAAS,+JAAe,CAAC,qBAAqB,CAAC;QAAE;QAAS;IAAe;IAE/E,+FAA+F;IAC/F,iHAAiH;IACjH,qEAAqE;IACrE,iFAAiF;IAEjF,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CACpC,QACA,SACA;QACI,MAAM,gJAAI,CAAC,KAAK;QAChB,OAAO;YACH,MAAM,gJAAI,CAAC,MAAM;YACjB,YAAY;gBACR,UAAU;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;gBAC9B,aAAa;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;gBACjC,QAAQ;oBAAE,MAAM,gJAAI,CAAC,MAAM;gBAAC;YAChC;YACA,UAAU;gBAAC;gBAAY;gBAAe;aAAS;QACnD;IACJ;IAGJ,OAAO;QACH,MAAM,SAAS,IAAI,IAAI,EAAE;QACzB,OAAO,SAAS,KAAK;QACrB,MAAM,SAAS,IAAI;QACnB,UAAU,KAAK,GAAG,KAAK;IAC3B;AACJ"}},
    {"offset": {"line": 3999, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/generation/contentDisplayService.ts"],"sourcesContent":["interface TurboSection {\n    title: string;\n}\n\nexport const buildTurboPlaceholder = (sections: TurboSection[], outlineSourceLabel: string): string => {\n    const headerBanner = `> 📑 **Active Blueprint:** ${outlineSourceLabel}\\n\\n`;\n\n    const placeholders = sections.map(s =>\n        `> ⏳ **Writing Section:** ${s.title}...`\n    ).join('\\n\\n');\n\n    return headerBanner + placeholders;\n};\n\nexport const mergeTurboSections = (sections: TurboSection[], sectionContents: string[]): string => {\n    const placeholders = sections.map((s, idx) => {\n        const content = sectionContents[idx];\n        if (content) return content;\n        return `> ⏳ **Writing Section:** ${s.title}...`;\n    }).join('\\n\\n');\n\n    const headerBanner = `> 📑 **Active Blueprint:** Turbo Mode\\n\\n`;\n\n    return headerBanner + placeholders;\n};\n"],"names":[],"mappings":";;;;;;AAIO,MAAM,wBAAwB,CAAC,UAA0B;IAC5D,MAAM,eAAe,CAAC,2BAA2B,EAAE,mBAAmB,IAAI,CAAC;IAE3E,MAAM,eAAe,SAAS,GAAG,CAAC,CAAA,IAC9B,CAAC,yBAAyB,EAAE,EAAE,KAAK,CAAC,GAAG,CAAC,EAC1C,IAAI,CAAC;IAEP,OAAO,eAAe;AAC1B;AAEO,MAAM,qBAAqB,CAAC,UAA0B;IACzD,MAAM,eAAe,SAAS,GAAG,CAAC,CAAC,GAAG;QAClC,MAAM,UAAU,eAAe,CAAC,IAAI;QACpC,IAAI,SAAS,OAAO;QACpB,OAAO,CAAC,yBAAyB,EAAE,EAAE,KAAK,CAAC,GAAG,CAAC;IACnD,GAAG,IAAI,CAAC;IAER,MAAM,eAAe,CAAC,yCAAyC,CAAC;IAEhE,OAAO,eAAe;AAC1B"}},
    {"offset": {"line": 4023, "column": 0}, "map": {"version":3,"sources":["file:///Users/rose/Documents/this_month/write-content/best-ai-text-writer.com/src/services/research/regionGroundingService.ts"],"sourcesContent":["/**\n * Region Grounding Service\n * Uses AI to validate and adapt content for the target market region\n * Supports: zh-TW (Taiwan), zh-HK (Hong Kong), zh-MY (Malaysia)\n */\n\nimport { aiService } from '../engine/aiService';\nimport { TokenUsage, CostBreakdown, TargetAudience } from '../../types';\n\nexport interface RegionIssue {\n    type: 'entity' | 'brand' | 'regulation' | 'currency' | 'location' | 'service';\n    original: string;\n    regionEquivalent: string;\n    confidence: number;\n    context: string;\n    sourceRegion: string;\n}\n\nexport interface RegionGroundingResult {\n    isRegionRelevant: boolean;\n    relevanceScore: number;\n    issues: RegionIssue[];\n    suggestions: { original: string; rewritten: string }[];\n}\n\nconst REGION_CONFIG: Record<TargetAudience, {\n    name: string;\n    currency: string;\n    excludeRegions: string[];\n    examples: string[];\n}> = {\n    'zh-TW': {\n        name: '台灣',\n        currency: 'NT$/新台幣',\n        excludeRegions: ['HK', 'CN', 'MY'],\n        examples: ['蝦皮', 'momo購物', '台北捷運', '健保', '高鐵', '全聯', '7-11']\n    },\n    'zh-HK': {\n        name: '香港',\n        currency: 'HKD/$港幣',\n        excludeRegions: ['TW', 'CN', 'MY'],\n        examples: ['HKTVmall', '港鐵', 'OK便利店', '八達通', '強積金']\n    },\n    'zh-MY': {\n        name: '馬來西亞',\n        currency: 'RM/馬幣',\n        excludeRegions: ['TW', 'HK', 'CN'],\n        examples: ['Lazada', 'Shopee', 'Grab', 'Touch \\'n Go', 'EPF']\n    }\n};\n\n/**\n * Get region label for display\n */\nexport const getRegionLabel = (audience: TargetAudience): string => {\n    return REGION_CONFIG[audience]?.name || audience;\n};\n\n/**\n * Analyze content to detect entities from other regions\n */\nexport const detectForeignEntities = async (\n    content: string,\n    targetAudience: TargetAudience\n): Promise<{\n    entities: { text: string; type: string; region: string }[];\n    usage: TokenUsage;\n    cost: CostBreakdown;\n    duration: number;\n}> => {\n    const config = REGION_CONFIG[targetAudience];\n    if (!config) {\n        return {\n            entities: [],\n            usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: 0\n        };\n    }\n\n    const prompt = `\n你是一位${config.name}市場內容分析專家。請分析以下內容，找出所有非${config.name}本地的實體（品牌、機構、地點、服務、法規等）。\n\n**目標市場：** ${config.name}\n**貨幣：** ${config.currency}\n**本地品牌範例：** ${config.examples.join('、')}\n\n**分析標準：**\n1. 其他地區特定實體（品牌、服務、機構）\n2. 其他地區法規或政策\n3. 其他地區貨幣表達\n4. 其他地區地點引用（除非是一般地理描述）\n\n**內容：**\n${content.substring(0, 6000)}\n\n**輸出 JSON 格式：**\n{\n  \"entities\": [\n    { \"text\": \"實體名稱\", \"type\": \"brand|service|location|regulation|currency\", \"region\": \"TW|HK|CN|MY|OTHER\" }\n  ]\n}\n\n只輸出 JSON，找不到就返回空陣列。`;\n\n    const response = await aiService.runJson<{\n        entities: { text: string; type: string; region: string }[];\n    }>(prompt, 'FLASH');\n\n    // Filter out entities that belong to the target region\n    const foreignEntities = (response.data.entities || []).filter(e =>\n        config.excludeRegions.includes(e.region) || e.region === 'OTHER'\n    );\n\n    return {\n        entities: foreignEntities,\n        usage: response.usage,\n        cost: response.cost,\n        duration: response.duration\n    };\n};\n\n/**\n * Find regional equivalents for detected foreign entities\n */\nexport const findRegionEquivalents = async (\n    entities: { text: string; type: string; region: string }[],\n    targetAudience: TargetAudience\n): Promise<{\n    mappings: RegionIssue[];\n    usage: TokenUsage;\n    cost: CostBreakdown;\n    duration: number;\n}> => {\n    const config = REGION_CONFIG[targetAudience];\n    if (entities.length === 0 || !config) {\n        return {\n            mappings: [],\n            usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: 0\n        };\n    }\n\n    // Process ALL entities in a single grounding call (no limit)\n    const entityList = entities.map(e => `- ${e.text} (${e.type})`).join('\\n');\n\n    const prompt = `\n你是一位${config.name}市場專家。請為以下非${config.name}實體找出${config.name}適合的替代選項。\n\n**需要查找替代的實體：**\n${entityList}\n\n**替代選項優先順序（請依序嘗試）：**\n1. **直接對應** - ${config.name}市場中功能相同的品牌/服務（如：台灣「全聯」→ 香港「百佳/惠康」）\n2. **同類替代** - 同品類中${config.name}較知名的品牌（如：台灣服裝品牌 → 任何香港知名服裝品牌）\n3. **通用描述** - 如果找不到具體品牌，使用通用描述（如：「韌 REN」→「本地運動服飾品牌」或「知名運動服裝店」）\n4. **刪除** - 只有在完全無法描述時才使用「[刪除]」\n\n**重要規則：**\n- 盡量提供替代，不要輕易使用「[刪除]」\n- 通用描述也是可接受的替代（如「大型藥妝連鎖店」、「本地電商平台」）\n- 每個輸入的實體都必須有一個對應的輸出`;\n\n    // Schema for structured output\n    const schema = {\n        type: 'object',\n        properties: {\n            mappings: {\n                type: 'array',\n                items: {\n                    type: 'object',\n                    properties: {\n                        type: { type: 'string', description: 'entity|brand|regulation|currency|location|service' },\n                        original: { type: 'string', description: '原實體名稱' },\n                        regionEquivalent: { type: 'string', description: `${config.name}對應的真實品牌/服務名稱，如找不到則填 \"[刪除]\"` },\n                        confidence: { type: 'number', description: '0.0-1.0 置信度' },\n                        context: { type: 'string', description: '說明為何這是合適的替代，或為何建議刪除' },\n                        sourceRegion: { type: 'string', description: '來源地區' }\n                    },\n                    required: ['original', 'regionEquivalent', 'context']\n                }\n            }\n        },\n        required: ['mappings']\n    };\n\n    // Use runJsonWithSearch with schema for stable structured output\n    const response = await aiService.runJsonWithSearch<{\n        mappings: RegionIssue[];\n    }>(prompt, 'FLASH', schema);\n\n    // Ensure ALL input entities have a mapping\n    const aiMappings = response.data.mappings || [];\n    const finalMappings: RegionIssue[] = entities.map(entity => {\n        const found = aiMappings.find(m => m.original === entity.text);\n        if (found) {\n            // Convert \"[刪除]\" to empty string for actual deletion\n            return {\n                ...found,\n                regionEquivalent: found.regionEquivalent === '[刪除]' ? '' : found.regionEquivalent\n            };\n        }\n        // If AI didn't return this entity, create empty replacement (for deletion)\n        return {\n            type: entity.type as any,\n            original: entity.text,\n            regionEquivalent: '',\n            confidence: 0,\n            context: '未找到合適替代，建議從內容中移除',\n            sourceRegion: entity.region\n        };\n    });\n\n    return {\n        mappings: finalMappings,\n        usage: response.usage,\n        cost: response.cost,\n        duration: response.duration\n    };\n};\n\n/**\n * Rewrite content for target region\n */\nexport const rewriteForRegion = async (\n    content: string,\n    mappings: RegionIssue[],\n    targetAudience: TargetAudience\n): Promise<{\n    rewrittenContent: string;\n    changes: { original: string; rewritten: string }[];\n    usage: TokenUsage;\n    cost: CostBreakdown;\n    duration: number;\n}> => {\n    const config = REGION_CONFIG[targetAudience];\n    if (mappings.length === 0 || !config) {\n        return {\n            rewrittenContent: content,\n            changes: [],\n            usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: 0\n        };\n    }\n\n    const mappingInstructions = mappings.map(m =>\n        `- 「${m.original}」→「${m.regionEquivalent}」：${m.context}`\n    ).join('\\n');\n\n    const prompt = `\n你是一位${config.name}市場內容編輯專家。請根據以下替換指引，改寫內容使其適合${config.name}讀者。\n\n**替換指引：**\n${mappingInstructions}\n\n**改寫規則：**\n1. 保持原文語意和結構\n2. 只替換指定的實體，其他內容保持不變\n3. 確保替換後的句子通順自然\n4. 如果某處替換會造成語意不通，可以適度調整上下文\n\n**原始內容：**\n${content.substring(0, 8000)}\n\n**輸出 JSON 格式：**\n{\n  \"rewrittenContent\": \"改寫後的完整內容\",\n  \"changes\": [\n    { \"original\": \"原句子\", \"rewritten\": \"改寫後句子\" }\n  ]\n}\n\n只輸出 JSON。`;\n\n    const response = await aiService.runJson<{\n        rewrittenContent: string;\n        changes: { original: string; rewritten: string }[];\n    }>(prompt, 'FLASH');\n\n    return {\n        rewrittenContent: response.data.rewrittenContent || content,\n        changes: response.data.changes || [],\n        usage: response.usage,\n        cost: response.cost,\n        duration: response.duration\n    };\n};\n\n/**\n * Full region grounding validation pipeline\n */\nexport const validateAndAdaptForRegion = async (\n    content: string,\n    targetAudience: TargetAudience\n): Promise<{\n    result: RegionGroundingResult;\n    rewrittenContent: string;\n    usage: TokenUsage;\n    cost: CostBreakdown;\n    duration: number;\n}> => {\n    const config = REGION_CONFIG[targetAudience];\n    const regionName = config?.name || targetAudience;\n\n    const startTime = Date.now();\n    let totalUsage: TokenUsage = { inputTokens: 0, outputTokens: 0, totalTokens: 0 };\n    let totalCost: CostBreakdown = { inputCost: 0, outputCost: 0, totalCost: 0 };\n\n    // Step 1: Detect foreign entities\n    const detectResult = await detectForeignEntities(content, targetAudience);\n    totalUsage.inputTokens += detectResult.usage.inputTokens;\n    totalUsage.outputTokens += detectResult.usage.outputTokens;\n    totalUsage.totalTokens += detectResult.usage.totalTokens;\n    totalCost.inputCost += detectResult.cost.inputCost;\n    totalCost.outputCost += detectResult.cost.outputCost;\n    totalCost.totalCost += detectResult.cost.totalCost;\n\n    console.log(`[RegionGrounding] Detected ${detectResult.entities.length} foreign entities for ${regionName}`);\n\n    if (detectResult.entities.length === 0) {\n        return {\n            result: {\n                isRegionRelevant: true,\n                relevanceScore: 100,\n                issues: [],\n                suggestions: []\n            },\n            rewrittenContent: content,\n            usage: totalUsage,\n            cost: totalCost,\n            duration: Date.now() - startTime\n        };\n    }\n\n    // Step 2: Find regional equivalents\n    const groundingResult = await findRegionEquivalents(detectResult.entities, targetAudience);\n    totalUsage.inputTokens += groundingResult.usage.inputTokens;\n    totalUsage.outputTokens += groundingResult.usage.outputTokens;\n    totalUsage.totalTokens += groundingResult.usage.totalTokens;\n    totalCost.inputCost += groundingResult.cost.inputCost;\n    totalCost.outputCost += groundingResult.cost.outputCost;\n    totalCost.totalCost += groundingResult.cost.totalCost;\n\n    console.log(`[RegionGrounding] Found ${groundingResult.mappings.length} ${regionName} equivalents`);\n\n    // Step 3: Rewrite content if we have mappings\n    let finalContent = content;\n    let changes: { original: string; rewritten: string }[] = [];\n\n    if (groundingResult.mappings.length > 0) {\n        const rewriteResult = await rewriteForRegion(content, groundingResult.mappings, targetAudience);\n        finalContent = rewriteResult.rewrittenContent;\n        changes = rewriteResult.changes;\n        totalUsage.inputTokens += rewriteResult.usage.inputTokens;\n        totalUsage.outputTokens += rewriteResult.usage.outputTokens;\n        totalUsage.totalTokens += rewriteResult.usage.totalTokens;\n        totalCost.inputCost += rewriteResult.cost.inputCost;\n        totalCost.outputCost += rewriteResult.cost.outputCost;\n        totalCost.totalCost += rewriteResult.cost.totalCost;\n    }\n\n    // Calculate relevance score\n    const totalEntities = detectResult.entities.length;\n    const resolvedEntities = groundingResult.mappings.filter(m => m.confidence > 0.7).length;\n    const relevanceScore = Math.round(((resolvedEntities) / Math.max(1, totalEntities)) * 100);\n\n    return {\n        result: {\n            isRegionRelevant: detectResult.entities.length === 0 || relevanceScore >= 80,\n            relevanceScore: Math.max(0, Math.min(100, 100 - (totalEntities - resolvedEntities) * 10)),\n            issues: groundingResult.mappings,\n            suggestions: changes\n        },\n        rewrittenContent: finalContent,\n        usage: totalUsage,\n        cost: totalCost,\n        duration: Date.now() - startTime\n    };\n};\n\n/**\n * AI-powered localization for section plans\n * Intelligently rewrites plans for target region, not just string replacement\n */\nexport const localizePlanWithAI = async (\n    data: {\n        generalPlan: string[];\n        conversionPlan: string[];\n        sections: { title: string; narrativePlan?: string[]; keyFacts?: string[]; uspNotes?: string[]; subheadings?: string[] }[];\n    },\n    replacements: { original: string; replacement: string; reason?: string }[],\n    targetAudience: TargetAudience\n): Promise<{\n    localizedGeneralPlan: string[];\n    localizedConversionPlan: string[];\n    localizedSections: typeof data.sections;\n    usage: TokenUsage;\n    cost: CostBreakdown;\n    duration: number;\n}> => {\n    const config = REGION_CONFIG[targetAudience];\n    if (!config || replacements.length === 0) {\n        return {\n            localizedGeneralPlan: data.generalPlan,\n            localizedConversionPlan: data.conversionPlan,\n            localizedSections: data.sections,\n            usage: { inputTokens: 0, outputTokens: 0, totalTokens: 0 },\n            cost: { inputCost: 0, outputCost: 0, totalCost: 0 },\n            duration: 0\n        };\n    }\n\n    // Build replacement instructions\n    const replacementGuide = replacements.map(r =>\n        r.replacement\n            ? `- 「${r.original}」→「${r.replacement}」${r.reason ? `（${r.reason}）` : ''}`\n            : `- 「${r.original}」→ 刪除此詞，重寫相關句子使其通順`\n    ).join('\\n');\n\n    // Serialize plans for AI\n    const plansJson = JSON.stringify({\n        generalPlan: data.generalPlan,\n        conversionPlan: data.conversionPlan,\n        sections: data.sections.map(s => ({\n            title: s.title,\n            narrativePlan: s.narrativePlan || [],\n            keyFacts: s.keyFacts || [],\n            uspNotes: s.uspNotes || [],\n            subheadings: s.subheadings || []\n        }))\n    }, null, 2);\n\n    const prompt = `\n你是一位${config.name}市場內容本地化專家。請將以下段落計劃進行本地化改寫，使其完全適合${config.name}讀者。\n\n**本地化替換指引：**\n${replacementGuide}\n\n**原始段落計劃：**\n${plansJson}\n\n**本地化規則：**\n1. 應用上述替換指引，將非${config.name}的品牌/服務/實體替換為適當的替代\n2. 如果替換後句子不通順，請適度改寫使其自然\n3. 如果某詞需要刪除，請重寫該句子使其意思完整，不要留下空白\n4. 保持原有的結構和格式（數組、標題等）\n5. 對於 generalPlan 和 conversionPlan，確保策略仍然適用於${config.name}市場\n\n**請輸出 JSON，格式與輸入相同：**\n{\n  \"generalPlan\": [\"本地化後的策略...\"],\n  \"conversionPlan\": [\"本地化後的轉換策略...\"],\n  \"sections\": [\n    {\n      \"title\": \"本地化後的標題\",\n      \"narrativePlan\": [\"本地化後的敘事計劃...\"],\n      \"keyFacts\": [\"本地化後的關鍵事實...\"],\n      \"uspNotes\": [\"本地化後的賣點...\"],\n      \"subheadings\": [\"本地化後的子標題...\"]\n    }\n  ]\n}\n\n只輸出 JSON，不要其他文字。`;\n\n    const schema = {\n        type: 'object',\n        properties: {\n            generalPlan: { type: 'array', items: { type: 'string' } },\n            conversionPlan: { type: 'array', items: { type: 'string' } },\n            sections: {\n                type: 'array',\n                items: {\n                    type: 'object',\n                    properties: {\n                        title: { type: 'string' },\n                        narrativePlan: { type: 'array', items: { type: 'string' } },\n                        keyFacts: { type: 'array', items: { type: 'string' } },\n                        uspNotes: { type: 'array', items: { type: 'string' } },\n                        subheadings: { type: 'array', items: { type: 'string' } }\n                    }\n                }\n            }\n        },\n        required: ['generalPlan', 'conversionPlan', 'sections']\n    };\n\n    const response = await aiService.runJson<{\n        generalPlan: string[];\n        conversionPlan: string[];\n        sections: typeof data.sections;\n    }>(prompt, 'FLASH', schema);\n\n    return {\n        localizedGeneralPlan: response.data.generalPlan || data.generalPlan,\n        localizedConversionPlan: response.data.conversionPlan || data.conversionPlan,\n        localizedSections: response.data.sections || data.sections,\n        usage: response.usage,\n        cost: response.cost,\n        duration: response.duration\n    };\n};\n"],"names":[],"mappings":"AAAA;;;;CAIC;;;;;;;;;;;;;;AAED;;AAmBA,MAAM,gBAKD;IACD,SAAS;QACL,MAAM;QACN,UAAU;QACV,gBAAgB;YAAC;YAAM;YAAM;SAAK;QAClC,UAAU;YAAC;YAAM;YAAU;YAAQ;YAAM;YAAM;YAAM;SAAO;IAChE;IACA,SAAS;QACL,MAAM;QACN,UAAU;QACV,gBAAgB;YAAC;YAAM;YAAM;SAAK;QAClC,UAAU;YAAC;YAAY;YAAM;YAAS;YAAO;SAAM;IACvD;IACA,SAAS;QACL,MAAM;QACN,UAAU;QACV,gBAAgB;YAAC;YAAM;YAAM;SAAK;QAClC,UAAU;YAAC;YAAU;YAAU;YAAQ;YAAgB;SAAM;IACjE;AACJ;AAKO,MAAM,iBAAiB,CAAC;IAC3B,OAAO,aAAa,CAAC,SAAS,EAAE,QAAQ;AAC5C;AAKO,MAAM,wBAAwB,OACjC,SACA;IAOA,MAAM,SAAS,aAAa,CAAC,eAAe;IAC5C,IAAI,CAAC,QAAQ;QACT,OAAO;YACH,UAAU,EAAE;YACZ,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YACzD,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU;QACd;IACJ;IAEA,MAAM,SAAS,CAAC;IAChB,EAAE,OAAO,IAAI,CAAC,sBAAsB,EAAE,OAAO,IAAI,CAAC;;UAE5C,EAAE,OAAO,IAAI,CAAC;QAChB,EAAE,OAAO,QAAQ,CAAC;YACd,EAAE,OAAO,QAAQ,CAAC,IAAI,CAAC,KAAK;;;;;;;;;AASxC,EAAE,QAAQ,SAAS,CAAC,GAAG,MAAM;;;;;;;;;mBASV,CAAC;IAEhB,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CAErC,QAAQ;IAEX,uDAAuD;IACvD,MAAM,kBAAkB,CAAC,SAAS,IAAI,CAAC,QAAQ,IAAI,EAAE,EAAE,MAAM,CAAC,CAAA,IAC1D,OAAO,cAAc,CAAC,QAAQ,CAAC,EAAE,MAAM,KAAK,EAAE,MAAM,KAAK;IAG7D,OAAO;QACH,UAAU;QACV,OAAO,SAAS,KAAK;QACrB,MAAM,SAAS,IAAI;QACnB,UAAU,SAAS,QAAQ;IAC/B;AACJ;AAKO,MAAM,wBAAwB,OACjC,UACA;IAOA,MAAM,SAAS,aAAa,CAAC,eAAe;IAC5C,IAAI,SAAS,MAAM,KAAK,KAAK,CAAC,QAAQ;QAClC,OAAO;YACH,UAAU,EAAE;YACZ,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YACzD,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU;QACd;IACJ;IAEA,6DAA6D;IAC7D,MAAM,aAAa,SAAS,GAAG,CAAC,CAAA,IAAK,CAAC,EAAE,EAAE,EAAE,IAAI,CAAC,EAAE,EAAE,EAAE,IAAI,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC;IAErE,MAAM,SAAS,CAAC;IAChB,EAAE,OAAO,IAAI,CAAC,UAAU,EAAE,OAAO,IAAI,CAAC,IAAI,EAAE,OAAO,IAAI,CAAC;;;AAG5D,EAAE,WAAW;;;cAGC,EAAE,OAAO,IAAI,CAAC;kBACV,EAAE,OAAO,IAAI,CAAC;;;;;;;oBAOZ,CAAC;IAEjB,+BAA+B;IAC/B,MAAM,SAAS;QACX,MAAM;QACN,YAAY;YACR,UAAU;gBACN,MAAM;gBACN,OAAO;oBACH,MAAM;oBACN,YAAY;wBACR,MAAM;4BAAE,MAAM;4BAAU,aAAa;wBAAoD;wBACzF,UAAU;4BAAE,MAAM;4BAAU,aAAa;wBAAQ;wBACjD,kBAAkB;4BAAE,MAAM;4BAAU,aAAa,GAAG,OAAO,IAAI,CAAC,0BAA0B,CAAC;wBAAC;wBAC5F,YAAY;4BAAE,MAAM;4BAAU,aAAa;wBAAc;wBACzD,SAAS;4BAAE,MAAM;4BAAU,aAAa;wBAAsB;wBAC9D,cAAc;4BAAE,MAAM;4BAAU,aAAa;wBAAO;oBACxD;oBACA,UAAU;wBAAC;wBAAY;wBAAoB;qBAAU;gBACzD;YACJ;QACJ;QACA,UAAU;YAAC;SAAW;IAC1B;IAEA,iEAAiE;IACjE,MAAM,WAAW,MAAM,mJAAS,CAAC,iBAAiB,CAE/C,QAAQ,SAAS;IAEpB,2CAA2C;IAC3C,MAAM,aAAa,SAAS,IAAI,CAAC,QAAQ,IAAI,EAAE;IAC/C,MAAM,gBAA+B,SAAS,GAAG,CAAC,CAAA;QAC9C,MAAM,QAAQ,WAAW,IAAI,CAAC,CAAA,IAAK,EAAE,QAAQ,KAAK,OAAO,IAAI;QAC7D,IAAI,OAAO;YACP,qDAAqD;YACrD,OAAO;gBACH,GAAG,KAAK;gBACR,kBAAkB,MAAM,gBAAgB,KAAK,SAAS,KAAK,MAAM,gBAAgB;YACrF;QACJ;QACA,2EAA2E;QAC3E,OAAO;YACH,MAAM,OAAO,IAAI;YACjB,UAAU,OAAO,IAAI;YACrB,kBAAkB;YAClB,YAAY;YACZ,SAAS;YACT,cAAc,OAAO,MAAM;QAC/B;IACJ;IAEA,OAAO;QACH,UAAU;QACV,OAAO,SAAS,KAAK;QACrB,MAAM,SAAS,IAAI;QACnB,UAAU,SAAS,QAAQ;IAC/B;AACJ;AAKO,MAAM,mBAAmB,OAC5B,SACA,UACA;IAQA,MAAM,SAAS,aAAa,CAAC,eAAe;IAC5C,IAAI,SAAS,MAAM,KAAK,KAAK,CAAC,QAAQ;QAClC,OAAO;YACH,kBAAkB;YAClB,SAAS,EAAE;YACX,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YACzD,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU;QACd;IACJ;IAEA,MAAM,sBAAsB,SAAS,GAAG,CAAC,CAAA,IACrC,CAAC,GAAG,EAAE,EAAE,QAAQ,CAAC,GAAG,EAAE,EAAE,gBAAgB,CAAC,EAAE,EAAE,EAAE,OAAO,EAAE,EAC1D,IAAI,CAAC;IAEP,MAAM,SAAS,CAAC;IAChB,EAAE,OAAO,IAAI,CAAC,2BAA2B,EAAE,OAAO,IAAI,CAAC;;;AAG3D,EAAE,oBAAoB;;;;;;;;;AAStB,EAAE,QAAQ,SAAS,CAAC,GAAG,MAAM;;;;;;;;;;SAUpB,CAAC;IAEN,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CAGrC,QAAQ;IAEX,OAAO;QACH,kBAAkB,SAAS,IAAI,CAAC,gBAAgB,IAAI;QACpD,SAAS,SAAS,IAAI,CAAC,OAAO,IAAI,EAAE;QACpC,OAAO,SAAS,KAAK;QACrB,MAAM,SAAS,IAAI;QACnB,UAAU,SAAS,QAAQ;IAC/B;AACJ;AAKO,MAAM,4BAA4B,OACrC,SACA;IAQA,MAAM,SAAS,aAAa,CAAC,eAAe;IAC5C,MAAM,aAAa,QAAQ,QAAQ;IAEnC,MAAM,YAAY,KAAK,GAAG;IAC1B,IAAI,aAAyB;QAAE,aAAa;QAAG,cAAc;QAAG,aAAa;IAAE;IAC/E,IAAI,YAA2B;QAAE,WAAW;QAAG,YAAY;QAAG,WAAW;IAAE;IAE3E,kCAAkC;IAClC,MAAM,eAAe,MAAM,sBAAsB,SAAS;IAC1D,WAAW,WAAW,IAAI,aAAa,KAAK,CAAC,WAAW;IACxD,WAAW,YAAY,IAAI,aAAa,KAAK,CAAC,YAAY;IAC1D,WAAW,WAAW,IAAI,aAAa,KAAK,CAAC,WAAW;IACxD,UAAU,SAAS,IAAI,aAAa,IAAI,CAAC,SAAS;IAClD,UAAU,UAAU,IAAI,aAAa,IAAI,CAAC,UAAU;IACpD,UAAU,SAAS,IAAI,aAAa,IAAI,CAAC,SAAS;IAElD,QAAQ,GAAG,CAAC,CAAC,2BAA2B,EAAE,aAAa,QAAQ,CAAC,MAAM,CAAC,sBAAsB,EAAE,YAAY;IAE3G,IAAI,aAAa,QAAQ,CAAC,MAAM,KAAK,GAAG;QACpC,OAAO;YACH,QAAQ;gBACJ,kBAAkB;gBAClB,gBAAgB;gBAChB,QAAQ,EAAE;gBACV,aAAa,EAAE;YACnB;YACA,kBAAkB;YAClB,OAAO;YACP,MAAM;YACN,UAAU,KAAK,GAAG,KAAK;QAC3B;IACJ;IAEA,oCAAoC;IACpC,MAAM,kBAAkB,MAAM,sBAAsB,aAAa,QAAQ,EAAE;IAC3E,WAAW,WAAW,IAAI,gBAAgB,KAAK,CAAC,WAAW;IAC3D,WAAW,YAAY,IAAI,gBAAgB,KAAK,CAAC,YAAY;IAC7D,WAAW,WAAW,IAAI,gBAAgB,KAAK,CAAC,WAAW;IAC3D,UAAU,SAAS,IAAI,gBAAgB,IAAI,CAAC,SAAS;IACrD,UAAU,UAAU,IAAI,gBAAgB,IAAI,CAAC,UAAU;IACvD,UAAU,SAAS,IAAI,gBAAgB,IAAI,CAAC,SAAS;IAErD,QAAQ,GAAG,CAAC,CAAC,wBAAwB,EAAE,gBAAgB,QAAQ,CAAC,MAAM,CAAC,CAAC,EAAE,WAAW,YAAY,CAAC;IAElG,8CAA8C;IAC9C,IAAI,eAAe;IACnB,IAAI,UAAqD,EAAE;IAE3D,IAAI,gBAAgB,QAAQ,CAAC,MAAM,GAAG,GAAG;QACrC,MAAM,gBAAgB,MAAM,iBAAiB,SAAS,gBAAgB,QAAQ,EAAE;QAChF,eAAe,cAAc,gBAAgB;QAC7C,UAAU,cAAc,OAAO;QAC/B,WAAW,WAAW,IAAI,cAAc,KAAK,CAAC,WAAW;QACzD,WAAW,YAAY,IAAI,cAAc,KAAK,CAAC,YAAY;QAC3D,WAAW,WAAW,IAAI,cAAc,KAAK,CAAC,WAAW;QACzD,UAAU,SAAS,IAAI,cAAc,IAAI,CAAC,SAAS;QACnD,UAAU,UAAU,IAAI,cAAc,IAAI,CAAC,UAAU;QACrD,UAAU,SAAS,IAAI,cAAc,IAAI,CAAC,SAAS;IACvD;IAEA,4BAA4B;IAC5B,MAAM,gBAAgB,aAAa,QAAQ,CAAC,MAAM;IAClD,MAAM,mBAAmB,gBAAgB,QAAQ,CAAC,MAAM,CAAC,CAAA,IAAK,EAAE,UAAU,GAAG,KAAK,MAAM;IACxF,MAAM,iBAAiB,KAAK,KAAK,CAAC,AAAE,mBAAoB,KAAK,GAAG,CAAC,GAAG,iBAAkB;IAEtF,OAAO;QACH,QAAQ;YACJ,kBAAkB,aAAa,QAAQ,CAAC,MAAM,KAAK,KAAK,kBAAkB;YAC1E,gBAAgB,KAAK,GAAG,CAAC,GAAG,KAAK,GAAG,CAAC,KAAK,MAAM,CAAC,gBAAgB,gBAAgB,IAAI;YACrF,QAAQ,gBAAgB,QAAQ;YAChC,aAAa;QACjB;QACA,kBAAkB;QAClB,OAAO;QACP,MAAM;QACN,UAAU,KAAK,GAAG,KAAK;IAC3B;AACJ;AAMO,MAAM,qBAAqB,OAC9B,MAKA,cACA;IASA,MAAM,SAAS,aAAa,CAAC,eAAe;IAC5C,IAAI,CAAC,UAAU,aAAa,MAAM,KAAK,GAAG;QACtC,OAAO;YACH,sBAAsB,KAAK,WAAW;YACtC,yBAAyB,KAAK,cAAc;YAC5C,mBAAmB,KAAK,QAAQ;YAChC,OAAO;gBAAE,aAAa;gBAAG,cAAc;gBAAG,aAAa;YAAE;YACzD,MAAM;gBAAE,WAAW;gBAAG,YAAY;gBAAG,WAAW;YAAE;YAClD,UAAU;QACd;IACJ;IAEA,iCAAiC;IACjC,MAAM,mBAAmB,aAAa,GAAG,CAAC,CAAA,IACtC,EAAE,WAAW,GACP,CAAC,GAAG,EAAE,EAAE,QAAQ,CAAC,GAAG,EAAE,EAAE,WAAW,CAAC,CAAC,EAAE,EAAE,MAAM,GAAG,CAAC,CAAC,EAAE,EAAE,MAAM,CAAC,CAAC,CAAC,GAAG,IAAI,GACxE,CAAC,GAAG,EAAE,EAAE,QAAQ,CAAC,kBAAkB,CAAC,EAC5C,IAAI,CAAC;IAEP,yBAAyB;IACzB,MAAM,YAAY,KAAK,SAAS,CAAC;QAC7B,aAAa,KAAK,WAAW;QAC7B,gBAAgB,KAAK,cAAc;QACnC,UAAU,KAAK,QAAQ,CAAC,GAAG,CAAC,CAAA,IAAK,CAAC;gBAC9B,OAAO,EAAE,KAAK;gBACd,eAAe,EAAE,aAAa,IAAI,EAAE;gBACpC,UAAU,EAAE,QAAQ,IAAI,EAAE;gBAC1B,UAAU,EAAE,QAAQ,IAAI,EAAE;gBAC1B,aAAa,EAAE,WAAW,IAAI,EAAE;YACpC,CAAC;IACL,GAAG,MAAM;IAET,MAAM,SAAS,CAAC;IAChB,EAAE,OAAO,IAAI,CAAC,gCAAgC,EAAE,OAAO,IAAI,CAAC;;;AAGhE,EAAE,iBAAiB;;;AAGnB,EAAE,UAAU;;;cAGE,EAAE,OAAO,IAAI,CAAC;;;;4CAIgB,EAAE,OAAO,IAAI,CAAC;;;;;;;;;;;;;;;;;gBAiB1C,CAAC;IAEb,MAAM,SAAS;QACX,MAAM;QACN,YAAY;YACR,aAAa;gBAAE,MAAM;gBAAS,OAAO;oBAAE,MAAM;gBAAS;YAAE;YACxD,gBAAgB;gBAAE,MAAM;gBAAS,OAAO;oBAAE,MAAM;gBAAS;YAAE;YAC3D,UAAU;gBACN,MAAM;gBACN,OAAO;oBACH,MAAM;oBACN,YAAY;wBACR,OAAO;4BAAE,MAAM;wBAAS;wBACxB,eAAe;4BAAE,MAAM;4BAAS,OAAO;gCAAE,MAAM;4BAAS;wBAAE;wBAC1D,UAAU;4BAAE,MAAM;4BAAS,OAAO;gCAAE,MAAM;4BAAS;wBAAE;wBACrD,UAAU;4BAAE,MAAM;4BAAS,OAAO;gCAAE,MAAM;4BAAS;wBAAE;wBACrD,aAAa;4BAAE,MAAM;4BAAS,OAAO;gCAAE,MAAM;4BAAS;wBAAE;oBAC5D;gBACJ;YACJ;QACJ;QACA,UAAU;YAAC;YAAe;YAAkB;SAAW;IAC3D;IAEA,MAAM,WAAW,MAAM,mJAAS,CAAC,OAAO,CAIrC,QAAQ,SAAS;IAEpB,OAAO;QACH,sBAAsB,SAAS,IAAI,CAAC,WAAW,IAAI,KAAK,WAAW;QACnE,yBAAyB,SAAS,IAAI,CAAC,cAAc,IAAI,KAAK,cAAc;QAC5E,mBAAmB,SAAS,IAAI,CAAC,QAAQ,IAAI,KAAK,QAAQ;QAC1D,OAAO,SAAS,KAAK;QACrB,MAAM,SAAS,IAAI;QACnB,UAAU,SAAS,QAAQ;IAC/B;AACJ"}}]
}